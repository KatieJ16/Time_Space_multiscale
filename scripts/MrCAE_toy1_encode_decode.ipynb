{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Model 1: 2 spatial modes with different oscillating frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Created by Yuying Liu, 09/23/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Phi(x, t) = u(x)cos(\\omega_0 t) + v(x)cos(\\omega_1 t + \\frac{\\pi}{4})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../src/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "    \n",
    "import torch_cae_multilevel_V4 as net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "w0 = 0.5\n",
    "w1 = 4.0\n",
    "sigma0 = 10.0\n",
    "sigma1 = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define two modes\n",
    "def phi1(x, y, t):\n",
    "    return 1./np.cosh((x+1)/sigma0)/np.cosh((y-1)/sigma0)*np.cos(w0*t)\n",
    "\n",
    "def phi2(x, y, t):\n",
    "    return 1./(sigma1*np.sqrt(2*np.pi))*np.exp(-((x-1)**2+(y+1)**2)/(2*sigma1**2))*np.cos(w1*t + np.pi/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mesh grids\n",
    "x = np.linspace(-5, 5, 127)\n",
    "y = np.linspace(-5, 5, 127)\n",
    "t = np.linspace(0, 8*np.pi, 500)\n",
    "xgrid, ygrid, tgrid = np.meshgrid(x, y, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127, 127, 500)\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "Phi = phi1(xgrid, ygrid, tgrid) + phi2(xgrid, ygrid, tgrid)\n",
    "scaled_Phi = (Phi - Phi.min()) / (Phi.max() - Phi.min())\n",
    "print(Phi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MrCAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the progressive training framework. \n",
    "One could have flexible control over each training step: low-level models are cheap to obtain, and higher level models are built based on them -- one can always revert back to the previous level and adjust the parameters to re-train the model if it is not satisfying. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.data.shape =  torch.Size([5000, 1, 127, 127])\n"
     ]
    }
   ],
   "source": [
    "# init model & load data\n",
    "data_path = '../data/toy1_longer'#toy1.npy'\n",
    "model_path = '../model/toy1_longer/'\n",
    "result_path = '../result/toy1_longer/'\n",
    "\n",
    "full_data_path = os.path.join(data_path, 'to_encode.npy')\n",
    "# np.save(full_data_path, scaled_Phi.T)\n",
    "\n",
    "dataset = net.MultiScaleDynamicsDataSet(full_data_path, n_levels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level =  0\n",
      "archs[i] =  [1]\n",
      "widen_sizes =  []\n",
      "deepening\n",
      "n_levels =  5\n",
      "using maps\n",
      "*************************************************\n",
      "Model @Level 0:\n",
      "Perform deepening & widening, train each architectures ...\n",
      "model layers: \n",
      "['activation', 'L0_Conv_0', 'L0_deConv_0']\n",
      "losses printing format: local: mse/max/overall, global: mse/max/overall\n",
      "epoch [1/3000]\n",
      "[training set] local: 0.0026/0.0064/0.0045, global: 0.0025/0.0509/0.0267\n",
      "[validation set] local: 0.0026/0.0065/0.0045, global: 0.0025/0.0514/0.0270\n",
      "epoch [300/3000]:\n",
      "[training set] local: 0.0001/0.0044/0.0023, global: 0.0001/0.0459/0.0230\n",
      "[validation set] local: 0.0001/0.0047/0.0024, global: 0.0002/0.0487/0.0244\n",
      "early stopping at 300th iteration due to satisfying reconstruction!\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CAE(\n",
       "  (activation): Sequential()\n",
       "  (L0_Conv_0): Conv2dBlock(\n",
       "    (activation): ReLU()\n",
       "    (B0): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2))\n",
       "  )\n",
       "  (L0_deConv_0): Conv2dBlock(\n",
       "    (activation): ReLU()\n",
       "    (B0): ConvTranspose2d(1, 1, kernel_size=(3, 3), stride=(2, 2))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "archs = [[1]]#,[1,3,5,7]]\n",
    "tols = [0.001]#, 0.0005]#, 0.0001]\n",
    "net.train_net(archs=archs, dataset=dataset, max_epoch=3000, batch_size=350, \n",
    "              tols=tols, activation=torch.nn.Sequential(), w=0.5, model_path=model_path, \n",
    "              result_path=result_path, std=0.01, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model names: model_L{level}_{index}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-3a54866d8c00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model names: model_L{level}_{index}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# trained models at different levels\n",
    "models = {}\n",
    "print('model names: model_L{level}_{index}')\n",
    "for file_name in sorted(os.listdir(model_path)):\n",
    "    model_name, _ = file_name.split('.')\n",
    "    print(model_name)\n",
    "    models[model_name] = torch.load(os.path.join(model_path, file_name))\n",
    "\n",
    "inds = np.array(sorted(dataset.test_inds))\n",
    "dataset.test_inds = inds\n",
    "n_snapshots = len(inds)\n",
    "n_samples = 6\n",
    "n_step = n_snapshots // 6\n",
    "\n",
    "model = models['model_L0_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['model_L0_0']\n",
    "i = 0\n",
    "\n",
    "_, _, data = dataset.obtain_data_at_current_level(level=1)\n",
    "print()\n",
    "# print(model.cur_level)\n",
    "output, _, _, _ = model(data[[0], :, :, :], model.cur_level, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_local_op(data, mode='conv', ave=True):\n",
    "    \"\"\"\n",
    "    :param data: data to be processed\n",
    "    :param device: which device is the data placed in?\n",
    "    :param mode: string, 'conv' or 'deconv'\n",
    "    :param ave: if to use local average or sample the center\n",
    "    :return: processed data\n",
    "    \"\"\"\n",
    "    in_channels, out_channels, n_per_dim, _ = data.size()\n",
    "    n = min(in_channels, out_channels)\n",
    "    print(\"data.size() = \", data.size())\n",
    "#     in_channels= 1\n",
    "#     out_channels = 1\n",
    "#     n = 1\n",
    "    op = torch.nn.Conv2d(out_channels, out_channels, 3, stride=2, padding=0)\n",
    "    op.weight.data = torch.zeros(op.weight.data.size())\n",
    "    print(\"op.weight.data = \", op.weight.data.size())\n",
    "    op.bias.data = torch.zeros(op.bias.data.size())\n",
    "    for i in range(n):\n",
    "        print(\"op.weight.data[i, i, 1, 1].size() = \", op.weight.data[i, i, 1, 1].size())\n",
    "        op.weight.data[i, i, 1, 1] = torch.ones(op.weight.data[i, i, 1, 1].size())\n",
    "\n",
    "    # make them non-trainable\n",
    "    for param in op.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    transformed = np.zeros((in_channels, out_channels, n_per_dim//2, n_per_dim//2))\n",
    "    print(\"transformed shape = \", transformed.shape)\n",
    "#     for i in range(in_channels//)\n",
    "#     print(op(data).shape)\n",
    "    return op(data)\n",
    "\n",
    "\n",
    "#save encoded data\n",
    "\n",
    "to_encode_path = os.path.join(data_path, 'to_encode.npy')\n",
    "to_encode_raw = np.load(to_encode_path)\n",
    "# print(\"to_encode_raw shape = \", to_encode_raw.shape)\n",
    "# ghj\n",
    "\n",
    "n_levels = 5\n",
    "level = 0\n",
    "# print(data.shape)\n",
    "to_encode = torch.tensor(to_encode_raw).unsqueeze(1).float()\n",
    "\n",
    "# to_encode_right_size = torch.zeros((to_encode.size()))\n",
    "\n",
    "# n_runs, _,_,_ = to_encode.size()\n",
    "\n",
    "# for r in range(n_runs):\n",
    "#     print(\"r = \", r)\n",
    "# to_encode_this = to_encode[0].unsqueeze(1).float()\n",
    "for i in range(n_levels - level - 1):\n",
    "    to_encode = apply_local_op((to_encode), ave=False)\n",
    "#     to_encode_right_size[r, :,0,:,:] = to_encode_data\n",
    "    \n",
    "print(\"to_encode shape = \", to_encode.shape)\n",
    "\n",
    "\n",
    "# return train_data\n",
    "    \n",
    "# np.save(full_data_path, scaled_Phi.T)\n",
    "\n",
    "# dataset_encode = net.MultiScaleDynamicsDataSet(to_encode_path, n_levels=5)\n",
    "\n",
    "level = 0\n",
    "filter_group_num = 0\n",
    "model = models['model_L{}_{}'.format(level, filter_group_num)]\n",
    "# data = dataset_encode.obtain_data_at_current_level_all(level=0)\n",
    "\n",
    "encoded = model.encode(to_encode, model.cur_level)#, verbose = True)\n",
    "# decoded = model.decode(encoded, model.cur_level)\n",
    "\n",
    "print(encoded.shape)\n",
    "# print(decoded.shape)\n",
    "\n",
    "#save encoded data \n",
    "np.save(os.path.join(data_path, 'data_L{}_{}'.format(level, filter_group_num)), encoded.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['model_L0_0']\n",
    "i = 0\n",
    "\n",
    "_, _, data = dataset.obtain_data_at_current_level(level=1)\n",
    "# print(model.cur_level)\n",
    "output, _, _, _ = model(data[[0], :, :, :], model.cur_level, verbose = True)\n",
    "\n",
    "plt.imshow(output[0,0].detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "encoded = model.encode(data[[0], :, :, :], model.cur_level)#, verbose = True)\n",
    "decoded = model.decode(encoded, model.cur_level)#, verbose = True)\n",
    "\n",
    "plt.imshow(decoded[0,0].detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "_, _, data = dataset.obtain_data_at_current_level(level=0)\n",
    "output, _, _, _ = model(data[[i*n_step], :, :, :], model.cur_level)\n",
    "\n",
    "# plt.imshow(data[0,0,])\n",
    "\n",
    "\n",
    "print(output.shape)\n",
    "encoded = model.encode(data[[i*n_step], :, :, :], model.cur_level)\n",
    "print(encoded.shape)\n",
    "decoded = model.decode(encoded, model.cur_level)\n",
    "print(decoded.shape)\n",
    "\n",
    "plt.imshow(output[0,0].detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(decoded[0,0].detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "print(output)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask(data, mask, mask_type='resolved', width=1):\n",
    "    \"\"\"\n",
    "    :param data: data to be processed\n",
    "    :param mask: mask, a 2D torch tensor of 0s and 1s\n",
    "    :param mask_type: resolved map or unresolved map\n",
    "    :param width: int, specify how large the region is\n",
    "    :return: a 4D torch tensor represents masked data\n",
    "    \"\"\"\n",
    "    if not isinstance(width, int):\n",
    "        raise ValueError('width should be a positive integer!')\n",
    "\n",
    "    # convert to unresolved mask\n",
    "    if mask_type == 'resolved':\n",
    "        mask = 1 - mask\n",
    "        print(mask)\n",
    "    elif mask_type == 'unresolved':\n",
    "        mask = mask\n",
    "    else:\n",
    "        raise ValueError('mask_type could only be resolved or unresolved!')\n",
    "\n",
    "    # expansion\n",
    "    dx = [i for i in range(-width, width+1)]\n",
    "    dy = [i for i in range(-width, width+1)]\n",
    "    \n",
    "    print(\"dx = \", dx)\n",
    "    print(\"dy = \", dy)\n",
    "    m, n = mask.size()\n",
    "    print(\"mask.nonzero() = \", mask.nonzero())\n",
    "    for c in mask.nonzero():\n",
    "        x, y = int(c[0]), int(c[1])\n",
    "        for i in range(2*width+1):\n",
    "            for j in range(2*width+1):\n",
    "#                 print(i, \": \", j)\n",
    "                if 0 <= x + dx[i] < m and 0 <= y + dy[j] < n:\n",
    "#                     print(\"x + dx[i] =\", x + dx[i])\n",
    "                    mask[x + dx[i], y + dy[j]] = 1\n",
    "\n",
    "    # apply\n",
    "#     print(\"data shape = \", data.shape)\n",
    "#     print(\"mask.unsqueeze(0).unsqueeze(0).float() shape = \", mask.unsqueeze(0).unsqueeze(0).float().shape)\n",
    "    print(\"mask.unsqueeze(0).unsqueeze(0).float()= \", mask.unsqueeze(0).unsqueeze(0).float())\n",
    "    masked_data = data * mask.unsqueeze(0).unsqueeze(0).float()\n",
    "    return masked_data\n",
    "\n",
    "# x = torch.tensor([[[[1, 2, 3, 4, 5, 6], [4, 5, 6, 7, 8, 9, ], [7, 8, 9, 10, 11, 12]]]])\n",
    "# mask = torch.tensor([[0,0,0,0,0,0]]\n",
    "\n",
    "print(x.shape)\n",
    "print(mask.size())\n",
    "masked_x = apply_mask(x, mask)\n",
    "print(masked_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained models at different levels\n",
    "models = {}\n",
    "print('model names: model_L{level}_{index}')\n",
    "for file_name in sorted(os.listdir(model_path)):\n",
    "    model_name, _ = file_name.split('.')\n",
    "    print(model_name)\n",
    "    models[model_name] = torch.load(os.path.join(model_path, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.resolved_maps['0']['0'].cpu().detach().numpy().shape)\n",
    "print(model.resolved_maps['2']['3'].cpu().detach().numpy().shape)\n",
    "\n",
    "print(model.parameters())\n",
    "# for param in model.parameters():\n",
    "#     print(param)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print( name ,\" : \", param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the finest model\n",
    "model = models['model_L2_3']\n",
    "\n",
    "# resolved maps at different levels (that suggest poorly reconstructed regions)\n",
    "for i in range(3):\n",
    "    print(model.resolved_maps[str(i)].keys())\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.pcolor(model.resolved_maps['0']['0'].cpu().detach().numpy(), cmap='binary', vmin=0, vmax=1)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "fig.savefig(os.path.join(result_path, 'L0_I0.png'))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.pcolor(model.resolved_maps['0']['1'].cpu().detach().numpy(), cmap='binary', vmin=0, vmax=1)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "fig.savefig(os.path.join(result_path, 'L0_I1.png'))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.pcolor(model.resolved_maps['1']['0'].cpu().detach().numpy(), cmap='binary', vmin=0, vmax=1)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "fig.savefig(os.path.join(result_path, 'L1_I0.png'))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.pcolor(model.resolved_maps['1']['1'].cpu().detach().numpy(), cmap='binary', vmin=0, vmax=1)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "fig.savefig(os.path.join(result_path, 'L1_I1.png'))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.pcolor(model.resolved_maps['1']['2'].cpu().detach().numpy(), cmap='binary', vmin=0, vmax=1)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "fig.savefig(os.path.join(result_path, 'L1_I2.png'))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.pcolor(model.resolved_maps['2']['0'].cpu().detach().numpy(), cmap='binary', vmin=0, vmax=1)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "fig.savefig(os.path.join(result_path, 'L2_I0.png'))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.pcolor(model.resolved_maps['2']['1'].cpu().detach().numpy(), cmap='binary', vmin=0, vmax=1)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "fig.savefig(os.path.join(result_path, 'L2_I1.png'))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.pcolor(model.resolved_maps['2']['2'].cpu().detach().numpy(), cmap='binary', vmin=0, vmax=1)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "fig.savefig(os.path.join(result_path, 'L2_I2.png'))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.pcolor(model.resolved_maps['2']['3'].cpu().detach().numpy(), cmap='binary', vmin=0, vmax=1)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "fig.savefig(os.path.join(result_path, 'L2_I3.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.array(sorted(dataset.test_inds))\n",
    "dataset.test_inds = inds\n",
    "n_snapshots = len(inds)\n",
    "n_samples = 6\n",
    "n_step = n_snapshots // 6\n",
    "\n",
    "model = models['model_L2_3']\n",
    "fig, axes = plt.subplots(1, n_samples, figsize=(n_samples*11, 10))\n",
    "plt.subplots_adjust(wspace=0.01)\n",
    "for i in range(n_samples):\n",
    "    _, _, data = dataset.obtain_data_at_current_level(level=2)\n",
    "    output, _, _, _ = model(data[[i*n_step], :, :, :], model.cur_level)\n",
    "    axes[i].pcolor(output.squeeze().cpu().detach().numpy(), cmap='viridis')\n",
    "    axes[i].set_xticks([])\n",
    "    axes[i].set_yticks([])\n",
    "fig.savefig(os.path.join(result_path, 'test_L2_reconstructions.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models['model_L2_3']\n",
    "\n",
    "_, _, data = dataset.obtain_data_at_current_level(level=4)\n",
    "i = 0\n",
    "\n",
    "model = models['model_L0_0']\n",
    "encoded = model.encode(data[[i*n_step], :, :, :], model.cur_level)[0]\n",
    "print(encoded.shape)\n",
    "plt.imshow(encoded[0].detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show() \n",
    "\n",
    "\n",
    "x = model(data[[i*n_step], :, :, :], model.cur_level)[0]\n",
    "print(x.shape)\n",
    "plt.imshow(x[0,0].detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show() \n",
    "\n",
    "\n",
    "\n",
    "# model = models['model_L2_1']\n",
    "# encoded = model.encode(data[[i*n_step], :, :, :], model.cur_level)[0]\n",
    "# print(encoded.shape)\n",
    "# plt.imshow(encoded[0].detach().numpy())\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "\n",
    "# model = models['model_L2_2']\n",
    "# encoded = model.encode(data[[i*n_step], :, :, :], model.cur_level)[0]\n",
    "# print(encoded.shape)\n",
    "# plt.imshow(encoded[0].detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['model_L0_3']# plt.colorbar()\n",
    "print(len(model.resolved_maps))\n",
    "print(model.resolved_maps.keys())\n",
    "\n",
    "# print(model.resolved_maps['0']['0'])\n",
    "plt.imshow(model.loss_each_stage['0']['0'].detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(model.resolved_maps['0']['0'])\n",
    "plt.show()\n",
    "plt.imshow(model.loss_each_stage['0']['1'].detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(model.resolved_maps['0']['1'])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(model.loss_each_stage['0']['2'].detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(model.resolved_maps['0']['2'])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(model.loss_each_stage['0']['3'].detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(model.resolved_maps['0']['3'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstructions of test snapshots\n",
    "\n",
    "inds = np.array(sorted(dataset.test_inds))\n",
    "dataset.test_inds = inds\n",
    "n_snapshots = len(inds)\n",
    "n_samples = 6\n",
    "n_step = n_snapshots // 6\n",
    "\n",
    "fig, axes = plt.subplots(1, n_samples, figsize=(n_samples*11, 10))\n",
    "plt.subplots_adjust(wspace=0.01)\n",
    "for i in range(n_samples):\n",
    "    axes[i].pcolor(dataset.data[inds[i*n_step], :, :, :].squeeze().cpu().detach().numpy(), cmap='viridis')\n",
    "    axes[i].set_xticks([])\n",
    "    axes[i].set_yticks([])\n",
    "fig.savefig(os.path.join(result_path, 'test_samples.png'), bbox_inches='tight')\n",
    "\n",
    "\n",
    "model = models['model_L0_1']\n",
    "fig, axes = plt.subplots(1, n_samples, figsize=(n_samples*11, 10))\n",
    "plt.subplots_adjust(wspace=0.01)\n",
    "for i in range(n_samples):\n",
    "    _, _, data = dataset.obtain_data_at_current_level(level=0)\n",
    "    output, _, _, _ = model(data[[i*n_step], :, :, :], model.cur_level)\n",
    "    axes[i].pcolor(output.squeeze().cpu().detach().numpy(), cmap='viridis')\n",
    "    axes[i].set_xticks([])\n",
    "    axes[i].set_yticks([])\n",
    "fig.savefig(os.path.join(result_path, 'test_L0_reconstructions.png'), bbox_inches='tight')\n",
    "\n",
    "\n",
    "model = models['model_L1_2']\n",
    "fig, axes = plt.subplots(1, n_samples, figsize=(n_samples*11, 10))\n",
    "plt.subplots_adjust(wspace=0.01)\n",
    "for i in range(n_samples):\n",
    "    _, _, data = dataset.obtain_data_at_current_level(level=1)\n",
    "    output, _, _, _ = model(data[[i*n_step], :, :, :], model.cur_level)\n",
    "    axes[i].pcolor(output.squeeze().cpu().detach().numpy(), cmap='viridis')\n",
    "    axes[i].set_xticks([])\n",
    "    axes[i].set_yticks([])\n",
    "fig.savefig(os.path.join(result_path, 'test_L1_reconstructions.png'), bbox_inches='tight')\n",
    "\n",
    "\n",
    "model = models['model_L2_3']\n",
    "fig, axes = plt.subplots(1, n_samples, figsize=(n_samples*11, 10))\n",
    "plt.subplots_adjust(wspace=0.01)\n",
    "for i in range(n_samples):\n",
    "    _, _, data = dataset.obtain_data_at_current_level(level=2)\n",
    "    output, _, _, _ = model(data[[i*n_step], :, :, :], model.cur_level)\n",
    "    axes[i].pcolor(output.squeeze().cpu().detach().numpy(), cmap='viridis')\n",
    "    axes[i].set_xticks([])\n",
    "    axes[i].set_yticks([])\n",
    "fig.savefig(os.path.join(result_path, 'test_L2_reconstructions.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig1, axes = plt.subplots(2, 1, figsize=(8, 16))\n",
    "axes[0].pcolor(xgrid[:,:,0], ygrid[:,:,0], phi1(xgrid, ygrid, tgrid)[:, :, 0].T, cmap='viridis')\n",
    "axes[1].pcolor(xgrid[:,:,0], ygrid[:,:,0], phi2(xgrid, ygrid, tgrid)[:, :, 0].T, cmap='viridis')\n",
    "\n",
    "axes[0].set_xticks([])\n",
    "axes[1].set_xticks([])\n",
    "axes[0].set_yticks([])\n",
    "axes[1].set_yticks([])\n",
    "\n",
    "fig1.savefig(os.path.join(result_path, 'spatial_dynamics.png'))\n",
    "\n",
    "fig2, axes = plt.subplots(2, 1, figsize=(14*7, 24))\n",
    "axes[0].plot(t, np.cos(w0*t), t[inds[::n_step]], np.cos(w0*t[inds[::n_step]]), 'r.', linewidth=20, markersize=80)\n",
    "axes[1].plot(t, np.cos(w1*t + np.pi/4), t[inds[::n_step]], np.cos(w1*t[inds[::n_step]] + np.pi/4), 'r.', linewidth=20, markersize=80)\n",
    "#\n",
    "axes[0].set_xticks([])\n",
    "axes[1].set_xticks([])\n",
    "axes[0].set_yticks([])\n",
    "axes[1].set_yticks([])\n",
    "\n",
    "fig2.savefig(os.path.join(result_path, 'temporal_dynamics.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = {}\n",
    "for file_name in sorted(os.listdir(result_path)):\n",
    "    if file_name.endswith('.dat'):\n",
    "        key, _ = file_name.split('.')\n",
    "        with open(os.path.join(result_path, file_name), 'rb') as f: \n",
    "            records[key]= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_colors = 4\n",
    "colors = [(51/255, 51/255, 255/255)]+ \\\n",
    "         [(204/255, 153/255, 255/255), \n",
    "          (178/255, 102/255, 255/255),\n",
    "          (153/255, 51/255, 255/255),\n",
    "          (127/255, 0/255, 255/255),\n",
    "          (102/255, 0/255, 204/255),\n",
    "          (76/255, 0/255, 153/255)]\n",
    "\n",
    "fig1, ax1 = plt.subplots(nrows=1, ncols=1, figsize=(80, 16))\n",
    "\n",
    "# plot\n",
    "s = 0\n",
    "for i in range(3):\n",
    "    level_errs = records['val_errs'][i]\n",
    "    n_widens = len(level_errs)\n",
    "    ax1.axvline(x=s, color='k', linestyle='--', linewidth=20)\n",
    "    for j in range(n_widens):\n",
    "        op_err = level_errs[j]\n",
    "        ax1.plot(range(s, s + len(op_err)), np.log(op_err), color=colors[j], linewidth=20)\n",
    "        s += len(op_err)\n",
    "        \n",
    "ax1.axvline(x=s-1, color='k', linestyle='--', linewidth=20)\n",
    "\n",
    "ax1.xaxis.set_tick_params(labelsize=100)\n",
    "ax1.yaxis.set_tick_params(labelsize=100)\n",
    "\n",
    "fig1.savefig(os.path.join(result_path, 'err_iter_plot.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
