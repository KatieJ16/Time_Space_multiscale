{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ResNets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### created by Yuying Liu, 04/30/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is a template for training neural network time-steppers for different systems and different time scales. To reproduce the results in the paper, one needs to obtain all 11 neural network models for each nonlinear system under study. For setup details, please refer to Table 2 in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "module_path = os.path.abspath(os.path.join('../src/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import ResNet as net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjustables\n",
    "\n",
    "t = np.linspace(0, 8*np.pi, 500)\n",
    "k = 0                         # model index: should be in {0, 2, ..., 10}\n",
    "dt = t[1] - t[0]#0.01# 0.0005                     # time unit: 0.0005 for Lorenz and 0.01 for others\n",
    "system = 'toy1'#_4x4'         # system name: 'Hyperbolic', 'Cubic', 'VanDerPol', 'Hopf' or 'Lorenz'\n",
    "# total_dim = 16\n",
    "noise = 0.0                   # noise percentage: 0.00, 0.01 or 0.02\n",
    "\n",
    "lr = 1e-3                     # learning rate\n",
    "max_epoch = 100000            # the maximum training epoch \n",
    "batch_size = 320              # training batch size\n",
    "# arch = [total_dim, 128, 128, 128, total_dim]  # architecture of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 1, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "# paths\n",
    "data_dir = os.path.join('../data/', system)\n",
    "model_dir = os.path.join('../model/', system)\n",
    "\n",
    "level = 0\n",
    "filter_group_num = 0\n",
    "encoded = np.load(os.path.join(data_dir, 'data_L{}_{}.npy'.format(level, filter_group_num)))\n",
    "\n",
    "print(encoded.shape)\n",
    "n_steps, _, n_per_dim, _ = encoded.shape\n",
    "total_dim = n_per_dim **2\n",
    "arch = [total_dim, 128, 128, 128, total_dim] \n",
    "\n",
    "# global const\n",
    "n_forward = 5\n",
    "step_size = 2**k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 500, 4)\n",
      "(1, 2500, 9)\n",
      "(1, 1250, 9)\n",
      "(1, 1250, 9)\n",
      "1\n",
      "1\n",
      "1\n",
      "[[0.7202411  0.7316998  0.7302419  ... 0.7134938  0.724209   0.72017264]\n",
      " [0.72024053 0.7316991  0.73024106 ... 0.71343565 0.72415453 0.720172  ]\n",
      " [0.72023875 0.7316972  0.73023903 ... 0.71337503 0.72409767 0.7201702 ]\n",
      " ...\n",
      " [0.72023726 0.73169565 0.73023766 ... 0.71362925 0.7243348  0.7201687 ]\n",
      " [0.7202397  0.73169833 0.73024035 ... 0.71357685 0.7242862  0.72017115]\n",
      " [0.72024107 0.73169965 0.7302416  ... 0.7135221  0.72423524 0.7201725 ]]\n",
      "step_size*n_forward+1 =  6\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data = np.load(os.path.join(data_dir, \"train_data.npy\"))#inputs_2x2.npy\"))#'train_noise{}.npy'.format(noise)))\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "(100, 500, 4)\n",
    "train_data = encoded[:2500,0,:,:].reshape((1,2500,9))\n",
    "val_data = encoded[2500:3750,0,:,:].reshape((1,1250,9))\n",
    "test_data = encoded[3750:,0,:,:].reshape((1,1250,9))\n",
    "\n",
    "# print(np.load(\"../../data/Hyperbolic/train_noise0.0.npy\").shape)\n",
    "# data = np.expand_dims(data, 1)\n",
    "# train_data = data[:75]\n",
    "# val_data = data[75:90]\n",
    "# test_data = data[90:]\n",
    "# val_data = np.load(os.path.join(data_dir, 'val_noise{}.npy'.format(noise)))\n",
    "# test_data = np.load(os.path.join(data_dir, 'test_noise{}.npy'.format(noise)))\n",
    "print(train_data.shape)\n",
    "print(val_data.shape)\n",
    "print(test_data.shape)\n",
    "n_train = train_data.shape[0]\n",
    "n_val = val_data.shape[0]\n",
    "n_test = test_data.shape[0]\n",
    "print(n_train)\n",
    "print(n_val)\n",
    "print(n_test)\n",
    "\n",
    "print(train_data[0,:,:])\n",
    "\n",
    "print(\"step_size*n_forward+1 = \", step_size*n_forward+1)\n",
    "# create dataset object\n",
    "dataset = net.DataSet(train_data, val_data, test_data, dt, step_size, n_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(100):\n",
    "#     plt.plot(train_data[i,:,0], train_data[i,:,1], 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  0\n",
      "torch.Size([1, 9])\n",
      "create model model_D1_noise0.0_0.pt ...\n",
      "self.n_dim=  9\n",
      "dataset.n_dim =  9\n",
      "epoch 1000, training loss 1.2766795433327705e-11, validation loss 2.4129477929624166e-11\n",
      "(--> new model saved @ epoch 1000)\n",
      "--> model has reached an accuracy of 1e-8! Finished training!\n",
      "k =  1\n",
      "torch.Size([1, 9])\n",
      "create model model_D2_noise0.0_0.pt ...\n",
      "self.n_dim=  9\n",
      "dataset.n_dim =  9\n",
      "epoch 1000, training loss 1.8597998141522964e-10, validation loss 2.2648004305292346e-10\n",
      "(--> new model saved @ epoch 1000)\n",
      "--> model has reached an accuracy of 1e-8! Finished training!\n",
      "--> new model saved @ epoch 1001\n",
      "k =  2\n",
      "torch.Size([1, 9])\n",
      "create model model_D4_noise0.0_0.pt ...\n",
      "self.n_dim=  9\n",
      "dataset.n_dim =  9\n",
      "epoch 1000, training loss 1.874024935233365e-09, validation loss 1.969896690212636e-09\n",
      "(--> new model saved @ epoch 1000)\n",
      "--> model has reached an accuracy of 1e-8! Finished training!\n",
      "--> new model saved @ epoch 1001\n",
      "k =  3\n",
      "torch.Size([1, 9])\n",
      "create model model_D8_noise0.0_0.pt ...\n",
      "self.n_dim=  9\n",
      "dataset.n_dim =  9\n",
      "epoch 1000, training loss 3.3469355287252256e-08, validation loss 3.371189194467661e-08\n",
      "(--> new model saved @ epoch 1000)\n",
      "epoch 2000, training loss 4.546467025079437e-08, validation loss 4.265681852189118e-08\n",
      "epoch 3000, training loss 3.3862750825619514e-08, validation loss 3.413367366533748e-08\n",
      "epoch 4000, training loss 3.4795192505043815e-08, validation loss 3.500742806750168e-08\n",
      "epoch 5000, training loss 6.988456107137608e-07, validation loss 6.801846552662028e-07\n",
      "epoch 6000, training loss 3.348382193735233e-08, validation loss 3.3954858480456096e-08\n",
      "epoch 7000, training loss 3.207201970667484e-08, validation loss 3.226823963586867e-08\n",
      "(--> new model saved @ epoch 7000)\n",
      "epoch 8000, training loss 7.462679718628351e-08, validation loss 7.619411235282314e-08\n",
      "epoch 9000, training loss 3.4530675208088724e-08, validation loss 3.428028350072054e-08\n",
      "epoch 10000, training loss 3.079015087337211e-08, validation loss 3.097722256484303e-08\n",
      "(--> new model saved @ epoch 10000)\n",
      "epoch 11000, training loss 1.0282811899742228e-06, validation loss 1.0183489393966738e-06\n",
      "epoch 12000, training loss 3.021829897420503e-08, validation loss 3.011156479715282e-08\n",
      "(--> new model saved @ epoch 12000)\n",
      "epoch 13000, training loss 3.133086323714451e-08, validation loss 3.1819563872659273e-08\n",
      "epoch 14000, training loss 1.879510875824053e-07, validation loss 1.975119516828272e-07\n",
      "epoch 15000, training loss 3.010481819387678e-08, validation loss 3.028384298886522e-08\n",
      "epoch 16000, training loss 3.015712479736976e-08, validation loss 3.030450557162112e-08\n",
      "epoch 17000, training loss 2.988103275924914e-08, validation loss 3.014275407053901e-08\n",
      "epoch 18000, training loss 2.9859808847731983e-08, validation loss 3.02584908240533e-08\n",
      "epoch 19000, training loss 2.9442485782738004e-08, validation loss 2.9619595665053566e-08\n",
      "(--> new model saved @ epoch 19000)\n",
      "epoch 20000, training loss 3.501447309872674e-08, validation loss 3.611746635101554e-08\n",
      "epoch 21000, training loss 2.9392248634962925e-08, validation loss 2.9560105474502052e-08\n",
      "(--> new model saved @ epoch 21000)\n",
      "epoch 22000, training loss 2.9393191880444647e-08, validation loss 2.9512261079389646e-08\n",
      "(--> new model saved @ epoch 22000)\n",
      "epoch 23000, training loss 2.9361276077111143e-08, validation loss 2.955525602033049e-08\n",
      "epoch 24000, training loss 3.049269992061454e-08, validation loss 3.030394779557355e-08\n",
      "epoch 25000, training loss 3.378542245968674e-08, validation loss 3.446422169872676e-08\n",
      "epoch 26000, training loss 4.8405549790686564e-08, validation loss 5.267275327014431e-08\n",
      "epoch 27000, training loss 3.328544195824179e-08, validation loss 3.488137423346416e-08\n",
      "epoch 28000, training loss 2.909785834503964e-08, validation loss 2.9264764833669688e-08\n",
      "(--> new model saved @ epoch 28000)\n",
      "epoch 29000, training loss 3.1993717897194074e-08, validation loss 3.106726609303223e-08\n",
      "epoch 30000, training loss 5.327833463297793e-08, validation loss 5.5675894117257485e-08\n",
      "k =  4\n",
      "torch.Size([1, 9])\n",
      "create model model_D16_noise0.0_0.pt ...\n",
      "self.n_dim=  9\n",
      "dataset.n_dim =  9\n",
      "epoch 1000, training loss 3.539370538874209e-07, validation loss 3.521055873534351e-07\n",
      "(--> new model saved @ epoch 1000)\n",
      "epoch 2000, training loss 4.733751097774075e-07, validation loss 4.773770001520461e-07\n",
      "epoch 3000, training loss 3.4417328720337537e-07, validation loss 3.510920691951469e-07\n",
      "(--> new model saved @ epoch 3000)\n",
      "epoch 4000, training loss 3.109357749053743e-07, validation loss 3.088857738475781e-07\n",
      "(--> new model saved @ epoch 4000)\n",
      "epoch 5000, training loss 3.0426107855419104e-07, validation loss 3.0214957291718747e-07\n",
      "(--> new model saved @ epoch 5000)\n",
      "epoch 6000, training loss 3.443826415150397e-07, validation loss 3.4979348129127175e-07\n",
      "epoch 7000, training loss 3.035572149201471e-07, validation loss 3.014515925769956e-07\n",
      "(--> new model saved @ epoch 7000)\n",
      "epoch 8000, training loss 5.465041112984181e-07, validation loss 5.740163260270492e-07\n",
      "epoch 9000, training loss 3.143052822451864e-07, validation loss 3.125799423742137e-07\n",
      "epoch 10000, training loss 3.0675792572765204e-07, validation loss 3.047298378078267e-07\n",
      "epoch 11000, training loss 3.137949988740729e-07, validation loss 3.132764732072246e-07\n",
      "epoch 12000, training loss 3.110603188360983e-07, validation loss 3.0799168371231644e-07\n",
      "epoch 13000, training loss 3.0740116585548094e-07, validation loss 3.053692410048825e-07\n",
      "epoch 14000, training loss 3.5086895877611823e-07, validation loss 3.526039051848784e-07\n",
      "epoch 15000, training loss 3.0709614406987384e-07, validation loss 3.0506595294355066e-07\n",
      "epoch 16000, training loss 3.1749860340823943e-07, validation loss 3.198908302692871e-07\n",
      "epoch 17000, training loss 3.036336977402243e-07, validation loss 3.01699117244425e-07\n",
      "epoch 18000, training loss 4.056667251006729e-07, validation loss 4.1337662537443975e-07\n",
      "epoch 19000, training loss 3.3201655469383695e-07, validation loss 3.3202701388290734e-07\n",
      "epoch 20000, training loss 3.042532625840977e-07, validation loss 3.0232894232540275e-07\n",
      "epoch 21000, training loss 3.1683566703577526e-07, validation loss 3.1485893714489066e-07\n",
      "epoch 22000, training loss 2.980945055242046e-07, validation loss 2.960698282095109e-07\n",
      "(--> new model saved @ epoch 22000)\n",
      "epoch 23000, training loss 2.9495467401829956e-07, validation loss 2.9477462248905795e-07\n",
      "(--> new model saved @ epoch 23000)\n",
      "epoch 24000, training loss 2.921137536304741e-07, validation loss 2.900441984365898e-07\n",
      "(--> new model saved @ epoch 24000)\n",
      "epoch 25000, training loss 2.994878229856113e-07, validation loss 2.9742759011242015e-07\n",
      "epoch 26000, training loss 2.7952418690802006e-07, validation loss 2.773784046894434e-07\n",
      "(--> new model saved @ epoch 26000)\n",
      "epoch 27000, training loss 3.139352031666931e-07, validation loss 3.1419818924405263e-07\n",
      "epoch 28000, training loss 2.907216298808635e-07, validation loss 2.9361081033130176e-07\n",
      "epoch 29000, training loss 2.857363483599329e-07, validation loss 2.8446478950172605e-07\n",
      "epoch 30000, training loss 3.053529553653789e-07, validation loss 3.035862050637661e-07\n",
      "k =  5\n",
      "torch.Size([1, 9])\n",
      "create model model_D32_noise0.0_0.pt ...\n",
      "self.n_dim=  9\n",
      "dataset.n_dim =  9\n",
      "epoch 1000, training loss 3.0224084639485227e-06, validation loss 3.01409158964816e-06\n",
      "(--> new model saved @ epoch 1000)\n",
      "epoch 2000, training loss 3.85847579309484e-06, validation loss 3.895060217473656e-06\n",
      "epoch 3000, training loss 2.3692446120548993e-06, validation loss 2.360350663366262e-06\n",
      "(--> new model saved @ epoch 3000)\n",
      "epoch 4000, training loss 2.2704975890519563e-06, validation loss 2.266960336783086e-06\n",
      "(--> new model saved @ epoch 4000)\n",
      "epoch 5000, training loss 1.947693817783147e-06, validation loss 1.9360036276339088e-06\n",
      "(--> new model saved @ epoch 5000)\n",
      "epoch 6000, training loss 1.7291051790380152e-06, validation loss 1.7248155472771032e-06\n",
      "(--> new model saved @ epoch 6000)\n",
      "epoch 7000, training loss 3.1714039323560428e-06, validation loss 3.0839000828564167e-06\n",
      "epoch 8000, training loss 1.5006945659479243e-06, validation loss 1.4881176184644573e-06\n",
      "(--> new model saved @ epoch 8000)\n",
      "epoch 9000, training loss 2.121751776940073e-06, validation loss 2.10401117328729e-06\n",
      "epoch 10000, training loss 1.1487071560623008e-06, validation loss 1.1556493291209335e-06\n",
      "(--> new model saved @ epoch 10000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11000, training loss 7.071535605973622e-07, validation loss 6.753234060852265e-07\n",
      "(--> new model saved @ epoch 11000)\n",
      "epoch 12000, training loss 4.831653086512233e-07, validation loss 4.737930510145816e-07\n",
      "(--> new model saved @ epoch 12000)\n",
      "epoch 13000, training loss 3.3403318866476184e-06, validation loss 3.412713567740866e-06\n",
      "epoch 14000, training loss 5.624761456601846e-07, validation loss 5.518149919225834e-07\n",
      "epoch 15000, training loss 3.2369626978834276e-07, validation loss 3.155631418394478e-07\n",
      "(--> new model saved @ epoch 15000)\n",
      "epoch 16000, training loss 6.047761758054548e-07, validation loss 5.835442493662413e-07\n",
      "epoch 17000, training loss 4.081916245013417e-07, validation loss 3.9822649000598176e-07\n",
      "epoch 18000, training loss 2.283374982425812e-07, validation loss 2.2163706603350875e-07\n",
      "(--> new model saved @ epoch 18000)\n",
      "epoch 19000, training loss 6.01978285885707e-07, validation loss 6.265152023843257e-07\n",
      "epoch 20000, training loss 1.7863165169273998e-07, validation loss 1.731085745859673e-07\n",
      "(--> new model saved @ epoch 20000)\n",
      "epoch 21000, training loss 2.525347326809424e-07, validation loss 2.4543058430026576e-07\n",
      "epoch 22000, training loss 1.6317234496909805e-07, validation loss 1.581626918323309e-07\n",
      "(--> new model saved @ epoch 22000)\n",
      "epoch 23000, training loss 2.2495478901873867e-07, validation loss 2.1853949760952673e-07\n",
      "epoch 24000, training loss 1.9325264588587743e-07, validation loss 1.872223123200456e-07\n",
      "epoch 25000, training loss 1.5116241058876767e-07, validation loss 1.464271974782605e-07\n",
      "(--> new model saved @ epoch 25000)\n",
      "epoch 26000, training loss 1.9704862097569276e-07, validation loss 1.9104859916296846e-07\n",
      "epoch 27000, training loss 2.272538779379829e-07, validation loss 2.183113849696383e-07\n",
      "epoch 28000, training loss 9.8247994628764e-07, validation loss 9.91628894553287e-07\n",
      "epoch 29000, training loss 1.7212916247899557e-07, validation loss 1.66721093819433e-07\n",
      "epoch 30000, training loss 1.976452495000558e-07, validation loss 1.9225467440264765e-07\n"
     ]
    }
   ],
   "source": [
    "for k in [ 0, 1, 2, 3, 4, 5]:\n",
    "    print(\"k = \", k)\n",
    "    step_size = 2**k\n",
    "    dataset = net.DataSet(train_data, val_data, test_data, dt, step_size, n_forward)\n",
    "    print(dataset.train_x.shape)\n",
    "    model_name = 'model_D{}_noise{}_0.pt'.format(step_size, noise)\n",
    "\n",
    "    # create/load model object\n",
    "    try:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        model = torch.load(os.path.join(model_dir, model_name), map_location=device)\n",
    "        model.device = device\n",
    "    except:\n",
    "        print('create model {} ...'.format(model_name))\n",
    "        model = net.ResNet(arch=arch, dt=dt, step_size=step_size)\n",
    "\n",
    "    # training\n",
    "    model.train_net(dataset, max_epoch=30000, batch_size=batch_size, lr=lr,\n",
    "                    model_path=os.path.join(model_dir, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-a5d5b61aa8a6>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-a5d5b61aa8a6>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    .\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
