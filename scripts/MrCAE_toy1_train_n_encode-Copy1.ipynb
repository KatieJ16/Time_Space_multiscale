{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Model 1: 2 spatial modes with different oscillating frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Created by Yuying Liu, 09/23/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Phi(x, t) = u(x)cos(\\omega_0 t) + v(x)cos(\\omega_1 t + \\frac{\\pi}{4})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ']' (torch_cae_multilevel_V4.py, line 276)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\16kat\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3418\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-9a991e250dfc>\"\u001b[1;36m, line \u001b[1;32m16\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    import torch_cae_multilevel_V4 as net\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\src\\torch_cae_multilevel_V4.py\"\u001b[1;36m, line \u001b[1;32m276\u001b[0m\n\u001b[1;33m    encoded= self.forward(encoded, level-1).format(level)](decoded)\u001b[0m\n\u001b[1;37m                                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unmatched ']'\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../src/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "    \n",
    "import torch_cae_multilevel_V4 as net\n",
    "import ResNet as tnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MrCAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the progressive training framework. \n",
    "One could have flexible control over each training step: low-level models are cheap to obtain, and higher level models are built based on them -- one can always revert back to the previous level and adjust the parameters to re-train the model if it is not satisfying. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model & load data\n",
    "data_path = '../data/toy1_longer'#toy1.npy'\n",
    "model_path = '../model/toy1_longer'#pace_models'\n",
    "result_path = '../result/toy1_longer/'\n",
    "\n",
    "full_data_path = os.path.join(data_path, 'data.npy')\n",
    "\n",
    "dataset = net.MultiScaleDynamicsDataSet(full_data_path, n_levels=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "archs = [[1]]#,[1,3,5,7]]\n",
    "tols = [0.001]#, 0.0005]#, 0.0001]\n",
    "# net.train_net(archs=archs, dataset=dataset, max_epoch=3000, batch_size=350, \n",
    "#               tols=tols, activation=torch.nn.Sequential(), w=0.5, model_path=model_path, \n",
    "#               result_path=result_path, std=0.01, verbose=3)\n",
    "\n",
    "#start with deepening\n",
    "model = net.train_net_one_stage(mode=1, n_filters=1, dataset=dataset, max_epoch=3000, batch_size=350, result_path = result_path,\n",
    "                        load_model=None, tol=tols[0],std=0.01,  model_path=model_path, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.cur_level)\n",
    "all_data = dataset.obtain_data_at_current_level_all(model.cur_level)\n",
    "print(all_data.shape)\n",
    "train_data, val_data, test_data =dataset.obtain_data_at_current_level(model.cur_level)\n",
    "encoded = model.encode(all_data, model.cur_level)\n",
    "print(encoded.shape)\n",
    "train_encoded = model.encode(train_data, model.cur_level).reshape((1, len(train_data), len(encoded[0,0])**2))\n",
    "val_encoded = model.encode(val_data, model.cur_level).reshape((1, len(val_encoded), len(encoded[0,0])**2))\n",
    "test_encoded = model.encode(test_data, model.cur_level).reshape((1, len(test_encoded), len(encoded[0,0])**2))\n",
    "\n",
    "print(train_encoded.shape)\n",
    "print(val_encoded.shape)\n",
    "print(test_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded.shape)\n",
    "plt.imshow(encoded[0,0].detach().numpy())\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting time inputs\n",
    "# t = np.linspace(0, 8*np.pi, 500)\n",
    "k = 0                         # model index: should be in {0, 2, ..., 10}\n",
    "dt = 1#t[1] - t[0]              # time unit: 0.0005 for Lorenz and 0.01 for others\n",
    "system = 'toy1_longer'\n",
    "noise = 0.0                   # noise percentage: 0.00, 0.01 or 0.02\n",
    "\n",
    "lr = 1e-3                     # learning rate\n",
    "max_epochs = 10000            # the maximum training epoch \n",
    "batch_size = 320              # training batch size\n",
    "min_k = 7\n",
    "max_k = 8\n",
    "\n",
    "print(encoded.shape)\n",
    "n_steps, _, total_dim= val_encoded.shape\n",
    "# total_dim = n_per_dim **2\n",
    "print(\"total_dim = \", total_dim)\n",
    "arch = [total_dim, 128, 128, 128, total_dim] \n",
    "\n",
    "# global const\n",
    "n_forward = 5\n",
    "step_size = 2**k\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='none')\n",
    "# print(dt)\n",
    "# dataset = tnet.DataSet(train_encoded, val_encoded, test_encoded, dt, step_size, n_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do time training \n",
    "models = list()\n",
    "step_sizes = list()\n",
    "for k in range(min_k, max_k):\n",
    "    print(\"k = \", k)\n",
    "    step_size = 2**k\n",
    "    step_sizes.append(step_size)\n",
    "    dataset = tnet.DataSet(train_encoded, val_encoded, test_encoded, dt, step_size, n_forward)#tnet.DataSet(train_data, val_data, test_data, dt, step_size, n_forward)\n",
    "#     print(dataset.train_x.shape)\n",
    "    model_name = 'model_D{}_noise{}_0.pt'.format(step_size, noise)\n",
    "    model_path_this = os.path.join(model_path, model_name)\n",
    "\n",
    "    # create/load model object\n",
    "    try:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        model_time = torch.load(os.path.join(model_dir, model_name), map_location=device)\n",
    "        model_time.device = device\n",
    "    except:\n",
    "        print('create model {} ...'.format(model_name))\n",
    "        print('dt = ', dt)\n",
    "        model_time = tnet.ResNet(arch=arch, dt=dt, step_size=step_size)\n",
    "\n",
    "    # training\n",
    "    model_time.train_net(dataset, max_epoch=max_epochs, batch_size=batch_size, lr=lr,\n",
    "                    model_path=os.path.join(model_path, model_name))\n",
    "    \n",
    "# load models\n",
    "# for step_size in step_sizes:\n",
    "    print('load model_D{}.pt'.format(step_size))\n",
    "    models.append(torch.load(model_path_this, map_location='cpu'))\n",
    "    \n",
    "# fix model consistencies trained on gpus (optional)\n",
    "for model in models:\n",
    "    model.device = 'cpu'\n",
    "    model._modules['increment']._modules['activation'] = torch.nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uniscale time-stepping with NN\n",
    "preds_mse = list()\n",
    "times = list()\n",
    "\n",
    "for model in tqdm(models):\n",
    "    start = time.time()\n",
    "    y_preds = model.uni_scale_forecast(torch.tensor(val_encoded[:, 0, :]).float(), n_steps=n_steps)\n",
    "    print(y_preds.shape)\n",
    "    end = time.time()\n",
    "    times.append(end - start)\n",
    "    print('criterion(torch.tensor(val_encoded[:, 1:, :]).float(), y_preds) shape =', criterion(torch.tensor(val_encoded[:, 1:, :]).float(), y_preds).mean(-1).shape )\n",
    "    preds_mse.append(criterion(torch.tensor(val_encoded[:, 1:, :]).float(), y_preds).mean(-1))\n",
    "    print(len(preds_mse))\n",
    "    \n",
    "# visualize forecasting error at each time step    \n",
    "fig = plt.figure(figsize=(20, 8))\n",
    "colors=iter(plt.cm.rainbow(np.linspace(0, 1, max_k-min_k)))\n",
    "print(colors)\n",
    "for k in range(max_k-min_k):\n",
    "    err = preds_mse[k]\n",
    "    print('err.shape = ', err.shape)\n",
    "    mean = err[0].detach().numpy()\n",
    "    rgb = next(colors)\n",
    "    print(mean.shape)\n",
    "#     print(len(t))\n",
    "#     print(t)\n",
    "    plt.semilogy(mean, linestyle='-', color=rgb, linewidth=3.0, label='$\\Delta\\ t$={}dt'.format(step_sizes[k]))\n",
    "plt.legend(fontsize=20, loc='upper right')\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "plt.ylim([1e-5, 10])\n",
    "# plt.xlim([0, len(t)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained models at different levels\n",
    "models = {}\n",
    "print('model names: model_L{level}_{index}')\n",
    "for file_name in sorted(os.listdir(model_path)):\n",
    "    print(file_name)\n",
    "    model_name, _ = file_name.split('.')\n",
    "    print(model_name)\n",
    "    models[model_name] = torch.load(os.path.join(model_path, file_name))\n",
    "\n",
    "inds = np.array(sorted(dataset.test_inds))\n",
    "dataset.test_inds = inds\n",
    "n_snapshots = len(inds)\n",
    "n_samples = 6\n",
    "n_step = n_snapshots // 6\n",
    "\n",
    "model = models['model_L0_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['model_L0_0']\n",
    "i = 0\n",
    "\n",
    "_, _, data = dataset.obtain_data_at_current_level(level=1)\n",
    "print()\n",
    "# print(model.cur_level)\n",
    "output, _, _, _ = model(data[[0], :, :, :], model.cur_level, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_local_op(data, mode='conv', ave=True):\n",
    "    \"\"\"\n",
    "    :param data: data to be processed\n",
    "    :param device: which device is the data placed in?\n",
    "    :param mode: string, 'conv' or 'deconv'\n",
    "    :param ave: if to use local average or sample the center\n",
    "    :return: processed data\n",
    "    \"\"\"\n",
    "    in_channels, out_channels, n_per_dim, _ = data.size()\n",
    "    n = min(in_channels, out_channels)\n",
    "    print(\"data.size() = \", data.size())\n",
    "#     in_channels= 1\n",
    "#     out_channels = 1\n",
    "#     n = 1\n",
    "    op = torch.nn.Conv2d(out_channels, out_channels, 3, stride=2, padding=0)\n",
    "    op.weight.data = torch.zeros(op.weight.data.size())\n",
    "    print(\"op.weight.data = \", op.weight.data.size())\n",
    "    op.bias.data = torch.zeros(op.bias.data.size())\n",
    "    for i in range(n):\n",
    "        print(\"op.weight.data[i, i, 1, 1].size() = \", op.weight.data[i, i, 1, 1].size())\n",
    "        op.weight.data[i, i, 1, 1] = torch.ones(op.weight.data[i, i, 1, 1].size())\n",
    "\n",
    "    # make them non-trainable\n",
    "    for param in op.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    transformed = np.zeros((in_channels, out_channels, n_per_dim//2, n_per_dim//2))\n",
    "    print(\"transformed shape = \", transformed.shape)\n",
    "#     for i in range(in_channels//)\n",
    "#     print(op(data).shape)\n",
    "    return op(data)\n",
    "\n",
    "\n",
    "#save encoded data\n",
    "\n",
    "to_encode_path = os.path.join(data_path, 'data.npy')\n",
    "to_encode_raw = np.load(to_encode_path)\n",
    "print(\"to_encode_raw shape = \", to_encode_raw.shape)\n",
    "\n",
    "n_levels = 5\n",
    "level = 0\n",
    "to_encode = torch.tensor(to_encode_raw).unsqueeze(1).float()\n",
    "\n",
    "# to_encode_right_size = torch.zeros((to_encode.size()))\n",
    "\n",
    "# n_runs, _,_,_ = to_encode.size()\n",
    "\n",
    "# for r in range(n_runs):\n",
    "#     print(\"r = \", r)\n",
    "# to_encode_this = to_encode[0].unsqueeze(1).float()\n",
    "for i in range(n_levels - level - 1):\n",
    "    to_encode = apply_local_op((to_encode), ave=False)\n",
    "#     to_encode_right_size[r, :,0,:,:] = to_encode_data\n",
    "    \n",
    "print(\"to_encode shape = \", to_encode.shape)\n",
    "\n",
    "\n",
    "# return train_data\n",
    "    \n",
    "# np.save(full_data_path, scaled_Phi.T)\n",
    "\n",
    "# dataset_encode = net.MultiScaleDynamicsDataSet(to_encode_path, n_levels=5)\n",
    "\n",
    "level = 0\n",
    "filter_group_num = 0\n",
    "model = models['model_L{}_{}'.format(level, filter_group_num)]\n",
    "# data = dataset_encode.obtain_data_at_current_level_all(level=0)\n",
    "\n",
    "encoded = model.encode(to_encode, model.cur_level)#, verbose = True)\n",
    "# decoded = model.decode(encoded, model.cur_level)\n",
    "\n",
    "print(encoded.shape)\n",
    "# print(decoded.shape)\n",
    "\n",
    "#save encoded data \n",
    "np.save(os.path.join(data_path, 'data_L{}_{}'.format(level, filter_group_num)), encoded.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['model_L0_0']\n",
    "i = 0\n",
    "\n",
    "_, _, data = dataset.obtain_data_at_current_level(level=0)\n",
    "print(data.shape)\n",
    "# print(model.cur_level)\n",
    "output, _, _, _ = model(data[[0], :, :, :], model.cur_level, verbose = True)\n",
    "\n",
    "plt.imshow(output[-0,0].detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "encoded = model.encode(data[[0], :, :, :], model.cur_level)#, verbose = True)\n",
    "decoded = model.decode(encoded, model.cur_level)#, verbose = True)\n",
    "\n",
    "plt.imshow(decoded[-1,0].detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(encoded[0,0].detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "_, _, data = dataset.obtain_data_at_current_level(level=0)\n",
    "output, _, _, _ = model(data[[i*n_step], :, :, :], model.cur_level)\n",
    "\n",
    "# plt.imshow(data[0,0,])\n",
    "\n",
    "\n",
    "print(output.shape)\n",
    "encoded = model.encode(data[[i*n_step], :, :, :], model.cur_level)\n",
    "print(encoded.shape)\n",
    "decoded = model.decode(encoded, model.cur_level)\n",
    "print(decoded.shape)\n",
    "\n",
    "plt.imshow(output[0,0].detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(decoded[0,0].detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "print(output)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask(data, mask, mask_type='resolved', width=1):\n",
    "    \"\"\"\n",
    "    :param data: data to be processed\n",
    "    :param mask: mask, a 2D torch tensor of 0s and 1s\n",
    "    :param mask_type: resolved map or unresolved map\n",
    "    :param width: int, specify how large the region is\n",
    "    :return: a 4D torch tensor represents masked data\n",
    "    \"\"\"\n",
    "    if not isinstance(width, int):\n",
    "        raise ValueError('width should be a positive integer!')\n",
    "\n",
    "    # convert to unresolved mask\n",
    "    if mask_type == 'resolved':\n",
    "        mask = 1 - mask\n",
    "        print(mask)\n",
    "    elif mask_type == 'unresolved':\n",
    "        mask = mask\n",
    "    else:\n",
    "        raise ValueError('mask_type could only be resolved or unresolved!')\n",
    "\n",
    "    # expansion\n",
    "    dx = [i for i in range(-width, width+1)]\n",
    "    dy = [i for i in range(-width, width+1)]\n",
    "    \n",
    "    print(\"dx = \", dx)\n",
    "    print(\"dy = \", dy)\n",
    "    m, n = mask.size()\n",
    "    print(\"mask.nonzero() = \", mask.nonzero())\n",
    "    for c in mask.nonzero():\n",
    "        x, y = int(c[0]), int(c[1])\n",
    "        for i in range(2*width+1):\n",
    "            for j in range(2*width+1):\n",
    "#                 print(i, \": \", j)\n",
    "                if 0 <= x + dx[i] < m and 0 <= y + dy[j] < n:\n",
    "#                     print(\"x + dx[i] =\", x + dx[i])\n",
    "                    mask[x + dx[i], y + dy[j]] = 1\n",
    "\n",
    "    # apply\n",
    "#     print(\"data shape = \", data.shape)\n",
    "#     print(\"mask.unsqueeze(0).unsqueeze(0).float() shape = \", mask.unsqueeze(0).unsqueeze(0).float().shape)\n",
    "    print(\"mask.unsqueeze(0).unsqueeze(0).float()= \", mask.unsqueeze(0).unsqueeze(0).float())\n",
    "    masked_data = data * mask.unsqueeze(0).unsqueeze(0).float()\n",
    "    return masked_data\n",
    "\n",
    "# x = torch.tensor([[[[1, 2, 3, 4, 5, 6], [4, 5, 6, 7, 8, 9, ], [7, 8, 9, 10, 11, 12]]]])\n",
    "# mask = torch.tensor([[0,0,0,0,0,0]]\n",
    "\n",
    "print(x.shape)\n",
    "print(mask.size())\n",
    "masked_x = apply_mask(x, mask)\n",
    "print(masked_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained models at different levels\n",
    "models = {}\n",
    "print('model names: model_L{level}_{index}')\n",
    "for file_name in sorted(os.listdir(model_path)):\n",
    "    model_name, _ = file_name.split('.')\n",
    "    print(model_name)\n",
    "    models[model_name] = torch.load(os.path.join(model_path, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.resolved_maps['0']['0'].cpu().detach().numpy().shape)\n",
    "print(model.resolved_maps['2']['3'].cpu().detach().numpy().shape)\n",
    "\n",
    "print(model.parameters())\n",
    "# for param in model.parameters():\n",
    "#     print(param)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print( name ,\" : \", param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the finest model\n",
    "model = models['model_L2_3']\n",
    "\n",
    "# resolved maps at different levels (that suggest poorly reconstructed regions)\n",
    "for i in range(3):\n",
    "    print(model.resolved_maps[str(i)].keys())\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.pcolor(model.resolved_maps['0']['0'].cpu().detach().numpy(), cmap='binary', vmin=0, vmax=1)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "fig.savefig(os.path.join(result_path, 'L0_I0.png'))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.pcolor(model.resolved_maps['0']['1'].cpu().detach().numpy(), cmap='binary', vmin=0, vmax=1)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "fig.savefig(os.path.join(result_path, 'L0_I1.png'))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.pcolor(model.resolved_maps['1']['0'].cpu().detach().numpy(), cmap='binary', vmin=0, vmax=1)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "fig.savefig(os.path.join(result_path, 'L1_I0.png'))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.pcolor(model.resolved_maps['1']['1'].cpu().detach().numpy(), cmap='binary', vmin=0, vmax=1)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "fig.savefig(os.path.join(result_path, 'L1_I1.png'))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.pcolor(model.resolved_maps['1']['2'].cpu().detach().numpy(), cmap='binary', vmin=0, vmax=1)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "fig.savefig(os.path.join(result_path, 'L1_I2.png'))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.pcolor(model.resolved_maps['2']['0'].cpu().detach().numpy(), cmap='binary', vmin=0, vmax=1)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "fig.savefig(os.path.join(result_path, 'L2_I0.png'))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.pcolor(model.resolved_maps['2']['1'].cpu().detach().numpy(), cmap='binary', vmin=0, vmax=1)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "fig.savefig(os.path.join(result_path, 'L2_I1.png'))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.pcolor(model.resolved_maps['2']['2'].cpu().detach().numpy(), cmap='binary', vmin=0, vmax=1)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "fig.savefig(os.path.join(result_path, 'L2_I2.png'))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.pcolor(model.resolved_maps['2']['3'].cpu().detach().numpy(), cmap='binary', vmin=0, vmax=1)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "fig.savefig(os.path.join(result_path, 'L2_I3.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.array(sorted(dataset.test_inds))\n",
    "dataset.test_inds = inds\n",
    "n_snapshots = len(inds)\n",
    "n_samples = 6\n",
    "n_step = n_snapshots // 6\n",
    "\n",
    "model = models['model_L2_3']\n",
    "fig, axes = plt.subplots(1, n_samples, figsize=(n_samples*11, 10))\n",
    "plt.subplots_adjust(wspace=0.01)\n",
    "for i in range(n_samples):\n",
    "    _, _, data = dataset.obtain_data_at_current_level(level=2)\n",
    "    output, _, _, _ = model(data[[i*n_step], :, :, :], model.cur_level)\n",
    "    axes[i].pcolor(output.squeeze().cpu().detach().numpy(), cmap='viridis')\n",
    "    axes[i].set_xticks([])\n",
    "    axes[i].set_yticks([])\n",
    "fig.savefig(os.path.join(result_path, 'test_L2_reconstructions.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models['model_L2_3']\n",
    "\n",
    "_, _, data = dataset.obtain_data_at_current_level(level=4)\n",
    "i = 0\n",
    "\n",
    "model = models['model_L0_0']\n",
    "encoded = model.encode(data[[i*n_step], :, :, :], model.cur_level)[0]\n",
    "print(encoded.shape)\n",
    "plt.imshow(encoded[0].detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show() \n",
    "\n",
    "\n",
    "x = model(data[[i*n_step], :, :, :], model.cur_level)[0]\n",
    "print(x.shape)\n",
    "plt.imshow(x[0,0].detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show() \n",
    "\n",
    "\n",
    "\n",
    "# model = models['model_L2_1']\n",
    "# encoded = model.encode(data[[i*n_step], :, :, :], model.cur_level)[0]\n",
    "# print(encoded.shape)\n",
    "# plt.imshow(encoded[0].detach().numpy())\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "\n",
    "# model = models['model_L2_2']\n",
    "# encoded = model.encode(data[[i*n_step], :, :, :], model.cur_level)[0]\n",
    "# print(encoded.shape)\n",
    "# plt.imshow(encoded[0].detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['model_L0_3']# plt.colorbar()\n",
    "print(len(model.resolved_maps))\n",
    "print(model.resolved_maps.keys())\n",
    "\n",
    "# print(model.resolved_maps['0']['0'])\n",
    "plt.imshow(model.loss_each_stage['0']['0'].detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(model.resolved_maps['0']['0'])\n",
    "plt.show()\n",
    "plt.imshow(model.loss_each_stage['0']['1'].detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(model.resolved_maps['0']['1'])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(model.loss_each_stage['0']['2'].detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(model.resolved_maps['0']['2'])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(model.loss_each_stage['0']['3'].detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(model.resolved_maps['0']['3'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstructions of test snapshots\n",
    "\n",
    "inds = np.array(sorted(dataset.test_inds))\n",
    "dataset.test_inds = inds\n",
    "n_snapshots = len(inds)\n",
    "n_samples = 6\n",
    "n_step = n_snapshots // 6\n",
    "\n",
    "fig, axes = plt.subplots(1, n_samples, figsize=(n_samples*11, 10))\n",
    "plt.subplots_adjust(wspace=0.01)\n",
    "for i in range(n_samples):\n",
    "    axes[i].pcolor(dataset.data[inds[i*n_step], :, :, :].squeeze().cpu().detach().numpy(), cmap='viridis')\n",
    "    axes[i].set_xticks([])\n",
    "    axes[i].set_yticks([])\n",
    "fig.savefig(os.path.join(result_path, 'test_samples.png'), bbox_inches='tight')\n",
    "\n",
    "\n",
    "model = models['model_L0_1']\n",
    "fig, axes = plt.subplots(1, n_samples, figsize=(n_samples*11, 10))\n",
    "plt.subplots_adjust(wspace=0.01)\n",
    "for i in range(n_samples):\n",
    "    _, _, data = dataset.obtain_data_at_current_level(level=0)\n",
    "    output, _, _, _ = model(data[[i*n_step], :, :, :], model.cur_level)\n",
    "    axes[i].pcolor(output.squeeze().cpu().detach().numpy(), cmap='viridis')\n",
    "    axes[i].set_xticks([])\n",
    "    axes[i].set_yticks([])\n",
    "fig.savefig(os.path.join(result_path, 'test_L0_reconstructions.png'), bbox_inches='tight')\n",
    "\n",
    "\n",
    "model = models['model_L1_2']\n",
    "fig, axes = plt.subplots(1, n_samples, figsize=(n_samples*11, 10))\n",
    "plt.subplots_adjust(wspace=0.01)\n",
    "for i in range(n_samples):\n",
    "    _, _, data = dataset.obtain_data_at_current_level(level=1)\n",
    "    output, _, _, _ = model(data[[i*n_step], :, :, :], model.cur_level)\n",
    "    axes[i].pcolor(output.squeeze().cpu().detach().numpy(), cmap='viridis')\n",
    "    axes[i].set_xticks([])\n",
    "    axes[i].set_yticks([])\n",
    "fig.savefig(os.path.join(result_path, 'test_L1_reconstructions.png'), bbox_inches='tight')\n",
    "\n",
    "\n",
    "model = models['model_L2_3']\n",
    "fig, axes = plt.subplots(1, n_samples, figsize=(n_samples*11, 10))\n",
    "plt.subplots_adjust(wspace=0.01)\n",
    "for i in range(n_samples):\n",
    "    _, _, data = dataset.obtain_data_at_current_level(level=2)\n",
    "    output, _, _, _ = model(data[[i*n_step], :, :, :], model.cur_level)\n",
    "    axes[i].pcolor(output.squeeze().cpu().detach().numpy(), cmap='viridis')\n",
    "    axes[i].set_xticks([])\n",
    "    axes[i].set_yticks([])\n",
    "fig.savefig(os.path.join(result_path, 'test_L2_reconstructions.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig1, axes = plt.subplots(2, 1, figsize=(8, 16))\n",
    "axes[0].pcolor(xgrid[:,:,0], ygrid[:,:,0], phi1(xgrid, ygrid, tgrid)[:, :, 0].T, cmap='viridis')\n",
    "axes[1].pcolor(xgrid[:,:,0], ygrid[:,:,0], phi2(xgrid, ygrid, tgrid)[:, :, 0].T, cmap='viridis')\n",
    "\n",
    "axes[0].set_xticks([])\n",
    "axes[1].set_xticks([])\n",
    "axes[0].set_yticks([])\n",
    "axes[1].set_yticks([])\n",
    "\n",
    "fig1.savefig(os.path.join(result_path, 'spatial_dynamics.png'))\n",
    "\n",
    "fig2, axes = plt.subplots(2, 1, figsize=(14*7, 24))\n",
    "axes[0].plot(t, np.cos(w0*t), t[inds[::n_step]], np.cos(w0*t[inds[::n_step]]), 'r.', linewidth=20, markersize=80)\n",
    "axes[1].plot(t, np.cos(w1*t + np.pi/4), t[inds[::n_step]], np.cos(w1*t[inds[::n_step]] + np.pi/4), 'r.', linewidth=20, markersize=80)\n",
    "#\n",
    "axes[0].set_xticks([])\n",
    "axes[1].set_xticks([])\n",
    "axes[0].set_yticks([])\n",
    "axes[1].set_yticks([])\n",
    "\n",
    "fig2.savefig(os.path.join(result_path, 'temporal_dynamics.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = {}\n",
    "for file_name in sorted(os.listdir(result_path)):\n",
    "    if file_name.endswith('.dat'):\n",
    "        key, _ = file_name.split('.')\n",
    "        with open(os.path.join(result_path, file_name), 'rb') as f: \n",
    "            records[key]= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_colors = 4\n",
    "colors = [(51/255, 51/255, 255/255)]+ \\\n",
    "         [(204/255, 153/255, 255/255), \n",
    "          (178/255, 102/255, 255/255),\n",
    "          (153/255, 51/255, 255/255),\n",
    "          (127/255, 0/255, 255/255),\n",
    "          (102/255, 0/255, 204/255),\n",
    "          (76/255, 0/255, 153/255)]\n",
    "\n",
    "fig1, ax1 = plt.subplots(nrows=1, ncols=1, figsize=(80, 16))\n",
    "\n",
    "# plot\n",
    "s = 0\n",
    "for i in range(3):\n",
    "    level_errs = records['val_errs'][i]\n",
    "    n_widens = len(level_errs)\n",
    "    ax1.axvline(x=s, color='k', linestyle='--', linewidth=20)\n",
    "    for j in range(n_widens):\n",
    "        op_err = level_errs[j]\n",
    "        ax1.plot(range(s, s + len(op_err)), np.log(op_err), color=colors[j], linewidth=20)\n",
    "        s += len(op_err)\n",
    "        \n",
    "ax1.axvline(x=s-1, color='k', linestyle='--', linewidth=20)\n",
    "\n",
    "ax1.xaxis.set_tick_params(labelsize=100)\n",
    "ax1.yaxis.set_tick_params(labelsize=100)\n",
    "\n",
    "fig1.savefig(os.path.join(result_path, 'err_iter_plot.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
