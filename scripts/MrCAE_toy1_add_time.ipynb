{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Model 1: 2 spatial modes with different oscillating frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Created by Yuying Liu, 09/23/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Phi(x, t) = u(x)cos(\\omega_0 t) + v(x)cos(\\omega_1 t + \\frac{\\pi}{4})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "KeyboardInterrupt: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b88b4ddcabf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;31m# Appease the type checker; ordinarily this binding is inserted by the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: KeyboardInterrupt: "
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../../multiscale_HiTS/src/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import utils\n",
    "import ResNet as tnet\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../src/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import torch_cae_multilevel_V4 as net\n",
    "from utils_MrCAE import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #make amd save data\n",
    "# # params\n",
    "# w0 = 0.5\n",
    "# w1 = 4.0\n",
    "# sigma0 = 10.0\n",
    "# sigma1 = 0.25\n",
    "\n",
    "# # define two modes\n",
    "# def phi1(x, y, t):\n",
    "#     return 1./np.cosh((x+1)/sigma0)/np.cosh((y-1)/sigma0)*np.cos(w0*t)\n",
    "\n",
    "# def phi2(x, y, t):\n",
    "#     return 1./(sigma1*np.sqrt(2*np.pi))*np.exp(-((x-1)**2+(y+1)**2)/(2*sigma1**2))*np.cos(w1*t + np.pi/4)\n",
    "\n",
    "# # mesh grids\n",
    "# x = np.linspace(-5, 5, 127)\n",
    "# y = np.linspace(-5, 5, 127)\n",
    "# t = np.linspace(0, 40*np.pi, 500)\n",
    "# xgrid, ygrid, tgrid = np.meshgrid(x, y, t)\n",
    "\n",
    "# # Phi = phi1(xgrid, ygrid, tgrid) + phi2(xgrid, ygrid, tgrid)\n",
    "# # print(Phi.shape)\n",
    "# # plt.plot(Phi[0,0])\n",
    "\n",
    "\n",
    "# # # generate data shape (100,500,127,127)\n",
    "# n_train = 500\n",
    "# n_val = 100\n",
    "# train_data = np.zeros((n_train,500,127,127))\n",
    "# for i in tqdm(range(n_train)):\n",
    "#     #t start somewhere between zero and 2 pi\n",
    "#     start_t = np.random.rand(1) * 2*np.pi\n",
    "# #     print(start_t)\n",
    "#     t = np.linspace(0, 40*np.pi, 500) + start_t\n",
    "#     xgrid, ygrid, tgrid = np.meshgrid(x, y, t)\n",
    "#     Phi = phi1(xgrid, ygrid, tgrid) + phi2(xgrid, ygrid, tgrid)\n",
    "#     scaled_Phi = (Phi - Phi.min()) / (Phi.max() - Phi.min())\n",
    "#     train_data[i] = scaled_Phi.T\n",
    "    \n",
    "# print(\"saving...\")\n",
    "# np.save('../data/toy1_train_data.npy',train_data)\n",
    "# val_data = np.zeros((n_val,500,127,127))\n",
    "# for i in tqdm(range(n_val)):\n",
    "#     #t start somewhere between zero and 2 pi\n",
    "#     start_t = np.random.rand(1) * 2*np.pi\n",
    "#     t = np.linspace(0, 40*np.pi, 500) + start_t\n",
    "#     xgrid, ygrid, tgrid = np.meshgrid(x, y, t)\n",
    "#     Phi = phi1(xgrid, ygrid, tgrid) + phi2(xgrid, ygrid, tgrid)\n",
    "#     scaled_Phi = (Phi - Phi.min()) / (Phi.max() - Phi.min())\n",
    "#     val_data[i] = scaled_Phi.T\n",
    "    \n",
    "# print(\"saving ...\")\n",
    "# np.save('../data/toy1_val_data.npy',val_data)\n",
    "# # scaled_Phi = (Phi - Phi.min()) / (Phi.max() - Phi.min())\n",
    "# # print(Phi.shape)\n",
    "\n",
    "\n",
    "# # #normalize \n",
    "\n",
    "# # train_data = (train_data - train_data.min()) / (train_data.max() - train_data.min())\n",
    "# # print(train_data.shape)\n",
    "# # val_data = (val_data - val_data.min()) / (val_data.max() - val_data.min())\n",
    "\n",
    "# # np.save(data_path, scaled_Phi.T)\n",
    "\n",
    "# # np.save('../data/toy1_train_data.npy',train_data)\n",
    "# # np.save('../data/toy1_val_data.npy',val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MrCAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "\n",
    "train_data = np.load('../data/toy1_train_data.npy')\n",
    "val_data = np.load('../data/toy1_val_data.npy')\n",
    "\n",
    "# plt.plot(train_data[0,:,0,0])\n",
    "# plt.plot(val_data[0,:,0,0])\n",
    "\n",
    "plt.plot(train_data[0,:,72,45])\n",
    "plt.show()\n",
    "plt.imshow(train_data[1,52])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the progressive training framework. \n",
    "One could have flexible control over each training step: low-level models are cheap to obtain, and higher level models are built based on them -- one can always revert back to the previous level and adjust the parameters to re-train the model if it is not satisfying. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model & load data\n",
    "# print(train_data.shape)\n",
    "data_path = '../data/toy1.npy'\n",
    "# np.save(data_path, train_data[:,0])\n",
    "# data_path = '../data/toy1_train_data.npy'\n",
    "model_path = '../model/toy1_space/'\n",
    "result_path = '../result/toy1_space/' \n",
    "\n",
    "# data = np.load(data_path)\n",
    "# print(data.shape)\n",
    "\n",
    "dataset = net.MultiScaleDynamicsDataSet(data_path, n_levels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, _,_ = dataset.obtain_data_at_current_level(0)\n",
    "\n",
    "# print(train_data.shape)\n",
    "# plt.imshow(train_data[0,0].cpu().detach().numpy(), vmin = 0.3, vmax=0.6)\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset.train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training in space\n",
    "# archs = [[1,2],[1,2],[1,3]]#,3,4],[1,2,3,4],[1,3,5,7]]\n",
    "# tols = [ 0.001, 0.0005, 0.0001]\n",
    "# net.train_net(archs=archs, dataset=dataset, max_epoch=2000, batch_size=350, \n",
    "#               tols=tols, activation=torch.nn.Sequential(), w=0.5, model_path=model_path, \n",
    "#               result_path=result_path, std=0.01, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained models at different levels\n",
    "models_space = {}\n",
    "print('model names: model_L{level}_{index}')\n",
    "for file_name in sorted(os.listdir(model_path)):\n",
    "    model_name, _ = file_name.split('.')\n",
    "    print(model_name)\n",
    "    models_space[model_name] = torch.load(os.path.join(model_path, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define some functions\n",
    "def obtain_data_at_current_level(data, level,n_levels):\n",
    "#         train_data = self.data[self.train_inds].to(self.device)\n",
    "#         val_data = self.data[self.val_inds].to(self.device)\n",
    "#         test_data = self.data[self.test_inds].to(self.device)\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        for i in range(n_levels - level - 1):\n",
    "            data = apply_local_op(data.to(device), device, ave=False)\n",
    "\n",
    "        return data\n",
    "    \n",
    "    \n",
    "def encode(self, x, level, query_in_out_each_level=False, query_hidden=False):\n",
    "        \"\"\"\n",
    "        :param x: a 4D input of NN\n",
    "        :param level: level index\n",
    "        :param query_in_out_each_level: if to query the input/output at each level\n",
    "        (maybe used for enforce losses at different levels)\n",
    "        :param query_hidden: if to query hidden representations\n",
    "        :return: output of NN, a list of hidden representations at current level\n",
    "        \"\"\"\n",
    "        # collectors\n",
    "        all_hidden = dict()\n",
    "        all_inputs = dict()\n",
    "        all_outputs = dict()\n",
    "\n",
    "        # forward prop\n",
    "        assert level >= 0, print('level index should be a non-negative integer!')\n",
    "        resolved_maps_dict = self.resolved_maps[str(level)]\n",
    "\n",
    "        if self.n_filter_groups_each_level['0'] == 1:\n",
    "            if query_in_out_each_level:\n",
    "                all_inputs['0'] = x\n",
    "            encoded = self._modules['L{}_Conv_0'.format(level)](x)\n",
    "\n",
    "            return encoded\n",
    "        #else:\n",
    "        i = self.n_filter_groups_each_level['0']-1\n",
    "#         print('L{}_Conv_{}'.format(level, i))\n",
    "        encoded = self._modules['L{}_Conv_{}'.format(level, i)](x)\n",
    "        if self.use_maps:\n",
    "            masked_encoded = apply_mask(encoded, resolved_maps_dict[str(i - 1)])\n",
    "        else:\n",
    "            masked_encoded = encoded\n",
    "        return masked_encoded\n",
    "    \n",
    "def decode(self, encoded, level, y=None, query_in_out_each_level=False, query_hidden=False):\n",
    "        \"\"\"\n",
    "        :param x: a 4D input of NN\n",
    "        :param level: level index\n",
    "        :param query_in_out_each_level: if to query the input/output at each level\n",
    "        (maybe used for enforce losses at different levels)\n",
    "        :param query_hidden: if to query hidden representations\n",
    "        :return: output of NN, a list of hidden representations at current level\n",
    "        \"\"\"\n",
    "        # collectors\n",
    "        all_hidden = dict()\n",
    "        all_inputs = dict()\n",
    "        all_outputs = dict()\n",
    "\n",
    "        # forward prop\n",
    "        assert level >= 0, print('level index should be a non-negative integer!')\n",
    "#         resolved_maps_dict = self.resolved_maps[str(level)]\n",
    "\n",
    "        if self.n_filter_groups_each_level['0'] == 1:\n",
    "            if query_hidden:\n",
    "                all_hidden['L0_0'] = encoded\n",
    "            # ----- pad -----\n",
    "            encoded = torch.nn.functional.pad(encoded, (1, 1, 1, 1), 'replicate')\n",
    "            # ---------------\n",
    "            y = self._modules['L{}_deConv_0'.format(level)](encoded)\n",
    "            # chop off the boundaries\n",
    "            y = y[:, :, 2:-2, 2:-2]\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            i =self.n_filter_groups_each_level['0'] -1\n",
    "            y = self._modules['L{}_deConv_{}'.format(level, i)](encoded)\n",
    "            \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models_space['model_L0_0']\n",
    "\n",
    "vmin=0.3\n",
    "vmax=0.6\n",
    "\n",
    "print(\"train_data shape = \", train_data.shape)\n",
    "train_data_this = obtain_data_at_current_level(torch.tensor(train_data[0]).unsqueeze(1).float(), 0,3)\n",
    "plt.imshow(train_data_this[0,0].cpu().detach().numpy(), vmin=vmin, vmax=vmax)\n",
    "plt.title(\"Train data L0_0\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "encoded = encode(model, train_data_this.to('cuda').float(), 0)\n",
    "print(encoded.shape)\n",
    "plt.imshow(encoded[0,0].cpu().detach().numpy())\n",
    "plt.title(\"Encoded at L0_0\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(encoded[:,0,9,5].cpu().detach().numpy(), label = \"(9,5)\")\n",
    "plt.plot(encoded[:,0,0,0].cpu().detach().numpy(), label = \"(0,0)\")\n",
    "plt.title(\"Encoded at L0_0\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "decoded = decode(model, encoded, 0)\n",
    "plt.imshow(decoded[0,0].cpu().detach().numpy(), vmin=vmin, vmax=vmax)\n",
    "plt.colorbar()\n",
    "plt.title(\"Decoded at L0_0\")\n",
    "plt.show()\n",
    "\n",
    "print(\"train_data_this = \", train_data_this.shape)\n",
    "forward,_,_,_ = model(torch.tensor(train_data_this[:,0]).unsqueeze(1).to('cuda').float(), 0)\n",
    "plt.imshow(forward[0,0].cpu().detach().numpy(), vmin=vmin, vmax=vmax)\n",
    "plt.title(\"Forward L0_0\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "print(forward.shape)\n",
    "plt.plot(train_data_this[:,0,18,12].cpu().detach().numpy(), label = \"truth\")\n",
    "plt.plot(forward[:,0,18,12].cpu().detach().numpy(), label = \"predicted\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"predicted vs truth on point (18,12)\")\n",
    "plt.show()\n",
    "\n",
    "print(train_data_this[:,0].shape)\n",
    "x = train_data_this[:,0].cpu().detach().numpy()\n",
    "y = forward[:,0].cpu().detach().numpy()\n",
    "# print(x.shape)\n",
    "# print(y.shape)\n",
    "# print((np.mean((x-y)**2, axis = (1,2))))\n",
    "plt.semilogy(np.max((x-y)**2, axis = (1,2)))\n",
    "plt.title(\"MSE\")\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(train_data[0,0], vmin=vmin, vmax=vmax)\n",
    "plt.title(\"Truth\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(model.resolved_maps['0']['0'].cpu().detach().numpy(), 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #encode the data \n",
    "model = models_space['model_L0_0']\n",
    "n_train, n_timesteps, _,_ = train_data.shape\n",
    "train_encoded = torch.zeros((n_train, n_timesteps, 15,15))\n",
    "val_encoded = torch.zeros((len(val_data), n_timesteps, 15,15))\n",
    "data = torch.tensor(train_data).unsqueeze(2).float()\n",
    "\n",
    "val_data_layer0 = torch.zeros((len(val_data), n_timesteps, 31,31))\n",
    "train_data_layer0 = torch.zeros((len(train_data), n_timesteps, 31,31))\n",
    "for i in tqdm(range(len(train_data))):\n",
    "    data_this = obtain_data_at_current_level(data[i], 0,3)\n",
    "    train_data_layer0[i] = data_this[:,0]\n",
    "    encoded = encode(model, data_this, 0)\n",
    "    train_encoded[i] = encoded[:,0]\n",
    "\n",
    "data = torch.tensor(val_data).unsqueeze(2).float()\n",
    "for i in tqdm(range(len(val_data))):\n",
    "    data_this = obtain_data_at_current_level(data[i], 0,3)\n",
    "#     print(data_this.shape)\n",
    "    val_data_layer0[i] = data_this[:,0]\n",
    "    encoded = encode(model, data_this, 0)\n",
    "    val_encoded[i] = encoded[:,0]\n",
    "    \n",
    "#plot to make sure they are relatively the same \n",
    "plt.plot(val_encoded[1,:,0,0].cpu().detach().numpy())\n",
    "\n",
    "plt.plot(train_encoded[0,:,0,0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing the time training\n",
    "print(\"inside train_one_timestep\")\n",
    "i=None\n",
    "j=None\n",
    "step_size = 2\n",
    "dt=1\n",
    "n_forward = 5\n",
    "arch = [225, 225*2, 225*2, 225*2, 225]\n",
    "batch_size = 500\n",
    "lr = 0.001\n",
    "# if (i is not None) and (j is not None):\n",
    "#     model_name = 'model_L{}_D{}_noise{}_i{}_j{}.pt'.format(current_size, step_size, noise, i, j)\n",
    "# else:\n",
    "model_name = 'model_L0_0_{}.pt'.format(step_size)#{}_D{}_noise{}.pt'.format(current_size, step_size, noise)\n",
    "model_path_this = os.path.join(model_path, model_name)\n",
    "\n",
    "dataset = utils.DataSet(train_encoded.flatten(start_dim=2), val_encoded.flatten(start_dim=2), val_encoded.flatten(start_dim=2), dt, step_size, n_forward)\n",
    "print('create model {} ...'.format(model_name))\n",
    "model = tnet.ResNet(arch=arch, dt=dt, step_size=step_size)\n",
    "\n",
    "# # training\n",
    "# model.train_net(dataset, max_epoch=30000, batch_size=batch_size, lr=lr,\n",
    "#             model_path=os.path.join(model_path, model_name))\n",
    "\n",
    "# # try: #if we already have a model saved\n",
    "# #     if make_new:\n",
    "# #         print(\"Making a new model. Old one deleted. model {}\".format(model_name))\n",
    "# #         assert False\n",
    "# #     model_time = torch.load(model_path_this)\n",
    "# #     print(\"model loaded: \", model_name)\n",
    "# #     if dont_train: #just load model, no training\n",
    "# #         print(\"Model not trained more\")\n",
    "# # #         return model_time\n",
    "# # except:\n",
    "# print('create model {} ...'.format(model_path_this))\n",
    "# model_time = tnet.ResNet(train_data, val_data, step_size,\n",
    "#                          model_name=model_name, model_dir=model_path, n_inputs=n_inputs,\n",
    "#                          n_hidden_nodes=20, n_hidden_layers=5, out_dim=15*15,\n",
    "#                          activation=nn.ReLU(), n_epochs=max_epochs,\n",
    "#                          threshold=threshold, print_every=print_every,\n",
    "#                          save_every=save_every)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"device = \", device)\n",
    "# model_time.to(device)\n",
    "\n",
    "# criterion = torch.nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model_time.parameters())\n",
    "\n",
    "# model_time.train_model(optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models\n",
    "step_sizes = [2,8]\n",
    "models_time = list()\n",
    "for step_size in step_sizes:\n",
    "    print('model_L0_0_{}.pt'.format(step_size))\n",
    "    models_time.append(torch.load(os.path.join(model_path,'model_L0_0_{}.pt'.format(step_size)),map_location='cpu'))\n",
    "#     models.append(torch.load(os.path.join(model_path, 'model_D{}_noise0.0.pt'.format(step_size)), map_location='cpu'))\n",
    "\n",
    "# fix model consistencies trained on gpus (optional)\n",
    "for model in models_time:\n",
    "    model.device = 'cpu'\n",
    "    model._modules['increment']._modules['activation'] = torch.nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniscale time-stepping with NN\n",
    "preds_mse = list()\n",
    "times = list()\n",
    "\n",
    "# test_data = val_data\n",
    "ks = [1]\n",
    "n_steps = val_encoded.shape[1] - 1\n",
    "t = [dt*(step+1) for step in range(n_steps)]\n",
    "criterion = torch.nn.MSELoss(reduction='none')\n",
    "\n",
    "test_data = torch.tensor(val_encoded).flatten(start_dim=2).float()\n",
    "print(test_data.shape)\n",
    "\n",
    "n_steps = 499\n",
    "for model in tqdm(models_time):\n",
    "#     start = time.time()\n",
    "    print(val_encoded.shape)\n",
    "    print(val_encoded[:, 0, :].flatten(start_dim=1).float().shape)\n",
    "    print(torch.tensor(val_encoded[:, 0, :]).flatten(start_dim=1).float().is_cuda)\n",
    "    y_preds = model.uni_scale_forecast(torch.tensor(val_encoded[:, 0, :]).flatten(start_dim=1).float(), n_steps=n_steps)\n",
    "#     end = time.time()\n",
    "#     times.append(end - start)\n",
    "    preds_mse.append(criterion(torch.tensor(val_encoded[:, 1:, :]).flatten(start_dim=2).float(), y_preds).mean(-1))\n",
    "    for i in [1,140]:#range(1):\n",
    "        \n",
    "        plt.plot(test_data[1, 1::2, i].detach().numpy(),label=\"Truth\")\n",
    "        plt.plot(y_preds[1,::2,i], label=\"predicted\")\n",
    "        x = y_preds[1,::model.step_size,i]\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "#         plt.plot(np.arange(len(x))*model.step_size,x, '.k')\n",
    "#     plt.xlim([0,200])\n",
    "    plt.legend()\n",
    "    plt.title(\"step size = \"+ str(model.step_size))\n",
    "    plt.show()\n",
    "    \n",
    "# visualize forecasting error at each time step    \n",
    "fig = plt.figure(figsize=(20, 8))\n",
    "colors=iter(plt.cm.rainbow(np.linspace(0, 1, len(step_sizes))))\n",
    "for k in range(len(preds_mse)):\n",
    "    err = preds_mse[k]\n",
    "    mean = err.mean(0).detach().numpy()\n",
    "    rgb = next(colors)\n",
    "    plt.plot(t, np.log10(mean), linestyle='-', color=rgb, linewidth=3.0, label='$\\Delta\\ t$={}'.format(step_sizes[k]*dt))\n",
    "plt.legend(fontsize=20, loc='upper right')\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# y_preds = model.uni_scale_forecast(torch.tensor(val_encoded[:, 0, :]).flatten(start_dim=1).float(), n_steps=n_steps)\n",
    "y_preds = tnet.vectorized_multi_scale_forecast(torch.tensor(val_encoded[:, 0, :]).flatten(start_dim=1).float(), n_steps=n_steps, models=models_time)\n",
    "# end = time.time()\n",
    "# multiscale_time = end - start\n",
    "multiscale_preds_mse = criterion(torch.tensor(test_data[:, 1:, :]).float(), y_preds).mean(-1)\n",
    "\n",
    "# visualize forecasting error at each time step    \n",
    "fig = plt.figure(figsize=(30, 10))\n",
    "colors=iter(plt.cm.rainbow(np.linspace(0, 1, len(step_sizes))))\n",
    "multiscale_err = multiscale_preds_mse.mean(0).detach().numpy()\n",
    "for k in range(len(step_sizes)):#len(preds_mse)):\n",
    "    err = preds_mse[k]\n",
    "    mean = err.mean(0).detach().numpy()\n",
    "    rgb = next(colors)\n",
    "    plt.semilogy(mean, linestyle='-', color=rgb, linewidth=5, label='$\\Delta\\ t$={}dt'.format(step_sizes[k]))\n",
    "plt.semilogy(multiscale_err, linestyle='-', color='k', linewidth=6, label='multiscale')\n",
    "plt.legend(fontsize=30, loc='upper center', ncol=6, bbox_to_anchor=(0.5, 1.2))\n",
    "plt.xticks(fontsize=60)\n",
    "plt.yticks(fontsize=60)\n",
    "\n",
    "# plt.ylim([1e-4, 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(val_encoded).flatten(start_dim=2)[0,1:,140].cpu().detach().numpy())\n",
    "for model in tqdm(models_time):\n",
    "    y_preds = model.uni_scale_forecast(torch.tensor(val_encoded[:, 0, :]).flatten(start_dim=1).float(), n_steps=n_steps)\n",
    "    plt.plot(y_preds[0,:,140].cpu().detach().numpy())\n",
    "\n",
    "y_preds = tnet.vectorized_multi_scale_forecast(torch.tensor(val_encoded[:, 0, :]).flatten(start_dim=1).float(), n_steps=n_steps, models=models_time)\n",
    "    \n",
    "plt.plot(y_preds[0,:,140].cpu().detach().numpy())\n",
    "plt.xlim([0,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(val_encoded).flatten(start_dim=2)[0,1:,0].cpu().detach().numpy())\n",
    "for model in tqdm(models_time):\n",
    "    y_preds = model.uni_scale_forecast(torch.tensor(val_encoded[:, 0, :]).flatten(start_dim=1).float(), n_steps=n_steps)\n",
    "    plt.plot(y_preds[0,:,0].cpu().detach().numpy())\n",
    "\n",
    "y_preds = tnet.vectorized_multi_scale_forecast(torch.tensor(val_encoded[:, 0, :]).flatten(start_dim=1).float(), n_steps=n_steps, models=models_time)\n",
    "    \n",
    "plt.plot(y_preds[0,:,0].cpu().detach().numpy())\n",
    "plt.plot([8,8], [0.5, 0.8], 'k')\n",
    "plt.xlim([0,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin = 0.5\n",
    "vmax = 0.6\n",
    "t = 7\n",
    "plt.imshow(val_encoded[0,t+1].cpu().detach().numpy(),vmin=vmin, vmax=vmax)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "# plt.plot(torch.tensor(val_encoded).flatten(start_dim=2)[0,1:,0].cpu().detach().numpy())\n",
    "for model in models_time:\n",
    "    print(model.step_size)\n",
    "    y_preds = model.uni_scale_forecast(torch.tensor(val_encoded[:, 0, :]).flatten(start_dim=1).float(), n_steps=n_steps)\n",
    "    plt.imshow(y_preds[0,t].reshape((15,15)).cpu().detach().numpy(),vmin=vmin, vmax=vmax)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "y_preds = tnet.vectorized_multi_scale_forecast(torch.tensor(val_encoded[:, 0, :]).flatten(start_dim=1).float(), n_steps=n_steps, models=models_time)\n",
    "    \n",
    "plt.imshow(y_preds[0,t].reshape((15,15)).cpu().detach().numpy(),vmin=vmin, vmax=vmax)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finding_index = torch.zeros((15,15))\n",
    "finding_index[8,5] = 1\n",
    "finding_index[9,5] = 1\n",
    "finding_index[8,6] = 1\n",
    "finding_index[9,6] = 1\n",
    "plt.imshow(finding_index.detach().numpy())\n",
    "finding_index = finding_index.flatten()\n",
    "\n",
    "print(finding_index.nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the vectorized just change the 4 squares\n",
    "import scipy\n",
    "def vectorized_multi_scale_forecast(x_init, n_steps, models):\n",
    "    \"\"\"\n",
    "    :param x_init: initial state torch array of shape n_test x n_dim\n",
    "    :param n_steps: number of steps forward in terms of dt\n",
    "    :param models: a list of models\n",
    "    :return: a torch array of size n_test x n_steps x n_dim,\n",
    "             a list of indices that are not achieved by interpolations\n",
    "    \"\"\"\n",
    "    # sort models by their step sizes (decreasing order)\n",
    "    step_sizes = [model.step_size for model in models]\n",
    "    models = [model for _, model in sorted(zip(step_sizes, models), reverse=True)]\n",
    "\n",
    "    # we assume models are sorted by their step sizes (decreasing order)\n",
    "    n_test, n_dim = x_init.shape\n",
    "#     device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    device='cpu'\n",
    "    indices = list()\n",
    "    extended_n_steps = n_steps + models[0].step_size\n",
    "    preds = torch.zeros(n_test, extended_n_steps + 1, n_dim).float().to(device)\n",
    "\n",
    "    # vectorized simulation\n",
    "    indices.append(0)\n",
    "    preds[:, 0, :] = x_init\n",
    "    total_step_sizes = n_steps\n",
    "    \n",
    "    idx_list = [125, 126,140, 141]\n",
    "    for model in models:\n",
    "#         if model.step_size < 8:\n",
    "#             n_forward = int(total_step_sizes/model.step_size)\n",
    "#             y_prev = preds[:, indices, :].reshape(-1, n_dim)\n",
    "#             indices_lists = [indices]\n",
    "#             for t in range(n_forward):\n",
    "#                 y_next = model(y_prev.to(device)).to(device)\n",
    "#     #             print(\"y_next is cuda = \", y_preds.is_cuda)\n",
    "#                 shifted_indices = [x + (t + 1) * model.step_size for x in indices]\n",
    "#                 indices_lists.append(shifted_indices)\n",
    "#                 for idx in idx_list:\n",
    "#                      preds[:, shifted_indices, idx] = y_next[:,idx].reshape(n_test, -1)\n",
    "#                 y_prev = y_next\n",
    "#             indices = [val for tup in zip(*indices_lists) for val in tup]\n",
    "#             total_step_sizes = model.step_size - 1\n",
    "#         else:\n",
    "            n_forward = int(total_step_sizes/model.step_size)\n",
    "            y_prev = preds[:, indices, :].reshape(-1, n_dim)\n",
    "            indices_lists = [indices]\n",
    "            for t in range(n_forward):\n",
    "\n",
    "                y_next = model(y_prev.to(device)).to(device)\n",
    "    #             print(\"y_next is cuda = \", y_preds.is_cuda)\n",
    "                shifted_indices = [x + (t + 1) * model.step_size for x in indices]\n",
    "                indices_lists.append(shifted_indices)\n",
    "                preds[:, shifted_indices, :] = y_next.reshape(n_test, -1, n_dim)\n",
    "                y_prev = y_next\n",
    "            indices = [val for tup in zip(*indices_lists) for val in tup]\n",
    "            total_step_sizes = model.step_size - 1\n",
    "            \n",
    "             # interpolations\n",
    "            sample_steps = range(1, n_steps+1-8)\n",
    "            valid_preds = preds[:, indices, :].detach().numpy()\n",
    "            cs = scipy.interpolate.interp1d(indices, valid_preds, kind='linear', axis=1)\n",
    "            y_preds = torch.tensor(cs(sample_steps)).float()\n",
    "\n",
    "    # simulate the tails\n",
    "    last_idx = indices[-1]\n",
    "    y_prev = preds[:, last_idx, :]\n",
    "    while last_idx < n_steps:\n",
    "        last_idx += models[-1].step_size\n",
    "        y_next = models[-1](y_prev)\n",
    "        preds[:, last_idx, :] = y_next\n",
    "        indices.append(last_idx)\n",
    "        y_prev = y_next\n",
    "\n",
    "    # interpolations\n",
    "    sample_steps = range(1, n_steps+1)\n",
    "    valid_preds = preds[:, indices, :].detach().numpy()\n",
    "    cs = scipy.interpolate.interp1d(indices, valid_preds, kind='linear', axis=1)\n",
    "    y_preds = torch.tensor(cs(sample_steps)).float()\n",
    "\n",
    "    return y_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = vectorized_multi_scale_forecast(torch.tensor(val_encoded[:, 0, :]).flatten(start_dim=1).float(), n_steps=n_steps, models=models_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(val_encoded).flatten(start_dim=2)[0,1:,140].cpu().detach().numpy())\n",
    "for model in tqdm(models_time):\n",
    "    y_preds = model.uni_scale_forecast(torch.tensor(val_encoded[0:1, 0, :]).flatten(start_dim=1).float(), n_steps=n_steps)\n",
    "    plt.plot(np.arange(len(y_preds[0,::8,140]))* 8, y_preds[0,::8,140].cpu().detach().numpy(), '.')\n",
    "\n",
    "y_preds = vectorized_multi_scale_forecast(torch.tensor(val_encoded[0:1, 0, :]).flatten(start_dim=1).float(), n_steps=n_steps, models=models_time)\n",
    "    \n",
    "plt.plot(y_preds[0,:,140].cpu().detach().numpy())\n",
    "plt.plot([8,8], [0.5, 0.8], 'k')\n",
    "plt.xlim([0,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_preds.shape)\n",
    "print(np.argmax(y_preds[0,0]))\n",
    "plt.plot(test_data[0,:,140].cpu().detach().numpy())\n",
    "plt.plot(y_preds[0,:,140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models_time[0]\n",
    "#get encoded versions of validation adn training\n",
    "print(torch.tensor(test_data[:, 0, :]).float().shape)\n",
    "y_preds = model.uni_scale_forecast(torch.tensor(test_data[:, 0, :]).float(), n_steps=n_steps)\n",
    "print(test_data.shape)\n",
    "print(y_preds.shape)\n",
    "encoded_t1_val = torch.reshape(y_preds, (100,499,15,15))\n",
    "# print(encoded_t1.shape)\n",
    "print(val_encoded.shape)\n",
    "\n",
    "print(torch.tensor(train_encoded[:,0,:]).flatten(start_dim=2).float().shape)\n",
    "y_preds = model.uni_scale_forecast(torch.tensor(train_encoded[:,0,:]).flatten(start_dim=1).float(), n_steps=n_steps)\n",
    "print(y_preds.shape)\n",
    "encoded_t1_train = torch.reshape(y_preds, (len(y_preds),n_steps,15,15))\n",
    "print(encoded_t1_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(encoded_t1_val[0,96])#, vmin=0.4, vmax=0.6)\n",
    "plt.colorbar()\n",
    "plt.title(\"encoded time t+96 predicted\")\n",
    "plt.show()\n",
    "\n",
    "print(encoded.shape)\n",
    "plt.imshow(val_encoded[0,97].cpu().detach().numpy())#, vmin=0.4, vmax=0.6)\n",
    "plt.title(\"encoded time t+96 truth\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "print(torch.reshape(val_encoded, (100,500,15,15)).shape)\n",
    "print(encoded_t1_val.shape)\n",
    "mse = np.mean(((torch.reshape(val_encoded, (100,500,15,15))[:,1:] - encoded_t1_val)**2).cpu().detach().numpy(), axis=(0,2,3))\n",
    "plt.xlabel(\"time\")\n",
    "plt.semilogy(mse)\n",
    "plt.show()\n",
    "\n",
    "print(torch.reshape(val_encoded, (100,500,15,15))[:,1:].shape)\n",
    "print(encoded_t1_val.shape)\n",
    "mse = np.mean(((torch.reshape(val_encoded, (100,500,15,15))[:,1:] - encoded_t1_val)**2).cpu().detach().numpy(), axis=(0,1))\n",
    "plt.imshow(mse)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # trained models at different levels\n",
    "# models_space = {}\n",
    "# print('model names: model_L{level}_{index}')\n",
    "# for file_name in sorted(os.listdir(model_path)):\n",
    "#     model_name, _ = file_name.split('.')\n",
    "#     print(model_name)\n",
    "#     models_space[model_name] = torch.load(os.path.join(model_path, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models['model_L0_0']\n",
    "# print(encoded_t1.shape)\n",
    "# decoded = decode(model, encoded_t1,0)\n",
    "\n",
    "# #decode the data \n",
    "model = models_space['model_L0_0']\n",
    "print(encoded_t1_train.shape)\n",
    "n_train, n_timesteps, _,_ = train_data.shape\n",
    "train_decoded = torch.zeros((n_train, n_timesteps-1, 31,31))\n",
    "val_decoded = torch.zeros((len(val_data), n_timesteps-1, 31,31))\n",
    "data = encoded_t1_train.unsqueeze(2).float().to(device)\n",
    "for i in tqdm(range(len(train_data))):\n",
    "#     print(data[i].shape)\n",
    "#     data_this = obtain_data_at_current_level(data[i], 0,3)\n",
    "    decoded = decode(model, data[i], 0)\n",
    "    print(decoded.shape)\n",
    "#     print(train_encoded.shape)\n",
    "#     print(encoded.shape)\n",
    "    train_decoded[i] = decoded[:,0]\n",
    "\n",
    "# data = torch.tensor(val_data).unsqueeze(2).float()\n",
    "data = encoded_t1_val.unsqueeze(2).float().to(device)\n",
    "for i in tqdm(range(len(val_data))):\n",
    "#     print(data[i].shape)\n",
    "#     data_this = obtain_data_at_current_level(data[i], 0,3)\n",
    "    decoded = decode(model, data[i], 0)\n",
    "    val_decoded[i] = decoded[:,0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin = 0.35#torch.min(val_decoded[0,0])\n",
    "vmax = 0.4#torch.max(val_decoded[0,0])\n",
    "\n",
    "plt.imshow(val_decoded[0,96].cpu().detach().numpy(), vmin=vmin, vmax=vmax)\n",
    "plt.title(\"Decoded t+96 predicted\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "data = torch.tensor(train_data[0]).unsqueeze(1).float()\n",
    "print(data.shape)\n",
    "data_this = obtain_data_at_current_level(data, 0,3)\n",
    "plt.imshow(val_data_layer0[0,97].cpu().detach().numpy(),vmin=vmin, vmax=vmax)\n",
    "plt.colorbar()\n",
    "plt.title(\"t+96 truth\")\n",
    "plt.show()\n",
    "\n",
    "# print(train_decoded[0].shape)\n",
    "# print(data_this[1:,0].shape)\n",
    "\n",
    "print(val_data_layer0.shape)\n",
    "print(val_decoded.shape)\n",
    "mse = np.mean(((val_decoded.cpu() - val_data_layer0[:,1:].cpu())**2).cpu().detach().numpy(), axis=(0,2,3))\n",
    "plt.xlabel(\"time\")\n",
    "plt.semilogy(mse)\n",
    "plt.title(\"mse for decoded data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_data_layer0[0,:,18,12])\n",
    "plt.plot(val_decoded[0,:,18,12].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleDynamicsDataSet():\n",
    "    def __init__(self, train_data, val_data, train_data_output, val_data_output, n_levels, map_path=None, train_ratio=0.7, valid_ratio=0.2):\n",
    "        # load data\n",
    "#         data = np.load(data_path)\n",
    "#         self.data = torch.tensor(data).unsqueeze(1).float()\n",
    "#         #\n",
    "#         if map_path is not None:\n",
    "#             map_data = 1 - np.load(map_path)\n",
    "#             self.map_data = torch.tensor(map_data).float()\n",
    "#         else:\n",
    "        \n",
    "\n",
    "        self.nt, self.nx, self.ny = train_data.shape\n",
    "        # partition\n",
    "        indices = np.arange(self.nt)\n",
    "        np.random.shuffle(indices)\n",
    "        n_train = int(train_ratio*self.nt)\n",
    "        n_val = int(valid_ratio*self.nt)\n",
    "        self.n_train = n_train\n",
    "        self.n_val = n_val\n",
    "        self.n_test = self.nt - n_train - n_val\n",
    "        self.train_inds = indices[:n_train]\n",
    "        self.val_inds = indices[n_train:n_train+n_val]\n",
    "        self.test_inds = indices[n_train+n_val:]\n",
    "        #\n",
    "        self.n_levels = n_levels\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#         self.map_data = self.map_data.to(self.device)\n",
    "        \n",
    "        self.data = torch.tensor(train_data).unsqueeze(1).float().to('cuda') \n",
    "        self.map_data = torch.ones(self.data.shape[-2:]).float().to('cuda')\n",
    "        self.train_data = torch.tensor(train_data).unsqueeze(1).float() \n",
    "        self.val_data = torch.tensor(val_data).unsqueeze(1).float() \n",
    "        \n",
    "        self.train_data_output = torch.tensor(train_data_output).unsqueeze(1).float() \n",
    "        self.val_data_output = torch.tensor(val_data_output).unsqueeze(1).float() \n",
    "        \n",
    "\n",
    "    def obtain_data_at_current_level(self, level):\n",
    "        train_data = self.train_data#self.data[self.train_inds].to(self.device)\n",
    "        val_data = self.val_data#self.data[self.val_inds].to(self.device)\n",
    "        test_data = self.val_data#self.data[self.test_inds].to(self.device)\n",
    "\n",
    "        for _ in range(self.n_levels - level - 1):\n",
    "            train_data = apply_local_op(train_data, self.device, ave=False)\n",
    "            val_data = apply_local_op(val_data, self.device, ave=False)\n",
    "            test_data = apply_local_op(test_data, self.device, ave=False)\n",
    "\n",
    "        return train_data, val_data, test_data\n",
    "    \n",
    "    def obtain_data_at_current_level_output(self, level):\n",
    "        train_data = self.train_data_output#self.data[self.train_inds].to(self.device)\n",
    "        val_data = self.val_data_output#self.data[self.val_inds].to(self.device)\n",
    "        test_data = self.val_data_output#self.data[self.test_inds].to(self.device)\n",
    "\n",
    "        for _ in range(self.n_levels - level - 1):\n",
    "            train_data = apply_local_op(train_data, self.device, ave=False)\n",
    "            val_data = apply_local_op(val_data, self.device, ave=False)\n",
    "            test_data = apply_local_op(test_data, self.device, ave=False)\n",
    "\n",
    "        return train_data, val_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_arch(self, dataset, max_epoch, batch_size,\n",
    "                   tol=None, lr=1e-3, w=0.5, verbose=1):\n",
    "        \"\"\"\n",
    "        :param dataset: a MultiScaleDynamicsDataSet object\n",
    "        :param max_epoch: maximum number of epochs\n",
    "        :param batch_size: batch size\n",
    "        :param tol: error tolerance (default is None)\n",
    "        :param lr: learning rate\n",
    "        :param w: w: loss = w * l2_loss + (1-w) * l_inf_loss\n",
    "        :param verbose: verbose level\n",
    "        :return: a list of train_losses, val_losses and timings\n",
    "        \"\"\"\n",
    "        # prepare data at this level\n",
    "        train_data, val_data, _ = dataset.obtain_data_at_current_level(self.cur_level)\n",
    "        output_train, output_val = dataset.obtain_data_at_current_level_output(self.cur_level)\n",
    "\n",
    "        # specify optimizer\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr, eps=1e-3, weight_decay=1e-5)\n",
    "        criterion = torch.nn.MSELoss(reduction='none')\n",
    "\n",
    "        # collectors\n",
    "        val_losses = list()\n",
    "        train_losses = list()\n",
    "        max_pos_set = set()\n",
    "\n",
    "        # training\n",
    "        epoch = 0\n",
    "        ave_loss_old = 1e+10\n",
    "        best_local_val_err = 1e+10\n",
    "        best_state_dict = self.state_dict()\n",
    "        while epoch < max_epoch:\n",
    "            epoch += 1\n",
    "            # =================== forward =====================\n",
    "            new_idxs = torch.randperm(dataset.n_train)\n",
    "            batch_idxs = new_idxs[:batch_size]\n",
    "            batch_train_data = train_data[batch_idxs, :, :, :]\n",
    "            output, _, _, _ = self.forward(batch_train_data, self.cur_level)\n",
    "            output_val, _, _, _ = self.forward(val_data, self.cur_level)\n",
    "            # =============== calculate losses ================\n",
    "            mean_loss_train = criterion(output, batch_train_data).mean()\n",
    "            max_loss_train = criterion(output, batch_train_data).mean(0).max()\n",
    "            assert 0 <= w <= 1, print('w should between 0 and 1 (inclusive)!')\n",
    "            loss = w * mean_loss_train + (1 - w) * max_loss_train\n",
    "            mean_loss_val = criterion(output_val, val_data).mean()\n",
    "            max_loss_val = criterion(output_val, val_data).mean(0).max()\n",
    "            loss_val = w * mean_loss_val + (1 - w) * max_loss_val\n",
    "            if loss_val.item() < best_local_val_err:\n",
    "                best_local_val_err = loss_val.item()\n",
    "                best_state_dict = self.state_dict()\n",
    "            # compute global scale losses\n",
    "            global_mean_loss, global_max_loss, _ = self.compute_global_loss(dataset, output, self.cur_level, dataset.train_inds[batch_idxs])\n",
    "            global_mean_val_loss, global_max_val_loss, tup = self.compute_global_loss(dataset, output_val, self.cur_level, dataset.val_inds)\n",
    "            global_loss = w * global_mean_loss + (1-w) * global_max_loss\n",
    "            global_val_loss = w * global_mean_val_loss + (1-w) * global_max_val_loss\n",
    "            # =================== backward ====================\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # ================= collect stat ==================\n",
    "            train_losses.append(global_loss.item())\n",
    "            val_losses.append(global_val_loss.item())\n",
    "            max_pos_set.add(tup)\n",
    "            # =================== log =========================\n",
    "            # 1st epoch\n",
    "            if epoch == 1 and verbose:\n",
    "                # init err\n",
    "                print('losses printing format: local: mse/max/overall, global: mse/max/overall')\n",
    "                print('epoch [1/{}]'.format(max_epoch))\n",
    "                print('[training set] local: {:.4f}/{:.4f}/{:.4f}, global: {:.4f}/{:.4f}/{:.4f}'.format(mean_loss_train.item(), max_loss_train.item(), loss.item(), global_mean_loss.item(), global_max_loss.item(), global_loss.item()))\n",
    "                print('[validation set] local: {:.4f}/{:.4f}/{:.4f}, global: {:.4f}/{:.4f}/{:.4f}'.format(mean_loss_val.item(), max_loss_val.item(), loss_val.item(), global_mean_val_loss.item(), global_max_val_loss.item(), global_val_loss.item()))\n",
    "            # every 1/10 max_epoch\n",
    "            if epoch % (max_epoch // 10) == 0:\n",
    "                print('epoch [{}/{}]:'.format(epoch, max_epoch))\n",
    "                print('[training set] local: {:.4f}/{:.4f}/{:.4f}, global: {:.4f}/{:.4f}/{:.4f}'.format(mean_loss_train.item(), max_loss_train.item(), loss.item(), global_mean_loss.item(), global_max_loss.item(), global_loss.item()))\n",
    "                print('[validation set] local: {:.4f}/{:.4f}/{:.4f}, global: {:.4f}/{:.4f}/{:.4f}'.format(mean_loss_val.item(), max_loss_val.item(), loss_val.item(), global_mean_val_loss.item(), global_max_val_loss.item(), global_val_loss.item()))\n",
    "                # check for early stopping\n",
    "                if tol is not None:\n",
    "                    # check if fully resolved?\n",
    "                    train_output, _, _, _ = self.forward(train_data, self.cur_level)\n",
    "                    fully_resolved, _, _ = check_pixel_level_loss(train_data, train_output, tol=tol, device=self.device, w=0.5)\n",
    "                    if fully_resolved and epoch < max_epoch:\n",
    "                        print('early stopping at {}th iteration due to satisfying reconstruction!'.format(epoch))\n",
    "                        break\n",
    "                if epoch > max_epoch // 2 and epoch < max_epoch:\n",
    "                    ave_loss = np.mean(train_losses[-(max_epoch // 10):])\n",
    "                    if (ave_loss_old - ave_loss) / ave_loss_old < 1e-3:\n",
    "                        # improvement is so small that we consider it as convergence\n",
    "                        print('early stopping at {}th iteration due to slow convergence!'.format(epoch))\n",
    "                        break\n",
    "                    ave_loss_old = ave_loss\n",
    "\n",
    "        # calculate the best validation error\n",
    "        self.load_state_dict(best_state_dict)\n",
    "        output_val, _, _, _ = self.forward(val_data, self.cur_level)\n",
    "        global_mean_val_loss, global_max_val_loss, _ = self.compute_global_loss(dataset, output_val, self.cur_level, dataset.val_inds)\n",
    "        global_val_loss = w * global_mean_val_loss + (1-w) * global_max_val_loss\n",
    "        best_val_loss = global_val_loss.item()\n",
    "        # check this level is clear if tolerance threshold is enabled\n",
    "        filter_index = self.n_filter_groups_each_level[str(self.cur_level)] - 1\n",
    "        train_output, _, _, _ = self.forward(train_data, self.cur_level)\n",
    "        if tol is not None:\n",
    "            _, _, resolved_map = check_pixel_level_loss(train_data, train_output, tol=tol, device=self.device, w=0.5)\n",
    "            self.resolved_maps[str(self.cur_level)][str(filter_index)] = resolved_map.float()\n",
    "            if resolved_map.all():\n",
    "                self.level_clear[str(self.cur_level)] = True\n",
    "\n",
    "        return self, val_losses, best_val_loss, max_pos_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for next layer, we are going to have val_decoded as input and val_data_layer0 as output\n",
    "\n",
    "print(train_decoded[:,0].shape)\n",
    "print(val_decoded[:,0].shape)\n",
    "dataset = MultiScaleDynamicsDataSet(train_decoded[:,0].to('cuda'), val_decoded[:,0].to('cuda'), \n",
    "                                    train_data_layer0[:,0].to('cuda'), val_data_layer0[:,0].to('cuda'), n_levels=1)\n",
    "\n",
    "# # training in space\n",
    "archs = [[1]]#,3,4],[1,2,3,4],[1,3,5,7]]\n",
    "tols = [ 0.001]#, 0.0005, 0.0001]\n",
    "net.train_net(archs=archs, dataset=dataset, max_epoch=2000, batch_size=350, \n",
    "              tols=tols, activation=torch.nn.Sequential(), w=0.5, model_path=model_path, \n",
    "              result_path=result_path, std=0.01, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all models (don't actually need all of them)\n",
    "\n",
    "# trained models at different levels\n",
    "models_space = {}\n",
    "print('model names: model_L{level}_{index}')\n",
    "for file_name in sorted(os.listdir(model_path)):\n",
    "    model_name, _ = file_name.split('.')\n",
    "    print(model_name)\n",
    "    models_space[model_name] = torch.load(os.path.join(model_path, file_name))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models_space['model_L0_0']\n",
    "\n",
    "data = torch.tensor(train_data[0]).unsqueeze(1).float()\n",
    "print(data.shape)\n",
    "data_this = obtain_data_at_current_level(data, 0,3)\n",
    "\n",
    "print(train_decoded[:,0:1].shape)\n",
    "forwarded,_,_,_ = model.forward(train_decoded[0].unsqueeze(1).to('cuda'), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(forwarded.shape)\n",
    "\n",
    "plt.imshow(forwarded[0,0].cpu().detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(train_decoded[0,0].cpu().detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(forwarded[:,0,0,0].cpu().detach().numpy())\n",
    "plt.plot(train_decoded[0,:,0,0].cpu().detach().numpy())\n",
    "# plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(val_decoded[0,:,12,18].cpu().detach().numpy())\n",
    "# plt.plot(val_data_layer0[0,1:,12,18].cpu().detach().numpy())\n",
    "\n",
    "plt.plot(val_decoded[0,:,18,12].cpu().detach().numpy())\n",
    "plt.plot(val_data_layer0[0,1:,18,12].cpu().detach().numpy())\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(val_data_layer0[0,0])\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_decoded[0,:,12,18].cpu().detach().numpy())\n",
    "print(train_decoded.shape)\n",
    "print(data_this.shape)\n",
    "data = torch.tensor(train_data[0]).unsqueeze(1).float()\n",
    "print(data.shape)\n",
    "data_this = obtain_data_at_current_level(data, 0,3)\n",
    "plt.plot(data_this[:,0, 12, 18].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#animate the first decoded\n",
    "\n",
    "def animate(snapshots, normalize = True, file_name = \"animation.gif\"):\n",
    "    \"\"\"\n",
    "    Makes an animation of snapshots in time\n",
    "    \n",
    "    inputs:\n",
    "    snapshots: np.array of size (n_time_steps,1, dim, dim)\n",
    "    normalize = True: whether to fix axis between 0 and 1 or not\n",
    "    file_name = \"animation.gif\": file name of where to save gif\n",
    "    \n",
    "    outputs:\n",
    "    no returned output\n",
    "    gif saved to file_name\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    fps = 30\n",
    "    nSeconds = len(snapshots)/fps\n",
    "    \n",
    "    # First set up the figure, the axis, and the plot element we want to animate\n",
    "    fig = plt.figure( figsize=(8,8) )\n",
    "\n",
    "    a = snapshots[0,:,:,:][0].T\n",
    "    if normalize:\n",
    "        im = plt.imshow(a, interpolation='none', aspect='auto', vmin=0.0, vmax=1.0)\n",
    "    else:\n",
    "        im = plt.imshow(a, interpolation='none', aspect='auto', vmin=np.min(snapshots), vmax=np.max(snapshots))\n",
    "    plt.colorbar()\n",
    "\n",
    "    print(\"Animating, may take a little while...\")\n",
    "\n",
    "    def animate_func(i):\n",
    "        if i % fps == 0:\n",
    "            print( '.', end ='' )\n",
    "\n",
    "        im.set_array(snapshots[i,:,:,:,][0].T)\n",
    "        return [im]\n",
    "\n",
    "    anim = animation.FuncAnimation(\n",
    "                                   fig,\n",
    "                                   animate_func,\n",
    "                                   frames = int(nSeconds * fps),\n",
    "                                   interval = 1000 / fps, # in ms\n",
    "                                   )\n",
    "    writergif = animation.PillowWriter(fps=30)\n",
    "    anim.save(file_name, writer=writergif)\n",
    "\n",
    "    print('Done! gif saved to ', file_name)\n",
    "    \n",
    "to_animate = val_decoded[0].unsqueeze(1).cpu().detach().numpy()\n",
    "animate(to_animate, False, 'decoded_0_0.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_decoded[0].unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
