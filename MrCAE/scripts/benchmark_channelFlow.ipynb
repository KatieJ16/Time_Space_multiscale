{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark: Channel Flow "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. We benchmark against the number of parameters / size of hidden representations of other similar networks.\n",
    "2. These other networks include:\n",
    "    a. The same multilevel CAE with nonlinear activations (but without progressive refinements).\n",
    "    b. Linear CAE with symmetric skipped connections.\n",
    "    c. CAE with symmetric skipped connections & nonlinear activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../src/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import torch_cae_multilevel_V4 as net\n",
    "import torch_cae_skip_connection as net2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/npy/channel_flow2.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4b3f4bdc2589>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# load data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'../data/npy/channel_flow2.npy'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultiScaleDynamicsDataSet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_levels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive - acornonsite.com\\MrCAE\\src\\utils.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data_path, n_levels, map_path, train_ratio, valid_ratio)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_levels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;31m# load data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    414\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m             \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/npy/channel_flow2.npy'"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data_path = '../data/npy/channel_flow2.npy'\n",
    "dataset = net.MultiScaleDynamicsDataSet(data_path, n_levels=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multilevel CAE with progressive refinements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archs = [[1,2,3,4],[1,3,5,7],[1,4,7,10],[1,6,11,16]]\n",
    "tols = [0.05, 0.03, 0.02, 0.01]\n",
    "net.train_net(archs=archs, dataset=dataset, max_epoch=4000, batch_size=350, tols=tols, activation=torch.nn.Sequential(), \n",
    "              w=0.5, model_path=model_path, result_path=result_path, std=0.01, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_path = '../result/fluid2'\n",
    "# results_1 = {}\n",
    "# for file_name in sorted(os.listdir(result_path)):\n",
    "#     if file_name.endswith('.dat'):\n",
    "#         key, _ = file_name.split('.')\n",
    "#         with open(os.path.join(result_path, file_name), 'rb') as f: \n",
    "#             records[key]= pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multilevel CAE with nonlinear activation functions (without progressive refinements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = results_1['arch']\n",
    "results_2 = net.train_net(arch=arch, dataset=dataset, max_epoch=3000, batch_size=350, tols=None,\n",
    "                       activation=torch.nn.ReLU(),  w=0.5, std=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAE with symmetric skipped connections & nonlinear activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = results_1['arch']\n",
    "n_params_3, errs_3 = net2.train_archs(arch=arch, activation=torch.nn.ReLU(), dataset=dataset, \n",
    "                                      base_epoch=3000, batch_size=350, w=0.5, std=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear CAE with symmetric skipped connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = results_1['arch']\n",
    "n_params_4, errs_4 = net2.train_archs(arch=arch, activation=torch.nn.Sequential(), dataset=dataset, \n",
    "                                      base_epoch=3000, batch_size=350, w=0.5, std=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### err - # of params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.log(results_1['n_params']), np.log(results_1['best_val_errs']), 'r-o', markersize=10, linewidth=2.0)\n",
    "plt.plot(np.log(results_2['n_params']), np.log(results_2['best_val_errs']), 'b-o', markersize=10, linewidth=2.0)\n",
    "plt.plot(np.log(n_params_3), np.log(errs_3), 'g-o', markersize=10, linewidth=2.0)\n",
    "plt.plot(np.log(n_params_4), np.log(errs_4), 'c-o', markersize=10, linewidth=2.0)\n",
    "plt.xlabel('log(number of parameters)', fontsize=20)\n",
    "plt.ylabel('log(validation error)', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### err - # of encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the size of encodings \n",
    "arch_diff = list()\n",
    "for l in arch:\n",
    "    arch_diff.append([1] + [l[i] - l[i-1] for i in range(1, len(l))])\n",
    "\n",
    "size_of_maps = list()\n",
    "for i in range(len(arch)):\n",
    "    size_of_maps.append([np.multiply(*results_1['model'].resolved_maps[str(i)]['0'].size())])\n",
    "    for j in range(len(results_1['model'].resolved_maps[str(i)]) - 1):\n",
    "        n1 = int(torch.sum(1 - results_1['model'].resolved_maps[str(i)][str(j)]))\n",
    "        size_of_maps[i].append(n1)\n",
    "\n",
    "n_encodings_1 = [0]\n",
    "n_encodings_234 = [0]\n",
    "for i in range(len(arch_diff)):\n",
    "    n_encodings_1.append(n_encodings_1[-1] + size_of_maps[i][0])\n",
    "    n_encodings_234.append(n_encodings_234[-1] + size_of_maps[i][0])\n",
    "    for j in range(1, len(arch_diff[i])):\n",
    "        # add_size = min(size_of_maps[i][j]*(2+arch_diff[i][j]), size_of_maps[i][0]*arch_diff[i][j])\n",
    "        add_size = size_of_maps[i][j] * arch_diff[i][j]\n",
    "        n_encodings_1.append(n_encodings_1[-1] + add_size)\n",
    "        n_encodings_234.append(n_encodings_234[-1] + size_of_maps[i][0]*arch_diff[i][j])\n",
    "\n",
    "n_encodings_1 = n_encodings_1[1:]\n",
    "n_encodings_234 = n_encodings_234[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.log(n_encodings_1), np.log(results_1['best_val_errs']), 'r-o', markersize=10, linewidth=2.0)\n",
    "plt.plot(np.log(n_encodings_234), np.log(results_2['best_val_errs']), 'b-o', markersize=10, linewidth=2.0)\n",
    "plt.plot(np.log(n_encodings_234), np.log(errs_3), 'k-o', markersize=10, linewidth=2.0)\n",
    "plt.plot(np.log(n_encodings_234), np.log(errs_4), 'c-o', markersize=10, linewidth=2.0)\n",
    "plt.xlabel('log(size of encodings)', fontsize=20)\n",
    "plt.ylabel('log(validation error)', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "result_path = '../result/fluid2/'\n",
    "\n",
    "with open(os.path.join(result_path, 'errs_1.dat'), \"wb\") as fp: \n",
    "    pickle.dump(results_1['best_val_errs'], fp)\n",
    "with open(os.path.join(result_path, 'errs_2.dat'), \"wb\") as fp: \n",
    "    pickle.dump(results_2['best_val_errs'], fp)\n",
    "with open(os.path.join(result_path, 'errs_3.dat'), \"wb\") as fp: \n",
    "    pickle.dump(errs_3, fp)\n",
    "with open(os.path.join(result_path, 'errs_4.dat'), \"wb\") as fp: \n",
    "    pickle.dump(errs_4, fp)\n",
    "    \n",
    "with open(os.path.join(result_path, 'n_params_1.dat'), \"wb\") as fp: \n",
    "    pickle.dump(results_1['n_params'], fp)\n",
    "with open(os.path.join(result_path, 'n_params_2.dat'), \"wb\") as fp: \n",
    "    pickle.dump(results_2['n_params'], fp)\n",
    "with open(os.path.join(result_path, 'n_params_3.dat'), \"wb\") as fp: \n",
    "    pickle.dump(n_params_3, fp)\n",
    "with open(os.path.join(result_path, 'n_params_4.dat'), \"wb\") as fp: \n",
    "    pickle.dump(n_params_4, fp)\n",
    "    \n",
    "with open(os.path.join(result_path, 'encodings_1.dat'), \"wb\") as fp: \n",
    "    pickle.dump(n_encodings_1, fp)\n",
    "with open(os.path.join(result_path, 'encodings_234.dat'), \"wb\") as fp: \n",
    "    pickle.dump(n_encodings_234, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
