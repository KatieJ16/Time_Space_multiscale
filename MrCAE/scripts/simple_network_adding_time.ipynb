{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#following example at https://analyticsindiamag.com/how-to-implement-convolutional-autoencoder-in-pytorch-with-cuda/\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../src/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import torch_cae_multilevel_V4 as net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "def animate(snapshots, file_name = \"animation.gif\"):\n",
    "\n",
    "\n",
    "    fps = 30\n",
    "    nSeconds = len(snapshots)/fps\n",
    "    fig = plt.figure( figsize=(8,8) )\n",
    "\n",
    "    a = snapshots[0,:,:,:][0].T\n",
    "    im = plt.imshow(a, interpolation='none', aspect='auto', vmin=np.min(snapshots), vmax=np.max(snapshots))\n",
    "    plt.colorbar()\n",
    "    \n",
    "    print(\"Animating, may take a little while...\")\n",
    "\n",
    "    def animate_func(i):\n",
    "        if i % fps == 0:\n",
    "            print( '.', end ='' )\n",
    "\n",
    "        im.set_array(snapshots[i,:,:,:,][0].T)\n",
    "        return [im]\n",
    "\n",
    "    anim = animation.FuncAnimation(\n",
    "                                   fig, \n",
    "                                   animate_func, \n",
    "                                   frames = int(nSeconds * fps),\n",
    "                                   interval = 1000 / fps, # in ms\n",
    "                                   )\n",
    "    writergif = animation.PillowWriter(fps=30)\n",
    "    anim.save(file_name, writer=writergif)#, fps=30)\n",
    "\n",
    "\n",
    "    print('Done! gif saved to ', file_name)\n",
    "\n",
    "    \n",
    "\n",
    "class MultiScaleDynamicsDataSet():\n",
    "    def __init__(self, data_path, n_levels, t_array = None, map_path=None, train_ratio=0.7, valid_ratio=0.2, shuffle=True):\n",
    "        # load data\n",
    "        data = np.load(data_path)\n",
    "        self.data = torch.tensor(data).unsqueeze(1).float()\n",
    "        if t_array is None:\n",
    "            self.t_array = torch.range(len(self.data)).float()\n",
    "        else:\n",
    "            self.t_array = torch.tensor(t_array).float()\n",
    "        #\n",
    "        if map_path is not None:\n",
    "            map_data = 1 - np.load(map_path)\n",
    "            self.map_data = torch.tensor(map_data).float()\n",
    "        else:\n",
    "            self.map_data = torch.ones(data.shape[-2:]).float()\n",
    "\n",
    "        self.nt, self.nx, self.ny = data.shape\n",
    "        # partition\n",
    "        indices = np.arange(self.nt)\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        n_train = int(train_ratio*self.nt)\n",
    "        n_val = int(valid_ratio*self.nt)\n",
    "        self.n_train = n_train\n",
    "        self.n_val = n_val\n",
    "        self.n_test = self.nt - n_train - n_val\n",
    "        self.train_inds = indices[:n_train]\n",
    "        self.val_inds = indices[n_train:n_train+n_val]\n",
    "        self.test_inds = indices[n_train+n_val:]\n",
    "        \n",
    "        #\n",
    "        self.n_levels = n_levels\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.map_data = self.map_data.to(self.device)\n",
    "\n",
    "    def obtain_data_at_current_level(self, level):\n",
    "        train_data = self.data[self.train_inds].to(self.device)\n",
    "\n",
    "        val_data = self.data[self.val_inds].to(self.device)\n",
    "        test_data = self.data[self.test_inds].to(self.device)\n",
    "        \n",
    "        for i in range(self.n_levels - level - 1):\n",
    "            train_data = apply_local_op(train_data, self.device, ave=False)\n",
    "            val_data = apply_local_op(val_data, self.device, ave=False)\n",
    "            test_data = apply_local_op(test_data, self.device, ave=False)\n",
    "\n",
    "        return train_data, val_data, test_data\n",
    "    \n",
    "    def get_times(self):\n",
    "        train_times = self.t_array[self.train_inds]\n",
    "        val_times = self.t_array[self.val_inds]\n",
    "        test_times = self.t_array[self.test_inds]\n",
    "        \n",
    "        return train_times, val_times, test_times\n",
    "\n",
    "    def obtain_data_of_size(self, size_per_dim):\n",
    "        train_data = self.data[self.train_inds].to(self.device)\n",
    "        \n",
    "        n_images, _, nx, ny = train_data.shape\n",
    "        \n",
    "        print(train_data.shape)\n",
    "        \n",
    "        stride_size = int(nx/size_per_dim)\n",
    "        \n",
    "        op = torch.nn.Conv2d(1, 1, stride_size, stride=stride_size, padding=0).to(device)\n",
    "        #averaging\n",
    "        op.weight.data[0,0, :, :] = torch.ones(op.weight.data[0,0, :, :].size()).to(device) / (stride_size **2)\n",
    "        new_train_data = op(train_data)\n",
    "\n",
    "        return new_train_data\n",
    "        \n",
    "        \n",
    "        \n",
    "def apply_local_op(data, device, mode='conv', ave=True):\n",
    "    \"\"\"\n",
    "    :param data: data to be processed\n",
    "    :param device: which device is the data placed in?\n",
    "    :param mode: string, 'conv' or 'deconv'\n",
    "    :param ave: if to use local average or sample the center\n",
    "    :return: processed data\n",
    "    \"\"\"\n",
    "    in_channels, out_channels, _, _ = data.size()\n",
    "    print(\"data size = \", data.shape)\n",
    "    n = min(in_channels, out_channels)\n",
    "    if mode == 'conv':\n",
    "        op = torch.nn.Conv2d(out_channels, out_channels, 3, stride=2, padding=0).to(device)\n",
    "    elif mode == 'deconv':\n",
    "        op = torch.nn.ConvTranspose2d(out_channels, out_channels, 3, stride=2, padding=0).to(device)\n",
    "    else:\n",
    "        raise ValueError('mode can only be conv or deconv!')\n",
    "    op.weight.data = torch.zeros(op.weight.data.size()).to(device)\n",
    "    op.bias.data = torch.zeros(op.bias.data.size()).to(device)\n",
    "\n",
    "    for i in range(n):\n",
    "        if mode == 'conv':\n",
    "            if ave:\n",
    "                op.weight.data[i, i, :, :] = torch.ones(op.weight.data[i, i, :, :].size()).to(device) / 9\n",
    "            else:\n",
    "                op.weight.data[i, i, 1, 1] = torch.ones(op.weight.data[i, i, 1, 1].size()).to(device)\n",
    "        elif mode == 'deconv':\n",
    "            op.weight.data[i, i, :, :] = torch.ones(op.weight.data[i, i, :, :].size()).to(device) / 4\n",
    "            op.weight.data[i, i, 0, 1] += 1 / 4\n",
    "            op.weight.data[i, i, 1, 0] += 1 / 4\n",
    "            op.weight.data[i, i, 1, 2] += 1 / 4\n",
    "            op.weight.data[i, i, 2, 1] += 1 / 4\n",
    "            op.weight.data[i, i, 1, 1] += 1 / 4\n",
    "            op.weight.data[i, i, 1, 1] += 1 / 2\n",
    "\n",
    "    # make them non-trainable\n",
    "    for param in op.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    return op(data)\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda:0'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return device\n",
    "\n",
    "device = get_device()\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 1, 127, 127])\n",
      "torch.Size([500, 1, 127, 127])\n",
      "Animating, may take a little while...\n",
      "..................Done! gif saved to  4x4.gif\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#make data set\n",
    "\n",
    "data_path = '../data/npy/toy1.npy'\n",
    "t = np.linspace(0, 8*np.pi, 500)\n",
    "dataset =MultiScaleDynamicsDataSet(data_path, n_levels=0, t_array=t, train_ratio=1.0,  valid_ratio=0.0, shuffle = False)\n",
    "data,_,_ = dataset.obtain_data_at_current_level(level=0)\n",
    "\n",
    "data_new = dataset.obtain_data_of_size(2).detach().numpy()\n",
    "train_times, _, _ = dataset.get_times()\n",
    "# animate(data_new, file_name = \"2x2.gif\")\n",
    "\n",
    "animate(dataset.obtain_data_of_size(4).detach().numpy(), file_name = \"4x4.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvAutoencoder(\n",
      "  (convfirst): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2))\n",
      "  (conv1): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (t_conv1): ConvTranspose2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (t_conv_last): ConvTranspose2d(1, 1, kernel_size=(3, 3), stride=(2, 2))\n",
      ")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-afaa1a3030e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConvAutoencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "#Define the Convolutional Autoencoder\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "       \n",
    "        #Encoder\n",
    "        self.convfirst = nn.Conv2d(1, 1, 3, stride = 2, padding=0)  \n",
    "        self.conv1 = nn.Conv2d(1, 1, 3, stride = 1, padding=1)  \n",
    "       \n",
    "        #Decoder\n",
    "        self.t_conv1 = nn.ConvTranspose2d(1, 1, 3, stride=1, padding=1)\n",
    "        self.t_conv_last = nn.ConvTranspose2d(1, 1, 3, stride=2, padding=0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.convfirst(x))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = torch.nn.functional.pad(x, (1, 1, 1, 1), 'replicate')\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        x = F.sigmoid(self.t_conv_last(x))\n",
    "        x = x[:, :, 2:-2, 2:-2]\n",
    "        return x\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = F.relu(self.convfirst(x))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = torch.nn.functional.pad(x, (1, 1, 1, 1), 'replicate')\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        x = F.sigmoid(self.t_conv_last(x))\n",
    "        x = x[:, :, 2:-2, 2:-2]\n",
    "        \n",
    "        return x\n",
    "\n",
    "#Instantiate the model\n",
    "model = ConvAutoencoder()\n",
    "print(model)\n",
    "model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss function\n",
    "criterion = nn.BCELoss()\n",
    "# criterion = nn.MSELoss()#reduction='none')\n",
    "\n",
    "#Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device = get_device()\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Epochs\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "\n",
    "#     i = 0\n",
    "    #Training\n",
    "    for images in train_loader:\n",
    "        images = images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "          \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Batch of test images\n",
    "dataiter = iter(train_loader)\n",
    "images = dataiter.next()\n",
    "\n",
    "#Sample outputs\n",
    "output = model(images)\n",
    "images = images.numpy()\n",
    "\n",
    "print(images.shape)\n",
    "batch_size = 50\n",
    "output = output.view(batch_size, 1, 31, 31)#127, 127)\n",
    "output = output.detach().numpy()\n",
    "\n",
    "image_min = np.min(images)\n",
    "image_max = np.max(images)\n",
    "print(\"image_min = \", image_min)\n",
    "print(\"image_max = \", image_max)\n",
    "\n",
    "print(\"output min = \", np.min(output))\n",
    "print(\"output max = \", np.max(output))\n",
    "#Original Images\n",
    "print(\"Original Images\")\n",
    "fig, axes = plt.subplots(nrows=1, ncols=5, sharex=True, sharey=True, figsize=(12,4))\n",
    "for idx in np.arange(5):\n",
    "    ax = fig.add_subplot(1, 5, idx+1, xticks=[], yticks=[])\n",
    "    print(images[idx][0].shape)\n",
    "    imshow(images[idx])#[0], vmin = -2.5, vmax = 2.5)\n",
    "#     ax.set_title(classes[labels[idx]])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "#Reconstructed Images\n",
    "print('Reconstructed Images')\n",
    "fig, axes = plt.subplots(nrows=1, ncols=5, sharex=True, sharey=True, figsize=(12,4))\n",
    "for idx in np.arange(5):\n",
    "    ax = fig.add_subplot(1, 5, idx+1, xticks=[], yticks=[])\n",
    "    imshow(output[idx])\n",
    "    plt.colorbar()\n",
    "#     ax.set_title(classes[labels[idx]])\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(data_new[0][0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "animate(data_new, file_name = \"2x2.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we want to evolve our 2x2 forward in time\n",
    "\n",
    "class time_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(time_net, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "time_model = time_net()\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(time_model.parameters(), lr=1e-3)\n",
    "\n",
    "inputs = torch.tensor(data_new[:-1])\n",
    "inputs = torch.flatten(inputs, start_dim=1, end_dim=-1)\n",
    "np.save(\"inputs_2x2.npy\", inputs.detach().numpy())\n",
    "print(\"inputs.shape = \", inputs.shape)\n",
    "outputs = torch.tensor(data_new[1:])\n",
    "outputs = torch.flatten(outputs, start_dim=1, end_dim=-1)\n",
    "print(\"outputs.shape = \", outputs.shape)\n",
    "n_epochs = 50000\n",
    "for t in range(n_epochs):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    predicted = time_model(inputs)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = criterion(predicted, outputs)\n",
    "    if t % 1000 == 0:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps_predict = 100\n",
    "\n",
    "predicted = inputs[0]\n",
    "current = inputs[0]\n",
    "for i in range(n_steps_predict):\n",
    "    next_step = time_model(current)#.reshape(1, 1, 2, 2)\n",
    "    predicted = torch.cat((predicted,next_step), 0)\n",
    "    current = next_step\n",
    "#     print(predicted.shape)\n",
    "    \n",
    "predicted = predicted.reshape(n_steps_predict+1, 1, 2, 2).detach().numpy()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(predicted[n_steps_predict][0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "inputs_np = inputs.reshape(499,1,2,2).detach().numpy()\n",
    "plt.imshow(inputs_np[n_steps_predict][0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "gh\n",
    "\n",
    "predicted = time_model(inputs).reshape(499, 1, 2, 2).detach().numpy()\n",
    "\n",
    "print(predicted.shape)\n",
    "plt.figure()\n",
    "plt.imshow(predicted[0][0],vmin = .8, vmax = 1.0)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "inputs_np = inputs.reshape(499, 1, 2, 2).detach().numpy()\n",
    "plt.figure()\n",
    "plt.imshow(inputs_np[1][0],vmin = .8, vmax = 1.0)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animate(predicted, \"2x2_predicted.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mse = np.sqrt((inputs_np[:101,0] - predicted[:,0])**2)\n",
    "total_mse = mse[:,0,0] + mse[:,0,1] + mse[:,1,0] + mse[:,1,1]\n",
    "print(total_mse.shape)\n",
    "\n",
    "\n",
    "t = np.linspace(0, 8*np.pi, 500)\n",
    "w0 = 0.5\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(t[:101],total_mse, \"r\")\n",
    "# ax.set_y_label(\"MSE\")\n",
    "ax2=ax.twinx()\n",
    "# ax2.plot(t[1:], np.cos(w0*t[1:]),\"b\")\n",
    "plt.title(\"Total Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs_np.shape)\n",
    "sum_all  = inputs_np[:,:,0,0] + inputs_np[:,:,0,1] + inputs_np[:,:,1,0] + inputs_np[:,:,1,1]\n",
    "sum_all.shape\n",
    "# plt.plot(sum_all)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(t[1:],sum_all, \"r\")\n",
    "# ax.set_y_label(\"MSE\")\n",
    "ax2=ax.twinx()\n",
    "ax2.plot(t[1:], np.cos(w0*t[1:]),\"b\")\n",
    "plt.title(\"Sum of all points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "mse = np.sqrt((inputs_np[:,0] - predicted[:,0])**2)\n",
    "\n",
    "plt.plot(t[1:], mse[:,0,0], label = \"(0,0)\")\n",
    "plt.plot(t[1:], mse[:,1,0], label = \"(1,0)\")\n",
    "plt.plot(t[1:], mse[:,0,1], label = \"(0,1)\")\n",
    "plt.plot(t[1:], mse[:,1,1], label = \"(1,1)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
