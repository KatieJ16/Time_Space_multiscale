{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark: toy 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. We benchmark against the number of parameters / size of hidden representations of other similar networks.\n",
    "2. These other networks include:\n",
    "    a. The same multilevel CAE with nonlinear activations (but without progressive refinements).\n",
    "    b. Linear CAE with symmetric skipped connections.\n",
    "    c. CAE with symmetric skipped connections & nonlinear activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../src/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import torch_cae_multilevel_V4 as net\n",
    "import torch_cae_skip_connection as net2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16kat\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data_path = '../data/npy/toy1.npy'\n",
    "model_path = '../model/toy1/'\n",
    "result_path = '../result/toy1/'\n",
    "dataset = net.MultiScaleDynamicsDataSet(data_path, n_levels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multilevel CAE with progressive refinements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************\n",
      "Model @Level 0:\n",
      "Perform deepening & widening, train each architectures ...\n",
      "model layers: \n",
      "['activation', 'L0_Conv_0', 'L0_deConv_0']\n",
      "losses printing format: local: mse/max/overall, global: mse/max/overall\n",
      "epoch [1/500]\n",
      "[training set] local: 0.0047/0.7267/0.3657, global: 0.0046/0.7267/0.3657\n",
      "[validation set] local: 0.0044/0.6333/0.3189, global: 0.0044/0.6333/0.3189\n",
      "epoch [50/500]:\n",
      "[training set] local: 0.0160/0.6591/0.3376, global: 0.0146/0.6669/0.3407\n",
      "[validation set] local: 0.0158/0.5634/0.2896, global: 0.0144/0.5754/0.2949\n",
      "epoch [100/500]:\n",
      "[training set] local: 0.0152/0.6187/0.3169, global: 0.0122/0.6307/0.3214\n",
      "[validation set] local: 0.0149/0.5273/0.2711, global: 0.0120/0.5443/0.2781\n",
      "epoch [150/500]:\n",
      "[training set] local: 0.0136/0.5342/0.2739, global: 0.0103/0.5521/0.2812\n",
      "[validation set] local: 0.0133/0.4548/0.2341, global: 0.0101/0.4765/0.2433\n",
      "epoch [200/500]:\n",
      "[training set] local: 0.0106/0.3939/0.2023, global: 0.0079/0.4370/0.2224\n",
      "[validation set] local: 0.0104/0.3352/0.1728, global: 0.0077/0.3773/0.1925\n",
      "epoch [250/500]:\n",
      "[training set] local: 0.0070/0.2322/0.1196, global: 0.0052/0.3104/0.1578\n",
      "[validation set] local: 0.0068/0.1976/0.1022, global: 0.0051/0.2684/0.1367\n",
      "epoch [300/500]:\n",
      "[training set] local: 0.0030/0.1409/0.0720, global: 0.0020/0.2393/0.1207\n",
      "[validation set] local: 0.0028/0.1250/0.0639, global: 0.0019/0.2085/0.1052\n",
      "epoch [350/500]:\n",
      "[training set] local: 0.0028/0.1129/0.0579, global: 0.0021/0.2080/0.1051\n",
      "[validation set] local: 0.0027/0.0992/0.0510, global: 0.0020/0.1807/0.0914\n",
      "epoch [400/500]:\n",
      "[training set] local: 0.0027/0.0954/0.0490, global: 0.0020/0.1930/0.0975\n",
      "[validation set] local: 0.0025/0.0853/0.0439, global: 0.0019/0.1675/0.0847\n",
      "epoch [450/500]:\n",
      "[training set] local: 0.0025/0.0817/0.0421, global: 0.0019/0.1824/0.0922\n",
      "[validation set] local: 0.0024/0.0731/0.0377, global: 0.0018/0.1586/0.0802\n",
      "epoch [500/500]:\n",
      "[training set] local: 0.0022/0.0705/0.0364, global: 0.0017/0.1741/0.0879\n",
      "[validation set] local: 0.0021/0.0634/0.0327, global: 0.0016/0.1514/0.0765\n",
      "-------------------------------------------------\n",
      "prepare attaching 1 more filters to current level arch ...\n",
      "model layers: \n",
      "['activation', 'L0_Conv_0', 'L0_deConv_0', 'L0_Conv_1', 'L0_deConv_1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\MrCAE\\src\\utils.py:137: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:882.)\n",
      "  for c in mask.nonzero():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses printing format: local: mse/max/overall, global: mse/max/overall\n",
      "epoch [1/500]\n",
      "[training set] local: 0.0022/0.0704/0.0363, global: 0.0017/0.1740/0.0878\n",
      "[validation set] local: 0.0021/0.0628/0.0325, global: 0.0016/0.1507/0.0761\n",
      "epoch [50/500]:\n",
      "[training set] local: 0.0014/0.0643/0.0329, global: 0.0010/0.1720/0.0865\n",
      "[validation set] local: 0.0013/0.0579/0.0296, global: 0.0009/0.1506/0.0757\n",
      "epoch [100/500]:\n",
      "[training set] local: 0.0014/0.0606/0.0310, global: 0.0009/0.1685/0.0847\n",
      "[validation set] local: 0.0013/0.0548/0.0280, global: 0.0008/0.1479/0.0744\n",
      "epoch [150/500]:\n",
      "[training set] local: 0.0014/0.0548/0.0281, global: 0.0009/0.1637/0.0823\n",
      "[validation set] local: 0.0013/0.0506/0.0260, global: 0.0008/0.1440/0.0724\n",
      "epoch [200/500]:\n",
      "[training set] local: 0.0015/0.0484/0.0250, global: 0.0009/0.1597/0.0803\n",
      "[validation set] local: 0.0014/0.0451/0.0233, global: 0.0008/0.1405/0.0707\n",
      "epoch [250/500]:\n",
      "[training set] local: 0.0017/0.0416/0.0216, global: 0.0009/0.1551/0.0780\n",
      "[validation set] local: 0.0016/0.0391/0.0203, global: 0.0009/0.1367/0.0688\n",
      "epoch [300/500]:\n",
      "[training set] local: 0.0018/0.0349/0.0183, global: 0.0010/0.1533/0.0771\n",
      "[validation set] local: 0.0017/0.0334/0.0175, global: 0.0009/0.1352/0.0680\n",
      "epoch [350/500]:\n",
      "[training set] local: 0.0018/0.0309/0.0163, global: 0.0011/0.1547/0.0779\n",
      "[validation set] local: 0.0017/0.0292/0.0154, global: 0.0010/0.1365/0.0688\n",
      "epoch [400/500]:\n",
      "[training set] local: 0.0018/0.0282/0.0150, global: 0.0010/0.1550/0.0780\n",
      "[validation set] local: 0.0017/0.0259/0.0138, global: 0.0010/0.1368/0.0689\n",
      "early stopping at 400th iteration due to slow convergence!\n",
      "\n",
      "level 0, resolved map 1 (after training):\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "-------------------------------------------------\n",
      "prepare attaching 1 more filters to current level arch ...\n",
      "model layers: \n",
      "['activation', 'L0_Conv_0', 'L0_deConv_0', 'L0_Conv_1', 'L0_deConv_1', 'L0_Conv_2', 'L0_deConv_2']\n",
      "losses printing format: local: mse/max/overall, global: mse/max/overall\n",
      "epoch [1/500]\n",
      "[training set] local: 0.0018/0.0281/0.0150, global: 0.0010/0.1551/0.0781\n",
      "[validation set] local: 0.0017/0.0259/0.0138, global: 0.0010/0.1371/0.0690\n",
      "epoch [50/500]:\n",
      "[training set] local: 0.0016/0.0269/0.0142, global: 0.0009/0.1524/0.0767\n",
      "[validation set] local: 0.0015/0.0243/0.0129, global: 0.0008/0.1344/0.0676\n",
      "epoch [100/500]:\n",
      "[training set] local: 0.0013/0.0255/0.0134, global: 0.0008/0.1504/0.0756\n",
      "[validation set] local: 0.0013/0.0228/0.0120, global: 0.0008/0.1332/0.0670\n",
      "epoch [150/500]:\n",
      "[training set] local: 0.0011/0.0236/0.0124, global: 0.0008/0.1449/0.0728\n",
      "[validation set] local: 0.0011/0.0219/0.0115, global: 0.0007/0.1280/0.0643\n",
      "epoch [200/500]:\n",
      "[training set] local: 0.0010/0.0207/0.0108, global: 0.0008/0.1390/0.0699\n",
      "[validation set] local: 0.0009/0.0190/0.0100, global: 0.0007/0.1233/0.0620\n",
      "epoch [250/500]:\n",
      "[training set] local: 0.0009/0.0179/0.0094, global: 0.0008/0.1324/0.0666\n",
      "[validation set] local: 0.0009/0.0172/0.0090, global: 0.0008/0.1171/0.0589\n",
      "epoch [300/500]:\n",
      "[training set] local: 0.0010/0.0153/0.0082, global: 0.0009/0.1264/0.0637\n",
      "[validation set] local: 0.0010/0.0147/0.0078, global: 0.0009/0.1117/0.0563\n",
      "epoch [350/500]:\n",
      "[training set] local: 0.0009/0.0140/0.0075, global: 0.0009/0.1222/0.0615\n",
      "[validation set] local: 0.0009/0.0137/0.0073, global: 0.0008/0.1077/0.0543\n",
      "epoch [400/500]:\n",
      "[training set] local: 0.0009/0.0130/0.0070, global: 0.0008/0.1183/0.0596\n",
      "[validation set] local: 0.0008/0.0120/0.0064, global: 0.0008/0.1043/0.0525\n",
      "epoch [450/500]:\n",
      "[training set] local: 0.0008/0.0118/0.0063, global: 0.0007/0.1144/0.0576\n",
      "[validation set] local: 0.0008/0.0113/0.0060, global: 0.0007/0.1010/0.0509\n",
      "epoch [500/500]:\n",
      "[training set] local: 0.0007/0.0108/0.0058, global: 0.0007/0.1104/0.0555\n",
      "[validation set] local: 0.0007/0.0102/0.0054, global: 0.0006/0.0976/0.0491\n",
      "\n",
      "level 0, resolved map 2 (after training):\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.]])\n",
      "-------------------------------------------------\n",
      "prepare attaching 1 more filters to current level arch ...\n",
      "model layers: \n",
      "['activation', 'L0_Conv_0', 'L0_deConv_0', 'L0_Conv_1', 'L0_deConv_1', 'L0_Conv_2', 'L0_deConv_2', 'L0_Conv_3', 'L0_deConv_3']\n",
      "losses printing format: local: mse/max/overall, global: mse/max/overall\n",
      "epoch [1/500]\n",
      "[training set] local: 0.0007/0.0107/0.0057, global: 0.0007/0.1105/0.0556\n",
      "[validation set] local: 0.0007/0.0102/0.0055, global: 0.0006/0.0976/0.0491\n",
      "epoch [50/500]:\n",
      "[training set] local: 0.0006/0.0097/0.0052, global: 0.0006/0.1083/0.0545\n",
      "[validation set] local: 0.0006/0.0093/0.0049, global: 0.0006/0.0956/0.0481\n",
      "epoch [100/500]:\n",
      "[training set] local: 0.0006/0.0085/0.0045, global: 0.0005/0.1050/0.0528\n",
      "[validation set] local: 0.0006/0.0082/0.0044, global: 0.0005/0.0927/0.0466\n",
      "epoch [150/500]:\n",
      "[training set] local: 0.0005/0.0073/0.0039, global: 0.0005/0.1013/0.0509\n",
      "[validation set] local: 0.0005/0.0071/0.0038, global: 0.0004/0.0893/0.0449\n",
      "epoch [200/500]:\n",
      "[training set] local: 0.0005/0.0063/0.0034, global: 0.0004/0.0975/0.0490\n",
      "[validation set] local: 0.0005/0.0061/0.0033, global: 0.0004/0.0859/0.0432\n",
      "epoch [250/500]:\n",
      "[training set] local: 0.0005/0.0054/0.0029, global: 0.0004/0.0956/0.0480\n",
      "[validation set] local: 0.0004/0.0052/0.0028, global: 0.0004/0.0844/0.0424\n",
      "epoch [300/500]:\n",
      "[training set] local: 0.0004/0.0047/0.0025, global: 0.0003/0.0932/0.0468\n",
      "[validation set] local: 0.0004/0.0045/0.0024, global: 0.0003/0.0824/0.0414\n",
      "epoch [350/500]:\n",
      "[training set] local: 0.0003/0.0038/0.0021, global: 0.0003/0.0893/0.0448\n",
      "[validation set] local: 0.0003/0.0037/0.0020, global: 0.0003/0.0788/0.0395\n",
      "epoch [400/500]:\n",
      "[training set] local: 0.0003/0.0031/0.0017, global: 0.0002/0.0862/0.0432\n",
      "[validation set] local: 0.0003/0.0030/0.0016, global: 0.0002/0.0760/0.0381\n",
      "epoch [450/500]:\n",
      "[training set] local: 0.0002/0.0025/0.0014, global: 0.0002/0.0842/0.0422\n",
      "[validation set] local: 0.0002/0.0023/0.0013, global: 0.0002/0.0744/0.0373\n",
      "epoch [500/500]:\n",
      "[training set] local: 0.0002/0.0020/0.0011, global: 0.0002/0.0822/0.0412\n",
      "[validation set] local: 0.0002/0.0019/0.0010, global: 0.0002/0.0726/0.0364\n",
      "\n",
      "level 0, resolved map 3 (after training):\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "-------------------------------------------------\n",
      "*************************************************\n",
      "Model @Level 1:\n",
      "Perform deepening & widening, train each architectures ...\n",
      "model layers: \n",
      "['activation', 'L0_Conv_0', 'L0_deConv_0', 'L0_Conv_1', 'L0_deConv_1', 'L0_Conv_2', 'L0_deConv_2', 'L0_Conv_3', 'L0_deConv_3', 'L1_Conv_0', 'L1_deConv_0']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses printing format: local: mse/max/overall, global: mse/max/overall\n",
      "epoch [1/500]\n",
      "[training set] local: 0.0035/0.0541/0.0288, global: 0.0035/0.0558/0.0296\n",
      "[validation set] local: 0.0033/0.0451/0.0242, global: 0.0033/0.0467/0.0250\n",
      "epoch [50/500]:\n",
      "[training set] local: 0.0009/0.0104/0.0056, global: 0.0005/0.0194/0.0100\n",
      "[validation set] local: 0.0009/0.0094/0.0051, global: 0.0005/0.0174/0.0089\n",
      "epoch [100/500]:\n",
      "[training set] local: 0.0007/0.0087/0.0047, global: 0.0004/0.0173/0.0089\n",
      "[validation set] local: 0.0007/0.0084/0.0046, global: 0.0004/0.0155/0.0079\n",
      "epoch [150/500]:\n",
      "[training set] local: 0.0007/0.0082/0.0044, global: 0.0004/0.0160/0.0082\n",
      "[validation set] local: 0.0006/0.0078/0.0042, global: 0.0004/0.0142/0.0073\n",
      "epoch [200/500]:\n",
      "[training set] local: 0.0008/0.0081/0.0045, global: 0.0005/0.0174/0.0090\n",
      "[validation set] local: 0.0008/0.0079/0.0044, global: 0.0005/0.0153/0.0079\n",
      "epoch [250/500]:\n",
      "[training set] local: 0.0008/0.0081/0.0044, global: 0.0005/0.0155/0.0080\n",
      "[validation set] local: 0.0008/0.0076/0.0042, global: 0.0004/0.0137/0.0071\n",
      "epoch [300/500]:\n",
      "[training set] local: 0.0008/0.0083/0.0045, global: 0.0005/0.0167/0.0086\n",
      "[validation set] local: 0.0008/0.0074/0.0041, global: 0.0005/0.0147/0.0076\n",
      "epoch [350/500]:\n",
      "[training set] local: 0.0009/0.0078/0.0043, global: 0.0006/0.0165/0.0085\n",
      "[validation set] local: 0.0009/0.0076/0.0042, global: 0.0005/0.0146/0.0076\n",
      "epoch [400/500]:\n",
      "[training set] local: 0.0009/0.0078/0.0043, global: 0.0005/0.0150/0.0078\n",
      "[validation set] local: 0.0008/0.0072/0.0040, global: 0.0005/0.0133/0.0069\n",
      "early stopping at 400th iteration due to slow convergence!\n",
      "-------------------------------------------------\n",
      "prepare attaching 1 more filters to current level arch ...\n",
      "model layers: \n",
      "['activation', 'L0_Conv_0', 'L0_deConv_0', 'L0_Conv_1', 'L0_deConv_1', 'L0_Conv_2', 'L0_deConv_2', 'L0_Conv_3', 'L0_deConv_3', 'L1_Conv_0', 'L1_deConv_0', 'L1_Conv_1', 'L1_deConv_1']\n",
      "losses printing format: local: mse/max/overall, global: mse/max/overall\n",
      "epoch [1/500]\n",
      "[training set] local: 0.0010/0.0079/0.0044, global: 0.0006/0.0149/0.0077\n",
      "[validation set] local: 0.0010/0.0074/0.0042, global: 0.0006/0.0135/0.0071\n",
      "epoch [50/500]:\n",
      "[training set] local: 0.0009/0.0076/0.0043, global: 0.0006/0.0162/0.0084\n",
      "[validation set] local: 0.0009/0.0070/0.0039, global: 0.0005/0.0143/0.0074\n",
      "epoch [100/500]:\n",
      "[training set] local: 0.0008/0.0077/0.0043, global: 0.0005/0.0148/0.0077\n",
      "[validation set] local: 0.0008/0.0074/0.0041, global: 0.0005/0.0129/0.0067\n",
      "epoch [150/500]:\n",
      "[training set] local: 0.0009/0.0077/0.0043, global: 0.0005/0.0155/0.0080\n",
      "[validation set] local: 0.0008/0.0073/0.0040, global: 0.0005/0.0136/0.0070\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6df37bdd863e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0marchs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0005\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0002\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m net.train_net(archs=archs, dataset=dataset, max_epoch=500, batch_size=350, \n\u001b[0m\u001b[0;32m      4\u001b[0m               \u001b[0mtols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m               result_path=result_path, std=0.01, verbose=2)\n",
      "\u001b[1;32m~\\OneDrive - acornonsite.com\\MrCAE\\src\\torch_cae_multilevel_V4.py\u001b[0m in \u001b[0;36mtrain_net\u001b[1;34m(archs, dataset, max_epoch, batch_size, result_path, tols, model_path, activation, lr, w, std, verbose)\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;31m# training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_levels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m         model = train_net_one_level(arch=archs[i], dataset=dataset, max_epoch=max_epoch,\n\u001b[0m\u001b[0;32m    453\u001b[0m                                     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresult_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m                                     \u001b[0mmodel_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtols\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - acornonsite.com\\MrCAE\\src\\torch_cae_multilevel_V4.py\u001b[0m in \u001b[0;36mtrain_net_one_level\u001b[1;34m(arch, dataset, max_epoch, batch_size, result_path, model_path, load_model, tol, activation, lr, w, std, verbose)\u001b[0m\n\u001b[0;32m    507\u001b[0m         \u001b[0mn_filters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwiden_sizes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcnt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m         \u001b[0mcnt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m         model = train_net_one_stage(mode=0, n_filters=n_filters, dataset=dataset, max_epoch=max_epoch,\n\u001b[0m\u001b[0;32m    510\u001b[0m                                     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresult_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m                                     \u001b[0mload_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - acornonsite.com\\MrCAE\\src\\torch_cae_multilevel_V4.py\u001b[0m in \u001b[0;36mtrain_net_one_stage\u001b[1;34m(mode, n_filters, dataset, max_epoch, batch_size, result_path, load_model, tol, activation, lr, w, std, model_path, verbose)\u001b[0m\n\u001b[0;32m    601\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model layers: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m         val_losses, best_val_loss, mset = model.train_arch(dataset, max_epoch=max_epoch, batch_size=batch_size,\n\u001b[0m\u001b[0;32m    604\u001b[0m                                                            lr=lr, tol=tol, verbose=verbose, w=w)\n\u001b[0;32m    605\u001b[0m         \u001b[1;31m# collect results & save model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - acornonsite.com\\MrCAE\\src\\torch_cae_multilevel_V4.py\u001b[0m in \u001b[0;36mtrain_arch\u001b[1;34m(self, dataset, max_epoch, batch_size, tol, lr, w, verbose)\u001b[0m\n\u001b[0;32m    320\u001b[0m                 \u001b[0mbest_state_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;31m# compute global scale losses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[0mglobal_mean_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_max_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_global_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcur_level\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_inds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_idxs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m             \u001b[0mglobal_mean_val_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_max_val_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_global_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcur_level\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_inds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mglobal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mglobal_mean_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mglobal_max_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - acornonsite.com\\MrCAE\\src\\torch_cae_multilevel_V4.py\u001b[0m in \u001b[0;36mcompute_global_loss\u001b[1;34m(self, dataset, output, cur_level, inds)\u001b[0m\n\u001b[0;32m    402\u001b[0m         \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'none'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[0mmse_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m         \u001b[0mmax_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m         \u001b[0mmax_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "archs = [[1,2,3,4],[1,2,3,4],[1,2,3,4]]\n",
    "tols = [0.001, 0.0005, 0.0002]\n",
    "net.train_net(archs=archs, dataset=dataset, max_epoch=500, batch_size=350, \n",
    "              tols=tols, activation=torch.nn.Sequential(), w=0.5, model_path=model_path, \n",
    "              result_path=result_path, std=0.01, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_path = '../result/toy1'\n",
    "# results_1 = {}\n",
    "# for file_name in sorted(os.listdir(result_path)):\n",
    "#     if file_name.endswith('.dat'):\n",
    "#         key, _ = file_name.split('.')\n",
    "#         with open(os.path.join(result_path, file_name), 'rb') as f: \n",
    "#             records[key]= pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multilevel CAE with nonlinear activation functions (without progressive refinements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = results_1['arch']\n",
    "results_2 = net.train_net(arch=arch, dataset=dataset, max_epoch=5000, batch_size=350, tols=None,\n",
    "                       activation=torch.nn.ReLU(),  w=0.5, std=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAE with symmetric skipped connections & nonlinear activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = results_1['arch']\n",
    "n_params_3, errs_3 = net2.train_archs(arch=arch, activation=torch.nn.ReLU(), dataset=dataset, \n",
    "                                      base_epoch=5000, batch_size=350, w=0.5, std=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear CAE with skipped connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = results_1['arch']\n",
    "n_params_4, errs_4 = net2.train_archs(arch=arch, activation=torch.nn.Sequential(), dataset=dataset, \n",
    "                                      base_epoch=5000, batch_size=350, w=0.5, std=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### err - # of params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(results_1['n_params'], np.log(results_1['best_val_errs']), 'r-o', markersize=10, linewidth=2.0)\n",
    "plt.plot(results_2['n_params'], np.log(results_2['best_val_errs']), 'b-o', markersize=10, linewidth=2.0)\n",
    "plt.plot(n_params_3, np.log(errs_3), 'g-o', markersize=10, linewidth=2.0)\n",
    "plt.plot(n_params_4, np.log(errs_4), 'c-o', markersize=10, linewidth=2.0)\n",
    "plt.xlabel('log(number of parameters)', fontsize=20)\n",
    "plt.ylabel('log(validation error)', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### err - # of encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the size of encodings \n",
    "arch_diff = list()\n",
    "for l in arch:\n",
    "    arch_diff.append([1] + [l[i] - l[i-1] for i in range(1, len(l))])\n",
    "\n",
    "size_of_maps = list()\n",
    "for i in range(len(arch)):\n",
    "    size_of_maps.append([np.multiply(*results_1['model'].resolved_maps[str(i)]['0'].size())])\n",
    "    for j in range(len(results_1['model'].resolved_maps[str(i)]) - 1):\n",
    "        n1 = int(torch.sum(1 - results_1['model'].resolved_maps[str(i)][str(j)]))\n",
    "        size_of_maps[i].append(n1)\n",
    "\n",
    "n_encodings_1 = [0]\n",
    "n_encodings_234 = [0]\n",
    "for i in range(len(arch_diff)):\n",
    "    n_encodings_1.append(n_encodings_1[-1] + size_of_maps[i][0])\n",
    "    n_encodings_234.append(n_encodings_234[-1] + size_of_maps[i][0])\n",
    "    for j in range(1, len(arch_diff[i])):\n",
    "        # add_size = min(size_of_maps[i][j]*(2+arch_diff[i][j]), size_of_maps[i][0]*arch_diff[i][j])\n",
    "        add_size = size_of_maps[i][j] * arch_diff[i][j]\n",
    "        n_encodings_1.append(n_encodings_1[-1] + add_size)\n",
    "        n_encodings_234.append(n_encodings_234[-1] + size_of_maps[i][0]*arch_diff[i][j])\n",
    "\n",
    "n_encodings_1 = n_encodings_1[1:]\n",
    "n_encodings_234 = n_encodings_234[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.log(n_encodings_1), np.log(results_1['best_val_errs']), 'r-o', markersize=10, linewidth=2.0)\n",
    "plt.plot(np.log(n_encodings_234), np.log(results_2['best_val_errs']), 'b-o', markersize=10, linewidth=2.0)\n",
    "plt.plot(np.log(n_encodings_234), np.log(errs_3), 'k-o', markersize=10, linewidth=2.0)\n",
    "plt.plot(np.log(n_encodings_234), np.log(errs_4), 'c-o', markersize=10, linewidth=2.0)\n",
    "plt.xlabel('log(size of encodings)', fontsize=20)\n",
    "plt.ylabel('log(validation error)', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
