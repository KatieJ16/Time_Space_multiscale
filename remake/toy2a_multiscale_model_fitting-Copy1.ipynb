{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multiscale model fitting for Toy2a\n",
    "\n",
    "Toy2a is a simplified version of toy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start with initalizing many things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "# import sys\n",
    "import torch\n",
    "# import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from tqdm.notebook import tqdm\n",
    "# import time\n",
    "import math\n",
    "\n",
    "# module_path = os.path.abspath(os.path.join('../src/'))\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)\n",
    "    \n",
    "    \n",
    "# import torch_cae_multilevel_V4 as net\n",
    "import ResNet2 as tnet\n",
    "# import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "data_dir = './data/toy2a'\n",
    "model_dir = './models/toy2a'\n",
    "result_dir = './result/toy2a'\n",
    "\n",
    "#load data\n",
    "train_data = torch.tensor(np.load(os.path.join(data_dir, 'train_data.npy')))\n",
    "val_data = torch.tensor(np.load(os.path.join(data_dir, 'val_data.npy')))\n",
    "test_data = torch.tensor(np.load(os.path.join(data_dir, 'test_data.npy')))\n",
    "\n",
    "data_of_sizes = {}\n",
    "current_size = 2\n",
    "unresolved_dict = {}\n",
    "model_keep = list()\n",
    "model_used_dict = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_size =  32\n",
      "x_end_idx =  33\n",
      "y_start_idx =  64\n",
      "y_end_idx =  194\n",
      "range(0, 33, 32)\n",
      "self.train_x shape =  torch.Size([100, 2, 64])\n",
      "train_ys shape =  torch.Size([100, 5, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16kat\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\utils_time2.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.train_x = torch.tensor(train_data[:, x_idx:x_end_idx:step_size, :]).float().to(self.device)\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\utils_time2.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.train_ys = torch.tensor(train_data[:, y_start_idx:y_end_idx:step_size, :]).float().to(self.device)\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\utils_time2.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.val_x = torch.tensor(val_data[:, x_idx:x_end_idx:step_size, :]).float().to(self.device)\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\utils_time2.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.val_ys = torch.tensor(val_data[:, y_start_idx:y_end_idx:step_size, :]).float().to(self.device)\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\utils_time2.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.test_x = torch.tensor(test_data[:, 0, :]).float().to(self.device)\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\utils_time2.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.test_ys = torch.tensor(test_data[:, 1:, :]).float().to(self.device)\n"
     ]
    }
   ],
   "source": [
    "#testing dataset new structure\n",
    "dt = 1\n",
    "step_size = 32\n",
    "n_forward = 5\n",
    "dataset = tnet.DataSet(torch.flatten(train_data,2,3), torch.flatten(val_data,2,3), torch.flatten(test_data,2,3), dt, step_size, n_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1500, 1, 64])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.train_ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions, will move these to a utils file eventually \n",
    "#====================================================================================\n",
    "# def data_of_size(data,size):\n",
    "#     \"\"\"\n",
    "#     Takes averages to shrink size of data\n",
    "#     Takes data of size (n_points, dim, dim) and shrinks to size (n_points, size, size)\n",
    "#     takes averages to shrink\n",
    "#     \"\"\"\n",
    "#     return decrease_to_size(torch.tensor(data).unsqueeze(1), size)[:,0,:,:]\n",
    "#====================================================================================\n",
    "\n",
    "\n",
    "def isPowerOfTwo(n):\n",
    "    \"\"\"\n",
    "    checks if n is a power of two\n",
    "    \n",
    "    input: n, int\n",
    "    \n",
    "    output: boolean\n",
    "    \"\"\"\n",
    "    return (np.ceil(np.log2(n)) == np.floor(np.log2(n)));\n",
    "#====================================================================================\n",
    "def shrink(data, low_dim):\n",
    "    '''\n",
    "    Shrinks data to certain size; either averages or takes endpoints\n",
    "    \n",
    "    inputs:\n",
    "        data: array of size (n_points, n_timesteps, dim, dim) that will shrink\n",
    "        low_dim: int, size to shrink to, low_dim must be less than or equal to dim\n",
    "        \n",
    "    output:\n",
    "        data: array of size (n_points, n_timesteps, low_dim, low_dim)\n",
    "    '''\n",
    "    \n",
    "    #check inputs\n",
    "    assert len(data.shape) == 4\n",
    "    n_points, n_timesteps, dim, _ = data.shape\n",
    "    assert dim >= low_dim\n",
    "    assert isPowerOfTwo(low_dim)\n",
    "    \n",
    "    if dim == low_dim: #same size, no change\n",
    "        return data\n",
    "    \n",
    "    while(dim > low_dim):\n",
    "        #shrink by 1 level until same size\n",
    "        data = apply_local_op(data.float(), 'cpu', ave=average)\n",
    "        current_size = data.shape[-1]\n",
    "        \n",
    "    return data\n",
    "#====================================================================================\n",
    "def ave_one_level(data):\n",
    "    '''\n",
    "    takes averages to shrink data 1 level\n",
    "    \n",
    "    inputs:\n",
    "        data: tensor of size (n_points, n_timesteps, dim, dim) that will shrink\n",
    "        \n",
    "    output:\n",
    "        processed data: tensor of size (n_points, n_timesteps, dim/2, dim/2)\n",
    "    '''\n",
    "    device = 'cpu'\n",
    "    if not torch.is_tensor(data): #needs to be a tensor\n",
    "        data = torch.tensor(data)\n",
    "        \n",
    "    assert len(data.shape) == 4\n",
    "#     if data.shape != 4:\n",
    "#         print(\"data.shape = \", data.shape)\n",
    "#         print(\"data.shape should be of length 4\")\n",
    "    n_points, n_timesteps, dim, _ = data.shape\n",
    "    \n",
    "    #dim needs to be even \n",
    "    assert dim % 2 == 0\n",
    "    \n",
    "    data_right_size = torch.flatten(data, 0,1).unsqueeze(1).float()\n",
    "    \n",
    "#     n = min(in_channels, out_channels)\n",
    "    op = torch.nn.Conv2d(1, 1, 2, stride=2, padding=0).to(device)\n",
    "   \n",
    "    op.weight.data = torch.zeros(op.weight.data.size()).to(device)\n",
    "    op.bias.data = torch.zeros(op.bias.data.size()).to(device)\n",
    "    op.weight.data[0,0, :, :] = torch.ones(op.weight.data[0,0, :, :].size()).to(device) / 4\n",
    "\n",
    "    # make them non-trainable\n",
    "    for param in op.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    print(\"Transforming\")\n",
    "        \n",
    "    shrunk = op(data_right_size)\n",
    "    \n",
    "    print(\"reshape to print\")\n",
    "    \n",
    "    return shrunk.squeeze(1).reshape((n_points, n_timesteps, dim//2, dim//2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_data.shape)\n",
    "# processed = ave_one_level(train_data)\n",
    "# print(processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#make a dictionary with train data of every size 128->1\n",
    "#====================================================================================\n",
    "\n",
    "def make_dict_all_sizes(data):\n",
    "    \"\"\"\n",
    "    Makes a dictionary of data at every refinedment size from current->1\n",
    "    \n",
    "    inputs:\n",
    "        data: tensor(or array) of size (n_points, n_timesteps, dim, dim)\n",
    "        \n",
    "    outputs: \n",
    "        dic: dictionary of tensors. Keys are dim size, tensors are size (n_points, n_timesteps, dim, dim)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n_points, n_timesteps, dim, _ = data.shape\n",
    "    \n",
    "    if not torch.is_tensor(data): #needs to be a tensor\n",
    "        data = torch.tensor(data)\n",
    "        \n",
    "    assert isPowerOfTwo(dim)\n",
    "        \n",
    "    dic = {str(dim): data}\n",
    "    \n",
    "    for i in range(int(np.log2(dim))):\n",
    "        #decrease\n",
    "        print(\"i = \", i)\n",
    "        data = ave_one_level(data)\n",
    "        dic[str(data.shape[-1])] = data\n",
    "    \n",
    "    print(dic.keys())\n",
    "    \n",
    "    return dic\n",
    "#====================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  0\n",
      "Transforming\n",
      "reshape to print\n",
      "i =  1\n",
      "Transforming\n",
      "reshape to print\n",
      "i =  2\n",
      "Transforming\n",
      "reshape to print\n",
      "dict_keys(['8', '4', '2', '1'])\n",
      "i =  0\n",
      "Transforming\n",
      "reshape to print\n",
      "i =  1\n",
      "Transforming\n",
      "reshape to print\n",
      "i =  2\n",
      "Transforming\n",
      "reshape to print\n",
      "dict_keys(['8', '4', '2', '1'])\n"
     ]
    }
   ],
   "source": [
    "train_dict = make_dict_all_sizes(train_data)\n",
    "val_dict = make_dict_all_sizes(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================\n",
    "def train_one_timestep(step_size, train_data, val_data, test_data, current_size, \n",
    "                       dt = 1, n_forward = 5, noise=0, make_new = False, dont_train = True, \n",
    "                       lr = 1e-3, max_epochs = 10000, batch_size = 50,threshold = 1e-4, \n",
    "                       model_dir = './models/toy2',i=None, j = None,print_every=1000):\n",
    "\n",
    "    \"\"\"\n",
    "    fits or loads model at 1 timestep\n",
    "    \n",
    "    inputs:\n",
    "        step_size: int \n",
    "        train_data: tensor size (n_points, n_timesteps, dim**2) \n",
    "        val_data:tensor size (n_val_points, n_timesteps, dim**2) \n",
    "        test_data:tensor size (n_test_points, n_timesteps, dim**2) \n",
    "        current_size: int, only used in file naming\n",
    "        dt = 1: float\n",
    "        n_forward = 5: int, number of steps to consider during training\n",
    "        noise=0: float, level of noise, (right now just used in file naming)\n",
    "        make_new = False: boolean, whether or not to make a new model if old already exists\n",
    "        dont_train = True: boolean, whether or not to train more if model loaded\n",
    "        lr = 1e-3: float, learning rate\n",
    "        max_epochs = 10000: int \n",
    "        batch_size = 50: int\n",
    "        threshold=1e-4: float, stop training when validation gets below threshold\n",
    "         \n",
    "    \n",
    "    outputs:\n",
    "        model_time: ResNet object of trained model. Also saved\n",
    "    \"\"\"\n",
    "    if (i is not None) and (j is not None):\n",
    "        \n",
    "        model_name = 'model_L{}_D{}_noise{}_i{}_j{}.pt'.format(current_size,step_size, noise, i, j)\n",
    "    else:\n",
    "        model_name = 'model_L{}_D{}_noise{}.pt'.format(current_size,step_size, noise)\n",
    "    model_path_this = os.path.join(model_dir, model_name)\n",
    "    \n",
    "    n_points, n_timesteps, total_dim = train_data.shape\n",
    "    arch = [total_dim, 128, 128, 128, total_dim] \n",
    "    \n",
    "    try: #if we already have a model saved\n",
    "        if make_new:\n",
    "            print(\"Making a new model. Old one deleted. model {}\".format(model_name))\n",
    "            assert False\n",
    "        model_time = torch.load(model_path_this)\n",
    "        print(\"model loaded: \", model_name)\n",
    "        print(\"don't train = \", dont_train)\n",
    "        if dont_train: #just load model, no training\n",
    "            return model_time\n",
    "    except:\n",
    "        print('create model {} ...'.format(model_name))\n",
    "        model_time = tnet.ResNet(arch=arch, dt=dt, step_size=step_size)\n",
    "\n",
    "    dataset = tnet.DataSet(train_data, val_data, test_data, dt, step_size, n_forward)\n",
    "    \n",
    "#     #plot the inputed data\n",
    "#     plt.figure()\n",
    "#     plt.plot(dataset.val_ys[0, :, 0])#, '.')\n",
    "# #     plt.plot(np.arange(len(dataset.val_ys[point_num, :, i]))*dataset.step_size, dataset.val_ys[point_num, :, i], '.')\n",
    "# #     plt.plot(torch.flatten(val_data, 2,3)[0,s_size:,0])\n",
    "#     plt.title('step size = '+str(step_size))\n",
    "#     #   plt.xlim([0,100])\n",
    "#     plt.show()\n",
    "    \n",
    "    # training\n",
    "    model_time.train_net(dataset, max_epoch=max_epochs, batch_size=batch_size, lr=lr,\n",
    "                    model_path=model_path_this,threshold= threshold,print_every=print_every)\n",
    "    \n",
    "    return model_time\n",
    "#====================================================================================\n",
    "\n",
    "def find_best_timestep(train_data, val_data, test_data, current_size, start_k = 0, largest_k = 7, \n",
    "                       dt = 1, n_forward = 5, noise=0, make_new = False, dont_train = True,\n",
    "                       lr = 1e-3, max_epochs = 10000, batch_size = 50,threshold = 1e-4, \n",
    "                       criterion = torch.nn.MSELoss(reduction='none'), model_dir = \"./models/toy2\",\n",
    "                       i=None, j = None,print_every= 1000):\n",
    "    \"\"\"\n",
    "    Trains models with different timestep sizes and finds lowest error\n",
    "    \n",
    "    inputs:\n",
    "     n_forward = 5, noise=0, make_new = False, dont_train = False):\n",
    "    \n",
    "        train_data: tensor size (n_points, n_timesteps, dim, dim), or  size (n_points, n_timesteps)\n",
    "        val_data:tensor size (n_val_points, n_timesteps, dim, dim) , or  size (n_val_points, n_timesteps)\n",
    "        test_data:tensor size (n_test_points, n_timesteps, dim, dim) , or  size (n_test_points, n_timesteps)\n",
    "        current_size: int, only used in file naming\n",
    "        start_k = 0: int, smallest timestep will be 2**start_k\n",
    "        largest_k = 7:int, largest timestep will be 2**largest_k\n",
    "        dt = 1: float\n",
    "        n_forward = 5: int, number of steps to consider during training\n",
    "        noise=0: float, level of noise, (right now just used in file naming)\n",
    "        make_new = False: boolean, whether or not to make a new model if old already exists\n",
    "        dont_train = False: boolean, whether or not to train more if model loaded\n",
    "        lr = 1e-3: float, learning rate\n",
    "        max_epochs = 10000: int \n",
    "        batch_size = 50: int\n",
    "        threshold=1e-4: float\n",
    "        criterion = torch.nn.MSELoss(reduction='none'))\n",
    "         \n",
    "         \n",
    "    outputs:\n",
    "        models: list of ResNet models\n",
    "        step_sizes: list of ints for the steps_sizes of models \n",
    "        mse_list: list of floats, mse of models \n",
    "        idx_lowest: int, index value with lowest mse\n",
    "         \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #transform data shapes if needed\n",
    "    if(len(train_data.shape)== 2):\n",
    "        train_data = train_data.unsqueeze(2).unsqueeze(3)\n",
    "        val_data = val_data.unsqueeze(2).unsqueeze(3)\n",
    "        test_data = test_data.unsqueeze(2).unsqueeze(3)\n",
    "    assert(len(train_data.shape)== 4)\n",
    "    assert(len(val_data.shape)== 4)\n",
    "    assert(len(test_data.shape)== 4)\n",
    "    \n",
    "    models = list()\n",
    "    step_sizes = list()\n",
    "    n_forward_list = list()\n",
    "    mse_lowest = 1e10 #big number\n",
    "    mse_list = list()\n",
    "    mse_less = 0\n",
    "    idx_lowest = -1\n",
    "    \n",
    "    #make data flat to right dim (n_points, n_timesteps, dim**2)\n",
    "    train_data = torch.flatten(train_data, 2,3)\n",
    "    val_data = torch.flatten(val_data, 2,3)\n",
    "    test_data = torch.flatten(test_data, 2,3)\n",
    "    \n",
    "    n_points, n_timesteps, total_dim = train_data.shape\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, k in enumerate(range(start_k, largest_k)):\n",
    "        step_size = 2**k\n",
    "        step_sizes.append(step_size)\n",
    "        \n",
    "        #going to make n_forward the max if can be \n",
    "#         n_forward = int((np.floor(n_timesteps/step_size)-1)/2)\n",
    "#         n_forward_list.append(n_forward)\n",
    "#         print(\"n_forward = \", n_forward)\n",
    "        \n",
    "        model_time = train_one_timestep(step_size, train_data, val_data, test_data, current_size, \n",
    "                                        make_new = make_new, dont_train = dont_train,i=i, j=j, \n",
    "                                        n_forward=n_forward, max_epochs=max_epochs,model_dir=model_dir, print_every = print_every)\n",
    "        \n",
    "        \n",
    "        models.append(model_time)\n",
    "    \n",
    "        #find error\n",
    "        \n",
    "        y_preds = model_time.uni_scale_forecast(val_data[:, 0, :].float(), n_steps=n_timesteps-1)\n",
    "        mse_all = criterion(val_data[:, 1:, :].float(), y_preds).mean(-1)\n",
    "\n",
    "        mean = mse_all.mean(0).detach().numpy()\n",
    "#         print(mean.shape)\n",
    "        mse_less = mean.mean()\n",
    "        mse_list.append(mse_less)\n",
    "\n",
    "        print(\"mse_lowest = \", mse_lowest)\n",
    "        print(\"mse_less= \", mse_less)\n",
    "        \n",
    "        if (mse_less< mse_lowest) or (math.isnan(mse_lowest)) or (math.isnan(mse_less)):\n",
    "            mse_lowest = mse_less\n",
    "            idx_lowest = idx\n",
    "\n",
    "    return models, step_sizes, mse_list, idx_lowest, n_forward_list\n",
    "#====================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded:  model_L1_D4_noise0.pt\n",
      "don't train =  True\n",
      "mse_lowest =  10000000000.0\n",
      "mse_less=  0.08014287\n",
      "model loaded:  model_L1_D8_noise0.pt\n",
      "don't train =  True\n",
      "mse_lowest =  0.08014287\n",
      "mse_less=  0.0778048\n",
      "model loaded:  model_L1_D16_noise0.pt\n",
      "don't train =  True\n",
      "mse_lowest =  0.0778048\n",
      "mse_less=  0.07775622\n",
      "model loaded:  model_L1_D32_noise0.pt\n",
      "don't train =  True\n",
      "mse_lowest =  0.07775622\n",
      "mse_less=  0.079275616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\ResNet.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  preds.insert(0, torch.tensor(x_init).float().to(self.device))\n"
     ]
    }
   ],
   "source": [
    "current_size = 1\n",
    "models, step_sizes, mse_list, idx_lowest,n_forward_list = find_best_timestep(train_dict[str(current_size)], \n",
    "                                                              val_dict[str(current_size)], \n",
    "                                                              val_dict[str(current_size)], current_size,model_dir=model_dir,# make_new=True, print_every=100, \n",
    "                                                             start_k=2, largest_k = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1500/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.n_dim=  1\n",
      "dataset.n_dim =  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16kat\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:446: UserWarning: Using a target size (torch.Size([32, 1, 1])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\16kat\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:446: UserWarning: Using a target size (torch.Size([1240, 1, 1])) that is different to the input size (torch.Size([1240, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> new model saved @ epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\ResNet.py:235: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  preds.insert(0, torch.tensor(x_init).float().to(device))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.n_dim=  1\n",
      "dataset.n_dim =  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16kat\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:446: UserWarning: Using a target size (torch.Size([32, 1, 1])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\16kat\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:446: UserWarning: Using a target size (torch.Size([1240, 1, 1])) that is different to the input size (torch.Size([1240, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> new model saved @ epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\ResNet.py:235: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  preds.insert(0, torch.tensor(x_init).float().to(device))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.n_dim=  1\n",
      "dataset.n_dim =  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16kat\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:446: UserWarning: Using a target size (torch.Size([32, 1, 1])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\16kat\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:446: UserWarning: Using a target size (torch.Size([1240, 1, 1])) that is different to the input size (torch.Size([1240, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> new model saved @ epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\ResNet.py:235: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  preds.insert(0, torch.tensor(x_init).float().to(device))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-7506895da2c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m#     plt.plot(dataset.val_ys[0, :, 0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti_scale_forecast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_ys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_forward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_ys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\ResNet.py\u001b[0m in \u001b[0;36mmulti_scale_forecast\u001b[1;34m(x_init, n_steps, models)\u001b[0m\n\u001b[0;32m    252\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcur_step\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mend_step\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m                 \u001b[0mcur_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m                 \u001b[0mtmp_steps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m                 \u001b[0mtmp_preds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\ResNet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x_init)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnext\u001b[0m \u001b[0mstep\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0mx\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \"\"\"\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mx_init\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'increment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_init\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0muni_scale_forecast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\ResNet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \"\"\"\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_layers\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Linear_{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;31m# no nonlinear activations in the last layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Linear_{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_layers\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1688\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1689\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1690\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1691\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# #plot a bunch\n",
    "# # \n",
    "# step_size = 4\n",
    "# dt =1\n",
    "# n_forward = int(500/step_size - 1)\n",
    "# model_name = 'model_L{}_D{}_noise{}.pt'.format(current_size,step_size, 0)\n",
    "# model_path_this = os.path.join(model_dir, model_name)\n",
    "\n",
    "# n_points, n_timesteps, total_dim = torch.flatten(train_dict[str(current_size)], 2,3).shape\n",
    "# arch = [total_dim, 128, 128, 128, total_dim] \n",
    "\n",
    "\n",
    "# model_time = tnet.ResNet(arch=arch, dt=dt, step_size=step_size)\n",
    "\n",
    "# dataset = tnet.DataSet(torch.flatten(train_dict[str(current_size)], 2,3), torch.flatten(val_dict[str(current_size)], 2,3), \n",
    "#                        torch.flatten(val_dict[str(current_size)], 2,3), dt, step_size, n_forward)\n",
    "\n",
    "# #plot the inputed data\n",
    "# plt.figure()\n",
    "# plt.plot(dataset.val_ys[0, :, 0])#, '.')\n",
    "# #     plt.plot(np.arange(len(dataset.val_ys[point_num, :, i]))*dataset.step_size, dataset.val_ys[point_num, :, i], '.')\n",
    "# #     plt.plot(torch.flatten(val_data, 2,3)[0,s_size:,0])\n",
    "# plt.title('step size = '+str(step_size)+\" start\")\n",
    "# #   plt.xlim([0,100])\n",
    "# plt.show()\n",
    "\n",
    "# # training\n",
    "# for i in range(10):\n",
    "#     model_time.train_net(dataset, batch_size = 32, max_epoch=10,model_path=model_path_this)\n",
    "#     models = list()\n",
    "#     models.append(model_time)\n",
    "#     plt.figure()\n",
    "# #     plt.plot(dataset.val_ys[0, :, 0])\n",
    "#     predicted = tnet.multi_scale_forecast(dataset.val_ys[:,0,:],n_forward, models)\n",
    "#     plt.plot(dataset.val_ys[0,:,0], '.')\n",
    "#     plt.plot(predicted[0,:,0])\n",
    "#     plt.title('i = '+str(i))\n",
    "#     #   plt.xlim([0,100])\n",
    "#     plt.show()\n",
    "\n",
    "# # return model_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "[4, 8, 16, 32] [0.08014287, 0.0778048, 0.07775622, 0.079275616]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-1bc4c9806569>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_preds = model.uni_scale_forecast(torch.tensor(data[:,0,:]).float(), n_steps=total_steps-1)\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\ResNet.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  preds.insert(0, torch.tensor(x_init).float().to(self.device))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#====================================================================================\n",
    "def plot_lowest_error(data, model, i = 0):\n",
    "    \"\"\"\n",
    "    Plot data at model, idx\n",
    "    \n",
    "    inputs:\n",
    "        data: tensor of shape (n_points, n_timesteps, dim, dim)\n",
    "        model: Resnet model to predict on \n",
    "        i: int, which validation point to graph\n",
    "    outputs:\n",
    "        No returned values, but graph shown\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    data  = torch.flatten(data, 2,3)\n",
    "    _, total_steps, _ = data.shape\n",
    "    y_preds = model.uni_scale_forecast(torch.tensor(data[:,0,:]).float(), n_steps=total_steps-1)\n",
    "    plt.plot(y_preds[i,:,0], label = \"Predicted\")\n",
    "    plt.plot(data[i,1:,0], label = \"Truth\")\n",
    "    plt.ylim([-.1, 1.1])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#====================================================================================\n",
    "\n",
    "print(step_sizes[idx_lowest])    \n",
    "print(step_sizes, mse_list)\n",
    "plot_lowest_error(val_dict[str(current_size)], models[idx_lowest], i =2)\n",
    "\n",
    "# print(train_data.shape)\n",
    "# dataset = tnet.DataSet(torch.flatten(train_data, 2,3), torch.flatten(val_data, 2,3), \n",
    "#                        torch.flatten(val_data, 2,3), 1, step_sizes[idx_lowest], 5)\n",
    "# dataset.plot_val_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_val_data(dataset, point_num = 0, i = 0, other_plot = None):\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(len(dataset.val_ys[point_num, :, i]))*dataset.step_size, dataset.val_ys[point_num, :, i])#, '.')\n",
    "        if other_plot is not None:\n",
    "            plt.plot(other_plot)\n",
    "#             plt.xlim([0,100])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset.train_x.shape)\n",
    "# print(128**2)\n",
    "train_data = train_dict['1']\n",
    "val_data = val_dict['1']\n",
    "plt.plot(val_data[0,:,0,0])\n",
    "print(train_data.shape)\n",
    "s_size = 8\n",
    "dataset = tnet.DataSet(torch.flatten(train_data, 2,3), torch.flatten(val_data, 2,3), \n",
    "                       torch.flatten(val_data, 2,3), 1, s_size, int(np.floor(499/s_size)))\n",
    "# plot_val_data(dataset, other_plot=torch.flatten(val_data, 2,3)[0,s_size:,0])\n",
    "point_num = 0\n",
    "i = 0\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(dataset.val_ys[point_num, :, i]))*dataset.step_size, dataset.val_ys[point_num, :, i])#, '.')\n",
    "plt.plot(np.arange(len(dataset.val_ys[point_num, :, i]))*dataset.step_size, dataset.val_ys[point_num, :, i], '.')\n",
    "plt.plot(torch.flatten(val_data, 2,3)[0,s_size:,0])\n",
    "#   plt.xlim([0,100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 32\n",
    "n_forward = int(500/step_size - 1)\n",
    "current_size = 1\n",
    "train_data = train_dict[str(current_size)]\n",
    "val_data = val_dict[str(current_size)]\n",
    "model_time = train_one_timestep(step_size, torch.flatten(train_data, 2,3), \n",
    "                                         torch.flatten(val_data, 2,3),  torch.flatten(val_data, 2,3), \n",
    "                                         1, n_forward=n_forward,  max_epochs = 1000,)\n",
    "dataset = tnet.DataSet(torch.flatten(train_data, 2,3), torch.flatten(val_data, 2,3), \n",
    "                       torch.flatten(val_data, 2,3), 1, step_size, n_forward)\n",
    "# print(model_time(dataset.val_ys[:,0,:],n_forward).shape)\n",
    "models = list()\n",
    "models.append(model_time)\n",
    "predicted = tnet.multi_scale_forecast(dataset.val_ys[:,0,:],n_forward, models)\n",
    "plt.plot(dataset.val_ys[0,:,0], '.')\n",
    "plt.plot(predicted[0,:,0])\n",
    "# plt.xlim([0,500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================\n",
    "def find_error_4(data, model, truth_data, tol = 1e-5):\n",
    "    \"\"\"\n",
    "    Find error over the 4 squares \n",
    "    \n",
    "    inputs:\n",
    "        data: tensor of size (n_points, n_timesteps, dim, dim) to be predicted or size (n_points, n_timesteps)\n",
    "        model: Resnet object to predict data on\n",
    "        truth_data: tensor of size (n_points, n_timesteps, dim_larger, dim_larger) compared on \n",
    "        tol = 1e-5: tolerance level to mark points as resolved or not\n",
    "        criterion = torch.nn.MSELoss(reduction='none')\n",
    "        \n",
    "    outputs:\n",
    "        resolved: boolean whether complete area is resolved or not\n",
    "        loss: array of floats for size (dim, dim) with mse of each square\n",
    "        unresolved: array of booleans, whether that part is resolved or not. (1 unresolved, 0 resolved)\n",
    "    \"\"\"\n",
    "    if(len(data.shape))==2:\n",
    "        data = data.unsqueeze(2).unsqueeze(3)\n",
    "    assert len(data.shape) == 4\n",
    "    n_points, n_timesteps, dim, _ = data.shape\n",
    "    data  = torch.flatten(data, 2,3)\n",
    "    y_preds = model.uni_scale_forecast(torch.tensor(data[:,0,:]).float(), n_steps=n_timesteps-1).reshape(( n_points, n_timesteps-1, dim,dim))\n",
    "    \n",
    "    _,_, truth_dim, _ = truth_data.shape\n",
    "    assert truth_dim >= dim\n",
    "    \n",
    "    loss = mse(y_preds, truth_data[:,1:])\n",
    "    \n",
    "    resolved =  loss.max() <= tol\n",
    "    unresolved_array = torch.tensor(loss <= tol)\n",
    "    \n",
    "    return resolved, loss, 1-unresolved_array.float()\n",
    "\n",
    "\n",
    "\n",
    "#====================================================================================    \n",
    "    \n",
    "def mse(data1, data2):\n",
    "    \"\"\"\n",
    "    Finds Mean Squared Error between data1 and data2\n",
    "    \n",
    "    inputs:\n",
    "        data1: tensor of shape (n_points, n_timestep, dim1, dim1)\n",
    "        data2: tensor of shape (n_points, n_timestep, dim2, dim2)\n",
    "        \n",
    "    output:\n",
    "        mse: array of size (min_dim, min_dim) with mse \n",
    "    \n",
    "    \"\"\"\n",
    "    #find bigger dim\n",
    "    size1 = data1.shape[-1]\n",
    "    size2 = data2.shape[-1]\n",
    "    size_max = max(size1, size2)\n",
    "    \n",
    "    #grow to save sizes and find mse\n",
    "    mse = np.mean((grow(data1, size_max) - grow(data2, size_max))**2, axis = (0, 1))\n",
    "    return mse\n",
    "#====================================================================================\n",
    "    \n",
    "def grow(data, dim_full=128):\n",
    "    '''\n",
    "    Grow tensor from any size to a bigger size\n",
    "    inputs: \n",
    "        data: tensor to grow, size (n_points, n_timesteps, dim_small, dim_small)\n",
    "        dim_full = 128: int of size to grow data to\n",
    "\n",
    "    outputs:\n",
    "        data_full: tensor size (n_points, n_timesteps, size_full, size_full)\n",
    "    '''\n",
    "    n_points, n_timesteps, dim_small, _ = data.shape \n",
    "    assert dim_full % dim_small == 0 #need small to be multiple of full\n",
    "\n",
    "    divide = dim_full // dim_small\n",
    "\n",
    "    data_full = np.zeros((n_points, n_timesteps, dim_full,dim_full))\n",
    "    for i in range(dim_small):\n",
    "        for j in range(dim_small):\n",
    "            repeated = np.repeat(np.repeat(data[:,:,i,j].reshape(n_points,n_timesteps,1,1), divide, axis = 2), divide, axis = 3)\n",
    "            data_full[:,:,i*divide:(i+1)*divide, j*divide:(j+1)*divide] = repeated\n",
    "    return data_full\n",
    "#====================================================================================\n",
    "\n",
    "\n",
    "resolved, loss, unresolved_list = find_error_4(val_dict['1'], models[idx_lowest], val_dict['2'])\n",
    "print(loss.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unresolved_dict[str(current_size)] = torch.tensor(unresolved_list)\n",
    "\n",
    "print(unresolved_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_train_data = unresolved_list * train_dict[str(current_size*2)]\n",
    "print(next_train_data.shape)\n",
    "plt.imshow(next_train_data[0,0])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keep.append(models[idx_lowest])\n",
    "model_used_dict[str(current_size)] = [[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================\n",
    "def find_error_1(data, model, tol = 1e-5):\n",
    "    \"\"\"\n",
    "    Find error over the 1 square\n",
    "    \n",
    "    inputs:\n",
    "        data: tensor of size (n_points, n_timesteps, dim, dim) to be predicted\n",
    "        model: Resnet object to predict data on\n",
    "        tol = 1e-5: tolerance level to mark points as resolved or not\n",
    "        criterion = torch.nn.MSELoss(reduction='none')\n",
    "        \n",
    "    outputs:\n",
    "        loss: float of mse\n",
    "        resolved: boolean whether resolved or not\n",
    "    \"\"\"\n",
    "    n_points, n_timesteps  = data.shape\n",
    "    dim = 1\n",
    "    data_input  = data.unsqueeze(2)\n",
    "    y_preds = model.uni_scale_forecast(torch.tensor(data_input[:,0,:]).float(), n_steps=n_timesteps-1).reshape(( n_points, n_timesteps-1, dim,dim))\n",
    "    data1 = data[:,1:]\n",
    "    data2 = y_preds[:,:,0,0]\n",
    "#     print()\n",
    "    loss = torch.mean((data1-data2)**2)#mse(y_preds, data[:,1:])\n",
    "    \n",
    "#     print(loss)\n",
    "    \n",
    "    return loss, loss <= tol\n",
    "\n",
    "#====================================================================================\n",
    "\n",
    "\n",
    "current_size = 2\n",
    "next_train_data = unresolved_list * train_dict[str(current_size)]\n",
    "\n",
    "model_idx_list = np.ones((current_size, current_size))*(-1) #start with all -1\n",
    "\n",
    "for i in range(current_size):\n",
    "    for j in range(current_size):\n",
    "        data_this = next_train_data[:,:,i,j]\n",
    "        if (torch.min(data_this) == 0) and (torch.max(data_this) == 0):\n",
    "            #don't need to do anything is model is resolved\n",
    "            continue\n",
    "        else:\n",
    "        #see if the error is low enough on already made model\n",
    "            for m, model in enumerate(model_keep):\n",
    "                loss, resolved = find_error_1(data_this, model)\n",
    "                step_size = model.step_size\n",
    "                print(\"loss = \", loss)\n",
    "                print(\"step_size = \", step_size)\n",
    "                if resolved:\n",
    "                    model_idx_list[i,j] == m\n",
    "                    break\n",
    "                else:\n",
    "                    pass\n",
    "            if not resolved:\n",
    "                i = 0\n",
    "                j = 1\n",
    "                k = int(np.log2(step_size))\n",
    "                print(\"k = \", k)\n",
    "                print(\"train_dict[str(current_size)][:,:,i,j] shape = \", train_dict[str(current_size)][:,:,i,j].shape)\n",
    "                #if no model good, train new model\n",
    "                models, step_sizes, mse_list, idx_lowest = find_best_timestep(train_dict[str(current_size)][:,:,i,j], \n",
    "                                                              val_dict[str(current_size)][:,:,i,j], \n",
    "                                                              val_dict[str(current_size)][:,:,i,j], current_size,model_dir=model_dir,\n",
    "                                                              i=i, j=j, start_k = max(0,k-1), largest_k = k+2)\n",
    "                \n",
    "                vbnm\n",
    "                resolved, loss, unresolved_list = find_error_4(val_dict[str(current_size)][:,:,i,j], \n",
    "                                                               models[idx_lowest], \n",
    "                                                               val_dict[str(current_size)][:,:, i*current_size:(i+1)*current_size, j*current_size:(j+1)*current_size])\n",
    "                model_keep.append(models[idx_lowest])\n",
    "                model_idx_list[i,j] == len(model_keep) #last model will be the one for this square\n",
    "            \n",
    "#             predicted = model.uni_scale_forecast(torch.tensor(data[:,0,:]).float(), n_steps=n_timesteps-1).reshape((  n_points, n_timesteps-1, dim,dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(step_sizes, mse_list, idx_lowest)\n",
    "resolved, loss, unresolved_list = find_error_4(val_dict[str(current_size)][:,:,i,j], \n",
    "                                                               models[idx_lowest], \n",
    "                                                               val_dict[str(current_size*2)][:,:, i*current_size:(i+1)*current_size, j*current_size:(j+1)*current_size])\n",
    "print(loss)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  models[idx_lowest]\n",
    "print(idx_lowest)\n",
    "n_timesteps = 500\n",
    "n_points = 10\n",
    "dim = 1\n",
    "# plt.plot(model(val_dict[str(current_size)][:,:,i,j].unsqueeze(2).unsqueeze(3))[0,:,0,0].detach().numpy(), label = \"predicted\")\n",
    "print(val_dict[str(current_size)][:,0,i,j].unsqueeze(1).shape)\n",
    "val_data_this = val_dict[str(current_size)][:,0,i,j].unsqueeze(1)\n",
    "predicted = model.uni_scale_forecast(val_data_this, n_steps=n_timesteps-1)\n",
    "print(predicted.shape)\n",
    "predicted_reshape = predicted.reshape((  n_points, n_timesteps-1, dim,dim))\n",
    "plt.plot(predicted_reshape[0,:,0], label = \"predicted\")\n",
    "print(val_dict[str(current_size*2)][:,:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size].shape)\n",
    "# plt.plot(val_dict[str(current_size*2)][:,1:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size][0,:,0,0], label = \"Truth\")\n",
    "\n",
    "plt.plot(val_dict[str(current_size*2)][:,1:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size][0,:,0,1], label = \"Truth\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(val_dict[str(current_size*2)][0,0])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = (16+32)/2\n",
    "print(step_size)\n",
    "model = train_one_timestep(int(28), train_dict[str(current_size)][:,:,i,j].unsqueeze(2), \n",
    "                           val_dict[str(current_size)][:,:,i,j].unsqueeze(2), \n",
    "                           val_dict[str(current_size)][:,:,i,j].unsqueeze(2), current_size)\n",
    "#                        dt = 1, n_forward = 5, noise=0, make_new = False, dont_train = True, \n",
    "#                        lr = 1e-3, max_epochs = 10000, batch_size = 50,threshold = 1e-4, \n",
    "#                        model_dir = './models/toy2',i=None, j = None):\n",
    "    \n",
    "#     train_dict[str(current_size)][:,:,i,j], \n",
    "#                                                               val_dict[str(current_size)][:,:,i,j], \n",
    "#                                                               val_dict[str(current_size)][:,:,i,j], current_size,model_dir=model_dir, \n",
    "#                                                               i=i, j=j, start_k = max(0,k-1), largest_k = k+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 1\n",
    "resolved, loss, unresolved_list = find_error_4(val_dict[str(current_size)][:,:,i,j], \n",
    "                                                               model, \n",
    "                                                               val_dict[str(current_size*2)][:,:, i*current_size:(i+1)*current_size, j*current_size:(j+1)*current_size])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model =  models[idx_lowest]\n",
    "print(idx_lowest)\n",
    "n_timesteps = 500\n",
    "n_points = 10\n",
    "dim = 1\n",
    "# plt.plot(model(val_dict[str(current_size)][:,:,i,j].unsqueeze(2).unsqueeze(3))[0,:,0,0].detach().numpy(), label = \"predicted\")\n",
    "print(val_dict[str(current_size)][:,0,i,j].unsqueeze(1).shape)\n",
    "val_data_this = val_dict[str(current_size)][:,0,i,j].unsqueeze(1)\n",
    "predicted = model.uni_scale_forecast(val_data_this, n_steps=n_timesteps-1)\n",
    "print(predicted.shape)\n",
    "predicted_reshape = predicted.reshape((  n_points, n_timesteps-1, dim,dim))\n",
    "plt.plot(predicted_reshape[0,:,0], label = \"predicted\")\n",
    "print(val_dict[str(current_size*2)][:,:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size].shape)\n",
    "# plt.plot(val_dict[str(current_size*2)][:,1:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size][0,:,0,0], label = \"Truth\")\n",
    "\n",
    "plt.plot(val_dict[str(current_size*2)][:,1:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size][0,:,0,1], label = \"Truth\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
