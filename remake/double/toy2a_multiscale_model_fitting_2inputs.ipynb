{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multiscale model fitting for Toy2a\n",
    "\n",
    "Toy2a is a simplified version of toy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start with initalizing many things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "# import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from tqdm.notebook import tqdm\n",
    "# import time\n",
    "import math\n",
    "\n",
    "# module_path = os.path.abspath(os.path.join('../'))\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)\n",
    "    \n",
    "    \n",
    "# import torch_cae_multilevel_V4 as net\n",
    "import double_Resnet as tnet\n",
    "# import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "data_dir = '../data/toy2a'\n",
    "model_dir = '../models/toy2a'\n",
    "result_dir = '../result/toy2a'\n",
    "\n",
    "#load data\n",
    "train_data = torch.tensor(np.load(os.path.join(data_dir, 'train_data.npy')))\n",
    "val_data = torch.tensor(np.load(os.path.join(data_dir, 'val_data.npy')))\n",
    "test_data = torch.tensor(np.load(os.path.join(data_dir, 'test_data.npy')))\n",
    "\n",
    "data_of_sizes = {}\n",
    "current_size = 2\n",
    "unresolved_dict = {}\n",
    "model_keep = list()\n",
    "model_used_dict = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_size =  32\n",
      "x_end_idx =  33\n",
      "y_start_idx =  64\n",
      "y_end_idx =  225\n",
      "range(0, 33, 32)\n",
      "self.train_x shape =  torch.Size([100, 128])\n",
      "train_ys shape =  torch.Size([100, 6, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16kat\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\double\\utils_time2.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.train_x = torch.tensor(train_data[:, x_idx:x_end_idx:step_size, :]).float().to(self.device)\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\double\\utils_time2.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.train_ys = torch.tensor(train_data[:, y_start_idx:y_end_idx:step_size, :]).float().to(self.device)\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\double\\utils_time2.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.val_x = torch.tensor(val_data[:, x_idx:x_end_idx:step_size, :]).float().to(self.device)\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\double\\utils_time2.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.val_ys = torch.tensor(val_data[:, y_start_idx:y_end_idx:step_size, :]).float().to(self.device)\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\double\\utils_time2.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.test_x = torch.tensor(test_data[:, 0, :]).float().to(self.device)\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\double\\utils_time2.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.test_ys = torch.tensor(test_data[:, 1:, :]).float().to(self.device)\n"
     ]
    }
   ],
   "source": [
    "#testing dataset new structure\n",
    "dt = 1\n",
    "step_size = 32\n",
    "n_forward = 5\n",
    "dataset = tnet.DataSet(torch.flatten(train_data,2,3), torch.flatten(val_data,2,3), torch.flatten(test_data,2,3), dt, step_size, n_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 6, 64])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.train_ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions, will move these to a utils file eventually \n",
    "#====================================================================================\n",
    "# def data_of_size(data,size):\n",
    "#     \"\"\"\n",
    "#     Takes averages to shrink size of data\n",
    "#     Takes data of size (n_points, dim, dim) and shrinks to size (n_points, size, size)\n",
    "#     takes averages to shrink\n",
    "#     \"\"\"\n",
    "#     return decrease_to_size(torch.tensor(data).unsqueeze(1), size)[:,0,:,:]\n",
    "#====================================================================================\n",
    "\n",
    "\n",
    "def isPowerOfTwo(n):\n",
    "    \"\"\"\n",
    "    checks if n is a power of two\n",
    "    \n",
    "    input: n, int\n",
    "    \n",
    "    output: boolean\n",
    "    \"\"\"\n",
    "    return (np.ceil(np.log2(n)) == np.floor(np.log2(n)));\n",
    "#====================================================================================\n",
    "def shrink(data, low_dim):\n",
    "    '''\n",
    "    Shrinks data to certain size; either averages or takes endpoints\n",
    "    \n",
    "    inputs:\n",
    "        data: array of size (n_points, n_timesteps, dim, dim) that will shrink\n",
    "        low_dim: int, size to shrink to, low_dim must be less than or equal to dim\n",
    "        \n",
    "    output:\n",
    "        data: array of size (n_points, n_timesteps, low_dim, low_dim)\n",
    "    '''\n",
    "    \n",
    "    #check inputs\n",
    "    assert len(data.shape) == 4\n",
    "    n_points, n_timesteps, dim, _ = data.shape\n",
    "    assert dim >= low_dim\n",
    "    assert isPowerOfTwo(low_dim)\n",
    "    \n",
    "    if dim == low_dim: #same size, no change\n",
    "        return data\n",
    "    \n",
    "    while(dim > low_dim):\n",
    "        #shrink by 1 level until same size\n",
    "        data = apply_local_op(data.float(), 'cpu', ave=average)\n",
    "        current_size = data.shape[-1]\n",
    "        \n",
    "    return data\n",
    "#====================================================================================\n",
    "def ave_one_level(data):\n",
    "    '''\n",
    "    takes averages to shrink data 1 level\n",
    "    \n",
    "    inputs:\n",
    "        data: tensor of size (n_points, n_timesteps, dim, dim) that will shrink\n",
    "        \n",
    "    output:\n",
    "        processed data: tensor of size (n_points, n_timesteps, dim/2, dim/2)\n",
    "    '''\n",
    "    device = 'cpu'\n",
    "    if not torch.is_tensor(data): #needs to be a tensor\n",
    "        data = torch.tensor(data)\n",
    "        \n",
    "    print(\"data shape = \", data.shape)\n",
    "    assert len(data.shape) == 4\n",
    "#     if data.shape != 4:\n",
    "#         print(\"data.shape = \", data.shape)\n",
    "#         print(\"data.shape should be of length 4\")\n",
    "    n_points, n_timesteps, dim, _ = data.shape\n",
    "    \n",
    "    #dim needs to be even \n",
    "    assert dim % 2 == 0\n",
    "    \n",
    "    data_right_size = torch.flatten(data, 0,1).unsqueeze(1).float()\n",
    "    \n",
    "#     n = min(in_channels, out_channels)\n",
    "    op = torch.nn.Conv2d(1, 1, 2, stride=2, padding=0).to(device)\n",
    "   \n",
    "    op.weight.data = torch.zeros(op.weight.data.size()).to(device)\n",
    "    op.bias.data = torch.zeros(op.bias.data.size()).to(device)\n",
    "    op.weight.data[0,0, :, :] = torch.ones(op.weight.data[0,0, :, :].size()).to(device) / 4\n",
    "\n",
    "    # make them non-trainable\n",
    "    for param in op.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    print(\"Transforming\")\n",
    "        \n",
    "    shrunk = op(data_right_size)\n",
    "    \n",
    "    print(\"reshape to print\")\n",
    "    \n",
    "    return shrunk.squeeze(1).reshape((n_points, n_timesteps, dim//2, dim//2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 500, 8, 8])\n",
      "data shape =  torch.Size([100, 500, 8, 8])\n",
      "Transforming\n",
      "reshape to print\n",
      "torch.Size([100, 500, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "processed = ave_one_level(train_data)\n",
    "print(processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#make a dictionary with train data of every size 128->1\n",
    "#====================================================================================\n",
    "\n",
    "def make_dict_all_sizes(data):\n",
    "    \"\"\"\n",
    "    Makes a dictionary of data at every refinedment size from current->1\n",
    "    \n",
    "    inputs:\n",
    "        data: tensor(or array) of size (n_points, n_timesteps, dim, dim)\n",
    "        \n",
    "    outputs: \n",
    "        dic: dictionary of tensors. Keys are dim size, tensors are size (n_points, n_timesteps, dim, dim)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n_points, n_timesteps, dim, _ = data.shape\n",
    "    \n",
    "    if not torch.is_tensor(data): #needs to be a tensor\n",
    "        data = torch.tensor(data)\n",
    "        \n",
    "    assert isPowerOfTwo(dim)\n",
    "        \n",
    "    dic = {str(dim): data}\n",
    "    \n",
    "    for i in range(int(np.log2(dim))):\n",
    "        #decrease\n",
    "        print(\"i = \", i)\n",
    "        data = ave_one_level(data)\n",
    "        dic[str(data.shape[-1])] = data\n",
    "    \n",
    "    print(dic.keys())\n",
    "    \n",
    "    return dic\n",
    "#====================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  0\n",
      "data shape =  torch.Size([100, 500, 8, 8])\n",
      "Transforming\n",
      "reshape to print\n",
      "i =  1\n",
      "data shape =  torch.Size([100, 500, 4, 4])\n",
      "Transforming\n",
      "reshape to print\n",
      "i =  2\n",
      "data shape =  torch.Size([100, 500, 2, 2])\n",
      "Transforming\n",
      "reshape to print\n",
      "dict_keys(['8', '4', '2', '1'])\n",
      "i =  0\n",
      "data shape =  torch.Size([10, 500, 8, 8])\n",
      "Transforming\n",
      "reshape to print\n",
      "i =  1\n",
      "data shape =  torch.Size([10, 500, 4, 4])\n",
      "Transforming\n",
      "reshape to print\n",
      "i =  2\n",
      "data shape =  torch.Size([10, 500, 2, 2])\n",
      "Transforming\n",
      "reshape to print\n",
      "dict_keys(['8', '4', '2', '1'])\n"
     ]
    }
   ],
   "source": [
    "train_dict = make_dict_all_sizes(train_data)\n",
    "val_dict = make_dict_all_sizes(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================\n",
    "def train_one_timestep(step_size, train_data, val_data, test_data, current_size, \n",
    "                       dt = 1, n_forward = 5, noise=0, make_new = False, dont_train = True, \n",
    "                       lr = 1e-3, max_epochs = 10000, batch_size = 50,threshold = 1e-4, \n",
    "                       model_dir = './models/toy2',i=None, j = None,print_every=1000):\n",
    "\n",
    "    \"\"\"\n",
    "    fits or loads model at 1 timestep\n",
    "    \n",
    "    inputs:\n",
    "        step_size: int \n",
    "        train_data: tensor size (n_points, n_timesteps, dim**2) \n",
    "        val_data:tensor size (n_val_points, n_timesteps, dim**2) \n",
    "        test_data:tensor size (n_test_points, n_timesteps, dim**2) \n",
    "        current_size: int, only used in file naming\n",
    "        dt = 1: float\n",
    "        n_forward = 5: int, number of steps to consider during training\n",
    "        noise=0: float, level of noise, (right now just used in file naming)\n",
    "        make_new = False: boolean, whether or not to make a new model if old already exists\n",
    "        dont_train = True: boolean, whether or not to train more if model loaded\n",
    "        lr = 1e-3: float, learning rate\n",
    "        max_epochs = 10000: int \n",
    "        batch_size = 50: int\n",
    "        threshold=1e-4: float, stop training when validation gets below threshold\n",
    "         \n",
    "    \n",
    "    outputs:\n",
    "        model_time: ResNet object of trained model. Also saved\n",
    "    \"\"\"\n",
    "    if (i is not None) and (j is not None):\n",
    "        \n",
    "        model_name = 'model_L{}_D{}_noise{}_i{}_j{}.pt'.format(current_size,step_size, noise, i, j)\n",
    "    else:\n",
    "        model_name = 'model_L{}_D{}_noise{}.pt'.format(current_size,step_size, noise)\n",
    "    model_path_this = os.path.join(model_dir, model_name)\n",
    "    \n",
    "    n_points, n_timesteps, total_dim = train_data.shape\n",
    "    arch = [total_dim*2, 128, 128, 128, total_dim] \n",
    "    \n",
    "    try: #if we already have a model saved\n",
    "        if make_new:\n",
    "            print(\"Making a new model. Old one deleted. model {}\".format(model_name))\n",
    "            assert False\n",
    "        model_time = torch.load(model_path_this)\n",
    "        print(\"model loaded: \", model_name)\n",
    "        print(\"don't train = \", dont_train)\n",
    "        if dont_train: #just load model, no training\n",
    "            return model_time\n",
    "    except:\n",
    "        print('create model {} ...'.format(model_name))\n",
    "        model_time = tnet.ResNet(arch=arch, dt=dt, step_size=step_size)\n",
    "\n",
    "    print(\"train_data before dataset\", train_data.shape)\n",
    "    dataset = tnet.DataSet(train_data, val_data, test_data, dt, step_size, n_forward)\n",
    "    print(\"train_x after\", dataset.train_x.shape)\n",
    "    \n",
    "#     #plot the inputed data\n",
    "#     plt.figure()\n",
    "#     plt.plot(dataset.val_ys[0, :, 0])#, '.')\n",
    "# #     plt.plot(np.arange(len(dataset.val_ys[point_num, :, i]))*dataset.step_size, dataset.val_ys[point_num, :, i], '.')\n",
    "# #     plt.plot(torch.flatten(val_data, 2,3)[0,s_size:,0])\n",
    "#     plt.title('step size = '+str(step_size))\n",
    "#     #   plt.xlim([0,100])\n",
    "#     plt.show()\n",
    "    \n",
    "    # training\n",
    "    model_time.train_net(dataset, max_epoch=max_epochs, batch_size=batch_size, lr=lr,\n",
    "                    model_path=model_path_this,threshold= threshold,print_every=print_every)\n",
    "    \n",
    "    return model_time\n",
    "#====================================================================================\n",
    "\n",
    "def find_best_timestep(train_data, val_data, test_data, current_size, start_k = 0, largest_k = 7, \n",
    "                       dt = 1, n_forward = 5, noise=0, make_new = False, dont_train = True,\n",
    "                       lr = 1e-3, max_epochs = 10000, batch_size = 50,threshold = 1e-4, \n",
    "                       criterion = torch.nn.MSELoss(reduction='none'), model_dir = \"./models/toy2\",\n",
    "                       i=None, j = None,print_every= 1000):\n",
    "    \"\"\"\n",
    "    Trains models with different timestep sizes and finds lowest error\n",
    "    \n",
    "    inputs:\n",
    "     n_forward = 5, noise=0, make_new = False, dont_train = False):\n",
    "    \n",
    "        train_data: tensor size (n_points, n_timesteps, dim, dim), or  size (n_points, n_timesteps)\n",
    "        val_data:tensor size (n_val_points, n_timesteps, dim, dim) , or  size (n_val_points, n_timesteps)\n",
    "        test_data:tensor size (n_test_points, n_timesteps, dim, dim) , or  size (n_test_points, n_timesteps)\n",
    "        current_size: int, only used in file naming\n",
    "        start_k = 0: int, smallest timestep will be 2**start_k\n",
    "        largest_k = 7:int, largest timestep will be 2**largest_k\n",
    "        dt = 1: float\n",
    "        n_forward = 5: int, number of steps to consider during training\n",
    "        noise=0: float, level of noise, (right now just used in file naming)\n",
    "        make_new = False: boolean, whether or not to make a new model if old already exists\n",
    "        dont_train = False: boolean, whether or not to train more if model loaded\n",
    "        lr = 1e-3: float, learning rate\n",
    "        max_epochs = 10000: int \n",
    "        batch_size = 50: int\n",
    "        threshold=1e-4: float\n",
    "        criterion = torch.nn.MSELoss(reduction='none'))\n",
    "         \n",
    "         \n",
    "    outputs:\n",
    "        models: list of ResNet models\n",
    "        step_sizes: list of ints for the steps_sizes of models \n",
    "        mse_list: list of floats, mse of models \n",
    "        idx_lowest: int, index value with lowest mse\n",
    "         \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #transform data shapes if needed\n",
    "    if(len(train_data.shape)== 2):\n",
    "        train_data = train_data.unsqueeze(2).unsqueeze(3)\n",
    "        val_data = val_data.unsqueeze(2).unsqueeze(3)\n",
    "        test_data = test_data.unsqueeze(2).unsqueeze(3)\n",
    "    assert(len(train_data.shape)== 4)\n",
    "    assert(len(val_data.shape)== 4)\n",
    "    assert(len(test_data.shape)== 4)\n",
    "    \n",
    "    models = list()\n",
    "    step_sizes = list()\n",
    "    n_forward_list = list()\n",
    "    mse_lowest = 1e10 #big number\n",
    "    mse_list = list()\n",
    "    mse_less = 0\n",
    "    idx_lowest = -1\n",
    "    \n",
    "    #make data flat to right dim (n_points, n_timesteps, dim**2)\n",
    "    train_data = torch.flatten(train_data, 2,3)\n",
    "    val_data = torch.flatten(val_data, 2,3)\n",
    "    test_data = torch.flatten(test_data, 2,3)\n",
    "    \n",
    "    n_points, n_timesteps, total_dim = train_data.shape\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, k in enumerate(range(start_k, largest_k)):\n",
    "        step_size = 2**k\n",
    "        step_sizes.append(step_size)\n",
    "        \n",
    "        #going to make n_forward the max if can be \n",
    "#         n_forward = int((np.floor(n_timesteps/step_size)-1)/2)\n",
    "#         n_forward_list.append(n_forward)\n",
    "#         print(\"n_forward = \", n_forward)\n",
    "        \n",
    "        print(\"train_data shape = \", train_data.shape)\n",
    "        model_time = train_one_timestep(step_size, train_data, val_data, test_data, current_size, \n",
    "                                        make_new = make_new, dont_train = dont_train,i=i, j=j, \n",
    "                                        n_forward=n_forward, max_epochs=max_epochs,model_dir=model_dir, print_every = print_every)\n",
    "        \n",
    "        \n",
    "        models.append(model_time)\n",
    "    \n",
    "        #find error\n",
    "        \n",
    "        y_preds = model_time.uni_scale_forecast(val_data[:, :2, 0].float(), n_steps=n_timesteps-1)\n",
    "        mse_all = criterion(val_data[:, 1:, :].float(), y_preds).mean(-1)\n",
    "\n",
    "        mean = mse_all.mean(0).detach().numpy()\n",
    "#         print(mean.shape)\n",
    "        mse_less = mean.mean()\n",
    "        mse_list.append(mse_less)\n",
    "\n",
    "        print(\"mse_lowest = \", mse_lowest)\n",
    "        print(\"mse_less= \", mse_less)\n",
    "        \n",
    "        if (mse_less< mse_lowest) or (math.isnan(mse_lowest)) or (math.isnan(mse_less)):\n",
    "            mse_lowest = mse_less\n",
    "            idx_lowest = idx\n",
    "\n",
    "    return models, step_sizes, mse_list, idx_lowest, n_forward_list\n",
    "#====================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_loss(model, x, ys, w=1.0):\n",
    "#         \"\"\"\n",
    "#         :param x: x batch, array of size batch_size x n_dim\n",
    "#         :param ys: ys batch, array of size batch_size x n_steps x n_dim\n",
    "#         :return: overall loss\n",
    "#         \"\"\"\n",
    "#         batch_size, n_steps, n_dim = ys.size()\n",
    "#         y_preds = torch.zeros(batch_size, n_steps, n_dim*2).float()#.to(self.device)\n",
    "#         y_prev = x\n",
    "#         for t in range(n_steps-1):\n",
    "#             y_next = model(y_prev)\n",
    "#             y_preds[:, t, :] = y_next\n",
    "#             print(\"y_next shap = = \", y_next.shape )\n",
    "#             y_prev[:,0] = y_prev[:,1]\n",
    "#             y_prev[:,1] = y_next[:,0]\n",
    "\n",
    "#         # compute loss\n",
    "#         criterion = torch.nn.MSELoss(reduction='none')\n",
    "#         loss = w * criterion(y_preds, ys).mean() + (1-w) * criterion(y_preds, ys).max()\n",
    "\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "step_size =  32\n",
      "x_end_idx =  33\n",
      "y_start_idx =  64\n",
      "y_end_idx =  225\n",
      "range(0, 33, 32)\n",
      "self.train_x shape =  torch.Size([100, 2])\n",
      "train_ys shape =  torch.Size([100, 6, 1])\n",
      "dataset ndim =  2\n",
      "torch.Size([100, 2])\n",
      "self.n_dim=  2\n",
      "dataset.n_dim =  2\n",
      "epoch 1, training loss 2.577415704727173, validation loss 2.716566801071167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16kat\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:446: UserWarning: Using a target size (torch.Size([100, 6, 1])) that is different to the input size (torch.Size([100, 6, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\16kat\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:446: UserWarning: Using a target size (torch.Size([10, 6, 1])) that is different to the input size (torch.Size([10, 6, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "current_size = 1\n",
    "train_data1 = torch.flatten(train_dict[str(current_size)], 2,3)\n",
    "val_data1 = torch.flatten(val_dict[str(current_size)], 2,3)\n",
    "# test_data1 = torch.flatten(val_dict[str(current_size)], 2,3)\n",
    "model_time = tnet.ResNet(arch=[2,3,3,4,1], dt=dt, step_size=step_size)\n",
    "print(model_time.n_dim )\n",
    "\n",
    "# print(\"train_data before dataset\", train_data.shape)\n",
    "dataset = tnet.DataSet(train_data1, val_data1, val_data1, dt, step_size, n_forward)\n",
    "print(\"dataset ndim = \", dataset.n_dim)\n",
    "print(dataset.train_x.shape)\n",
    "\n",
    "# calculate_loss(model_time,dataset.train_x, dataset.train_ys)\n",
    "\n",
    "model_time.train_net(dataset, 1, 100, w=1.0, lr=1e-3, model_path=None, threshold = 1e-8, print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3,],[4,5,6]])\n",
    "print(x.shape)\n",
    "print(x[:,0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape =  torch.Size([100, 500, 1])\n",
      "model loaded:  model_L1_D4_noise0.pt\n",
      "don't train =  False\n",
      "train_data before dataset torch.Size([100, 500, 1])\n",
      "step_size =  4\n",
      "x_end_idx =  5\n",
      "y_start_idx =  8\n",
      "y_end_idx =  29\n",
      "range(0, 5, 4)\n",
      "self.train_x shape =  torch.Size([100, 2])\n",
      "train_ys shape =  torch.Size([100, 6, 1])\n",
      "train_x after torch.Size([100, 2])\n",
      "self.n_dim=  2\n",
      "dataset.n_dim =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\double\\utils_time2.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.train_x = torch.tensor(train_data[:, x_idx:x_end_idx:step_size, :]).float().to(self.device)\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\double\\utils_time2.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.train_ys = torch.tensor(train_data[:, y_start_idx:y_end_idx:step_size, :]).float().to(self.device)\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\double\\utils_time2.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.val_x = torch.tensor(val_data[:, x_idx:x_end_idx:step_size, :]).float().to(self.device)\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\double\\utils_time2.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.val_ys = torch.tensor(val_data[:, y_start_idx:y_end_idx:step_size, :]).float().to(self.device)\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\double\\utils_time2.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.test_x = torch.tensor(test_data[:, 0, :]).float().to(self.device)\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\double\\utils_time2.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.test_ys = torch.tensor(test_data[:, 1:, :]).float().to(self.device)\n",
      "C:\\Users\\16kat\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:446: UserWarning: Using a target size (torch.Size([50, 6, 1])) that is different to the input size (torch.Size([50, 6, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\16kat\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:446: UserWarning: Using a target size (torch.Size([10, 6, 1])) that is different to the input size (torch.Size([10, 6, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, training loss 0.05530183017253876, validation loss 0.03641078621149063\n",
      "(--> new model saved @ epoch 100)\n",
      "epoch 200, training loss 0.044067639857530594, validation loss 0.036375146359205246\n",
      "(--> new model saved @ epoch 200)\n",
      "epoch 300, training loss 0.053435713052749634, validation loss 0.03636603057384491\n",
      "(--> new model saved @ epoch 300)\n",
      "epoch 400, training loss 0.04625026509165764, validation loss 0.03636180981993675\n",
      "(--> new model saved @ epoch 400)\n",
      "epoch 500, training loss 0.05846088379621506, validation loss 0.03635946288704872\n",
      "(--> new model saved @ epoch 500)\n",
      "x_prev shape =  torch.Size([10, 2])\n",
      "y_pred shape =  torch.Size([10, 1, 126])\n",
      "steps =  [0, 3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55, 59, 63, 67, 71, 75, 79, 83, 87, 91, 95, 99, 103, 107, 111, 115, 119, 123, 127, 131, 135, 139, 143, 147, 151, 155, 159, 163, 167, 171, 175, 179, 183, 187, 191, 195, 199, 203, 207, 211, 215, 219, 223, 227, 231, 235, 239, 243, 247, 251, 255, 259, 263, 267, 271, 275, 279, 283, 287, 291, 295, 299, 303, 307, 311, 315, 319, 323, 327, 331, 335, 339, 343, 347, 351, 355, 359, 363, 367, 371, 375, 379, 383, 387, 391, 395, 399, 403, 407, 411, 415, 419, 423, 427, 431, 435, 439, 443, 447, 451, 455, 459, 463, 467, 471, 475, 479, 483, 487, 491, 495, 499]\n",
      "len steps =  126\n",
      "mse_lowest =  10000000000.0\n",
      "mse_less=  0.110736966\n",
      "train_data shape =  torch.Size([100, 500, 1])\n",
      "model loaded:  model_L1_D8_noise0.pt\n",
      "don't train =  False\n",
      "train_data before dataset torch.Size([100, 500, 1])\n",
      "step_size =  8\n",
      "x_end_idx =  9\n",
      "y_start_idx =  16\n",
      "y_end_idx =  57\n",
      "range(0, 9, 8)\n",
      "self.train_x shape =  torch.Size([100, 2])\n",
      "train_ys shape =  torch.Size([100, 6, 1])\n",
      "train_x after torch.Size([100, 2])\n",
      "self.n_dim=  2\n",
      "dataset.n_dim =  2\n",
      "epoch 100, training loss 0.056707967072725296, validation loss 0.050215475261211395\n",
      "(--> new model saved @ epoch 100)\n",
      "epoch 200, training loss 0.05295469984412193, validation loss 0.05020826309919357\n",
      "(--> new model saved @ epoch 200)\n",
      "epoch 300, training loss 0.05284219607710838, validation loss 0.05020734295248985\n",
      "(--> new model saved @ epoch 300)\n",
      "epoch 400, training loss 0.04289093613624573, validation loss 0.050206366926431656\n",
      "(--> new model saved @ epoch 400)\n",
      "epoch 500, training loss 0.06455322355031967, validation loss 0.0502062626183033\n",
      "(--> new model saved @ epoch 500)\n",
      "x_prev shape =  torch.Size([10, 2])\n",
      "y_pred shape =  torch.Size([10, 1, 64])\n",
      "steps =  [0, 7, 15, 23, 31, 39, 47, 55, 63, 71, 79, 87, 95, 103, 111, 119, 127, 135, 143, 151, 159, 167, 175, 183, 191, 199, 207, 215, 223, 231, 239, 247, 255, 263, 271, 279, 287, 295, 303, 311, 319, 327, 335, 343, 351, 359, 367, 375, 383, 391, 399, 407, 415, 423, 431, 439, 447, 455, 463, 471, 479, 487, 495, 503]\n",
      "len steps =  64\n",
      "mse_lowest =  0.110736966\n",
      "mse_less=  0.09591629\n",
      "train_data shape =  torch.Size([100, 500, 1])\n",
      "model loaded:  model_L1_D16_noise0.pt\n",
      "don't train =  False\n",
      "train_data before dataset torch.Size([100, 500, 1])\n",
      "step_size =  16\n",
      "x_end_idx =  17\n",
      "y_start_idx =  32\n",
      "y_end_idx =  113\n",
      "range(0, 17, 16)\n",
      "self.train_x shape =  torch.Size([100, 2])\n",
      "train_ys shape =  torch.Size([100, 6, 1])\n",
      "train_x after torch.Size([100, 2])\n",
      "self.n_dim=  2\n",
      "dataset.n_dim =  2\n",
      "epoch 100, training loss 0.06568051129579544, validation loss 0.05465284362435341\n",
      "(--> new model saved @ epoch 100)\n",
      "epoch 200, training loss 0.05258018895983696, validation loss 0.05452724173665047\n",
      "(--> new model saved @ epoch 200)\n"
     ]
    }
   ],
   "source": [
    "import double_Resnet as tnet\n",
    "current_size = 1\n",
    "models, step_sizes, mse_list, idx_lowest,n_forward_list = find_best_timestep(train_dict[str(current_size)], \n",
    "                                                              val_dict[str(current_size)], \n",
    "                                                              val_dict[str(current_size)], current_size,model_dir=model_dir,# make_new=True, \n",
    "                                                             start_k=2, largest_k = 6, max_epochs = 500, print_every=100, dont_train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1500/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plot a bunch\n",
    "# # \n",
    "# step_size = 4\n",
    "# dt =1\n",
    "# n_forward = int(500/step_size - 1)\n",
    "# model_name = 'model_L{}_D{}_noise{}.pt'.format(current_size,step_size, 0)\n",
    "# model_path_this = os.path.join(model_dir, model_name)\n",
    "\n",
    "# n_points, n_timesteps, total_dim = torch.flatten(train_dict[str(current_size)], 2,3).shape\n",
    "# arch = [total_dim, 128, 128, 128, total_dim] \n",
    "\n",
    "\n",
    "# model_time = tnet.ResNet(arch=arch, dt=dt, step_size=step_size)\n",
    "\n",
    "# dataset = tnet.DataSet(torch.flatten(train_dict[str(current_size)], 2,3), torch.flatten(val_dict[str(current_size)], 2,3), \n",
    "#                        torch.flatten(val_dict[str(current_size)], 2,3), dt, step_size, n_forward)\n",
    "\n",
    "# #plot the inputed data\n",
    "# plt.figure()\n",
    "# plt.plot(dataset.val_ys[0, :, 0])#, '.')\n",
    "# #     plt.plot(np.arange(len(dataset.val_ys[point_num, :, i]))*dataset.step_size, dataset.val_ys[point_num, :, i], '.')\n",
    "# #     plt.plot(torch.flatten(val_data, 2,3)[0,s_size:,0])\n",
    "# plt.title('step size = '+str(step_size)+\" start\")\n",
    "# #   plt.xlim([0,100])\n",
    "# plt.show()\n",
    "\n",
    "# # training\n",
    "# for i in range(10):\n",
    "#     model_time.train_net(dataset, batch_size = 32, max_epoch=10,model_path=model_path_this)\n",
    "#     models = list()\n",
    "#     models.append(model_time)\n",
    "#     plt.figure()\n",
    "# #     plt.plot(dataset.val_ys[0, :, 0])\n",
    "#     predicted = tnet.multi_scale_forecast(dataset.val_ys[:,0,:],n_forward, models)\n",
    "#     plt.plot(dataset.val_ys[0,:,0], '.')\n",
    "#     plt.plot(predicted[0,:,0])\n",
    "#     plt.title('i = '+str(i))\n",
    "#     #   plt.xlim([0,100])\n",
    "#     plt.show()\n",
    "\n",
    "# # return model_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[4, 8, 16, 32] [0.11079766, 0.09525904, 0.10413397, 0.1560007]\n",
      "x_prev shape =  torch.Size([10, 2])\n",
      "y_pred shape =  torch.Size([10, 1, 17])\n",
      "steps =  [0, 31, 63, 95, 127, 159, 191, 223, 255, 287, 319, 351, 383, 415, 447, 479, 511]\n",
      "len steps =  17\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#====================================================================================\n",
    "def plot_lowest_error(data, model, i = 0):\n",
    "    \"\"\"\n",
    "    Plot data at model, idx\n",
    "    \n",
    "    inputs:\n",
    "        data: tensor of shape (n_points, n_timesteps, dim, dim)\n",
    "        model: Resnet model to predict on \n",
    "        i: int, which validation point to graph\n",
    "    outputs:\n",
    "        No returned values, but graph shown\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    data  = torch.flatten(data, 2,3)\n",
    "    _, total_steps, _ = data.shape\n",
    "    y_preds = model_time.uni_scale_forecast(data[:, :2, 0].float(), n_steps=total_steps-1)\n",
    "\n",
    "#     y_preds = model.uni_scale_forecast(torch.tensor(data[:,0:2,:]).float(), n_steps=total_steps-1)\n",
    "    plt.plot(y_preds[i,:,0], label = \"Predicted\")\n",
    "    plt.plot(data[i,1:,0], label = \"Truth\")\n",
    "    plt.ylim([-.1, 1.1])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#====================================================================================\n",
    "\n",
    "print(step_sizes[idx_lowest])    \n",
    "print(step_sizes, mse_list)\n",
    "plot_lowest_error(val_dict[str(current_size)], models[idx_lowest], i =2)\n",
    "\n",
    "# print(train_data.shape)\n",
    "# dataset = tnet.DataSet(torch.flatten(train_data, 2,3), torch.flatten(val_data, 2,3), \n",
    "#                        torch.flatten(val_data, 2,3), 1, step_sizes[idx_lowest], 5)\n",
    "# dataset.plot_val_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_val_data(dataset, point_num = 0, i = 0, other_plot = None):\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(len(dataset.val_ys[point_num, :, i]))*dataset.step_size, dataset.val_ys[point_num, :, i])#, '.')\n",
    "        if other_plot is not None:\n",
    "            plt.plot(other_plot)\n",
    "#             plt.xlim([0,100])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset.train_x.shape)\n",
    "# print(128**2)\n",
    "train_data = train_dict['1']\n",
    "val_data = val_dict['1']\n",
    "plt.plot(val_data[0,:,0,0])\n",
    "print(train_data.shape)\n",
    "s_size = 8\n",
    "dataset = tnet.DataSet(torch.flatten(train_data, 2,3), torch.flatten(val_data, 2,3), \n",
    "                       torch.flatten(val_data, 2,3), 1, s_size, int(np.floor(499/s_size)))\n",
    "# plot_val_data(dataset, other_plot=torch.flatten(val_data, 2,3)[0,s_size:,0])\n",
    "point_num = 0\n",
    "i = 0\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(dataset.val_ys[point_num, :, i]))*dataset.step_size, dataset.val_ys[point_num, :, i])#, '.')\n",
    "plt.plot(np.arange(len(dataset.val_ys[point_num, :, i]))*dataset.step_size, dataset.val_ys[point_num, :, i], '.')\n",
    "plt.plot(torch.flatten(val_data, 2,3)[0,s_size:,0])\n",
    "#   plt.xlim([0,100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 32\n",
    "n_forward = int(500/step_size - 1)\n",
    "current_size = 1\n",
    "train_data = train_dict[str(current_size)]\n",
    "val_data = val_dict[str(current_size)]\n",
    "model_time = train_one_timestep(step_size, torch.flatten(train_data, 2,3), \n",
    "                                         torch.flatten(val_data, 2,3),  torch.flatten(val_data, 2,3), \n",
    "                                         1, n_forward=n_forward,  max_epochs = 1000,)\n",
    "dataset = tnet.DataSet(torch.flatten(train_data, 2,3), torch.flatten(val_data, 2,3), \n",
    "                       torch.flatten(val_data, 2,3), 1, step_size, n_forward)\n",
    "# print(model_time(dataset.val_ys[:,0,:],n_forward).shape)\n",
    "models = list()\n",
    "models.append(model_time)\n",
    "predicted = tnet.multi_scale_forecast(dataset.val_ys[:,0,:],n_forward, models)\n",
    "plt.plot(dataset.val_ys[0,:,0], '.')\n",
    "plt.plot(predicted[0,:,0])\n",
    "# plt.xlim([0,500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================\n",
    "def find_error_4(data, model, truth_data, tol = 1e-5):\n",
    "    \"\"\"\n",
    "    Find error over the 4 squares \n",
    "    \n",
    "    inputs:\n",
    "        data: tensor of size (n_points, n_timesteps, dim, dim) to be predicted or size (n_points, n_timesteps)\n",
    "        model: Resnet object to predict data on\n",
    "        truth_data: tensor of size (n_points, n_timesteps, dim_larger, dim_larger) compared on \n",
    "        tol = 1e-5: tolerance level to mark points as resolved or not\n",
    "        criterion = torch.nn.MSELoss(reduction='none')\n",
    "        \n",
    "    outputs:\n",
    "        resolved: boolean whether complete area is resolved or not\n",
    "        loss: array of floats for size (dim, dim) with mse of each square\n",
    "        unresolved: array of booleans, whether that part is resolved or not. (1 unresolved, 0 resolved)\n",
    "    \"\"\"\n",
    "    if(len(data.shape))==2:\n",
    "        data = data.unsqueeze(2).unsqueeze(3)\n",
    "    assert len(data.shape) == 4\n",
    "    n_points, n_timesteps, dim, _ = data.shape\n",
    "    data  = torch.flatten(data, 2,3)\n",
    "    y_preds = model.uni_scale_forecast(torch.tensor(data[:,0,:]).float(), n_steps=n_timesteps-1).reshape(( n_points, n_timesteps-1, dim,dim))\n",
    "    \n",
    "    _,_, truth_dim, _ = truth_data.shape\n",
    "    assert truth_dim >= dim\n",
    "    \n",
    "    loss = mse(y_preds, truth_data[:,1:])\n",
    "    \n",
    "    resolved =  loss.max() <= tol\n",
    "    unresolved_array = torch.tensor(loss <= tol)\n",
    "    \n",
    "    return resolved, loss, 1-unresolved_array.float()\n",
    "\n",
    "\n",
    "\n",
    "#====================================================================================    \n",
    "    \n",
    "def mse(data1, data2):\n",
    "    \"\"\"\n",
    "    Finds Mean Squared Error between data1 and data2\n",
    "    \n",
    "    inputs:\n",
    "        data1: tensor of shape (n_points, n_timestep, dim1, dim1)\n",
    "        data2: tensor of shape (n_points, n_timestep, dim2, dim2)\n",
    "        \n",
    "    output:\n",
    "        mse: array of size (min_dim, min_dim) with mse \n",
    "    \n",
    "    \"\"\"\n",
    "    #find bigger dim\n",
    "    size1 = data1.shape[-1]\n",
    "    size2 = data2.shape[-1]\n",
    "    size_max = max(size1, size2)\n",
    "    \n",
    "    #grow to save sizes and find mse\n",
    "    mse = np.mean((grow(data1, size_max) - grow(data2, size_max))**2, axis = (0, 1))\n",
    "    return mse\n",
    "#====================================================================================\n",
    "    \n",
    "def grow(data, dim_full=128):\n",
    "    '''\n",
    "    Grow tensor from any size to a bigger size\n",
    "    inputs: \n",
    "        data: tensor to grow, size (n_points, n_timesteps, dim_small, dim_small)\n",
    "        dim_full = 128: int of size to grow data to\n",
    "\n",
    "    outputs:\n",
    "        data_full: tensor size (n_points, n_timesteps, size_full, size_full)\n",
    "    '''\n",
    "    n_points, n_timesteps, dim_small, _ = data.shape \n",
    "    assert dim_full % dim_small == 0 #need small to be multiple of full\n",
    "\n",
    "    divide = dim_full // dim_small\n",
    "\n",
    "    data_full = np.zeros((n_points, n_timesteps, dim_full,dim_full))\n",
    "    for i in range(dim_small):\n",
    "        for j in range(dim_small):\n",
    "            repeated = np.repeat(np.repeat(data[:,:,i,j].reshape(n_points,n_timesteps,1,1), divide, axis = 2), divide, axis = 3)\n",
    "            data_full[:,:,i*divide:(i+1)*divide, j*divide:(j+1)*divide] = repeated\n",
    "    return data_full\n",
    "#====================================================================================\n",
    "\n",
    "\n",
    "resolved, loss, unresolved_list = find_error_4(val_dict['1'], models[idx_lowest], val_dict['2'])\n",
    "print(loss.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unresolved_dict[str(current_size)] = torch.tensor(unresolved_list)\n",
    "\n",
    "print(unresolved_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_train_data = unresolved_list * train_dict[str(current_size*2)]\n",
    "print(next_train_data.shape)\n",
    "plt.imshow(next_train_data[0,0])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keep.append(models[idx_lowest])\n",
    "model_used_dict[str(current_size)] = [[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================\n",
    "def find_error_1(data, model, tol = 1e-5):\n",
    "    \"\"\"\n",
    "    Find error over the 1 square\n",
    "    \n",
    "    inputs:\n",
    "        data: tensor of size (n_points, n_timesteps, dim, dim) to be predicted\n",
    "        model: Resnet object to predict data on\n",
    "        tol = 1e-5: tolerance level to mark points as resolved or not\n",
    "        criterion = torch.nn.MSELoss(reduction='none')\n",
    "        \n",
    "    outputs:\n",
    "        loss: float of mse\n",
    "        resolved: boolean whether resolved or not\n",
    "    \"\"\"\n",
    "    n_points, n_timesteps  = data.shape\n",
    "    dim = 1\n",
    "    data_input  = data.unsqueeze(2)\n",
    "    y_preds = model.uni_scale_forecast(torch.tensor(data_input[:,0,:]).float(), n_steps=n_timesteps-1).reshape(( n_points, n_timesteps-1, dim,dim))\n",
    "    data1 = data[:,1:]\n",
    "    data2 = y_preds[:,:,0,0]\n",
    "#     print()\n",
    "    loss = torch.mean((data1-data2)**2)#mse(y_preds, data[:,1:])\n",
    "    \n",
    "#     print(loss)\n",
    "    \n",
    "    return loss, loss <= tol\n",
    "\n",
    "#====================================================================================\n",
    "\n",
    "\n",
    "current_size = 2\n",
    "next_train_data = unresolved_list * train_dict[str(current_size)]\n",
    "\n",
    "model_idx_list = np.ones((current_size, current_size))*(-1) #start with all -1\n",
    "\n",
    "for i in range(current_size):\n",
    "    for j in range(current_size):\n",
    "        data_this = next_train_data[:,:,i,j]\n",
    "        if (torch.min(data_this) == 0) and (torch.max(data_this) == 0):\n",
    "            #don't need to do anything is model is resolved\n",
    "            continue\n",
    "        else:\n",
    "        #see if the error is low enough on already made model\n",
    "            for m, model in enumerate(model_keep):\n",
    "                loss, resolved = find_error_1(data_this, model)\n",
    "                step_size = model.step_size\n",
    "                print(\"loss = \", loss)\n",
    "                print(\"step_size = \", step_size)\n",
    "                if resolved:\n",
    "                    model_idx_list[i,j] == m\n",
    "                    break\n",
    "                else:\n",
    "                    pass\n",
    "            if not resolved:\n",
    "                i = 0\n",
    "                j = 1\n",
    "                k = int(np.log2(step_size))\n",
    "                print(\"k = \", k)\n",
    "                print(\"train_dict[str(current_size)][:,:,i,j] shape = \", train_dict[str(current_size)][:,:,i,j].shape)\n",
    "                #if no model good, train new model\n",
    "                models, step_sizes, mse_list, idx_lowest = find_best_timestep(train_dict[str(current_size)][:,:,i,j], \n",
    "                                                              val_dict[str(current_size)][:,:,i,j], \n",
    "                                                              val_dict[str(current_size)][:,:,i,j], current_size,model_dir=model_dir,\n",
    "                                                              i=i, j=j, start_k = max(0,k-1), largest_k = k+2)\n",
    "                \n",
    "                vbnm\n",
    "                resolved, loss, unresolved_list = find_error_4(val_dict[str(current_size)][:,:,i,j], \n",
    "                                                               models[idx_lowest], \n",
    "                                                               val_dict[str(current_size)][:,:, i*current_size:(i+1)*current_size, j*current_size:(j+1)*current_size])\n",
    "                model_keep.append(models[idx_lowest])\n",
    "                model_idx_list[i,j] == len(model_keep) #last model will be the one for this square\n",
    "            \n",
    "#             predicted = model.uni_scale_forecast(torch.tensor(data[:,0,:]).float(), n_steps=n_timesteps-1).reshape((  n_points, n_timesteps-1, dim,dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(step_sizes, mse_list, idx_lowest)\n",
    "resolved, loss, unresolved_list = find_error_4(val_dict[str(current_size)][:,:,i,j], \n",
    "                                                               models[idx_lowest], \n",
    "                                                               val_dict[str(current_size*2)][:,:, i*current_size:(i+1)*current_size, j*current_size:(j+1)*current_size])\n",
    "print(loss)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  models[idx_lowest]\n",
    "print(idx_lowest)\n",
    "n_timesteps = 500\n",
    "n_points = 10\n",
    "dim = 1\n",
    "# plt.plot(model(val_dict[str(current_size)][:,:,i,j].unsqueeze(2).unsqueeze(3))[0,:,0,0].detach().numpy(), label = \"predicted\")\n",
    "print(val_dict[str(current_size)][:,0,i,j].unsqueeze(1).shape)\n",
    "val_data_this = val_dict[str(current_size)][:,0,i,j].unsqueeze(1)\n",
    "predicted = model.uni_scale_forecast(val_data_this, n_steps=n_timesteps-1)\n",
    "print(predicted.shape)\n",
    "predicted_reshape = predicted.reshape((  n_points, n_timesteps-1, dim,dim))\n",
    "plt.plot(predicted_reshape[0,:,0], label = \"predicted\")\n",
    "print(val_dict[str(current_size*2)][:,:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size].shape)\n",
    "# plt.plot(val_dict[str(current_size*2)][:,1:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size][0,:,0,0], label = \"Truth\")\n",
    "\n",
    "plt.plot(val_dict[str(current_size*2)][:,1:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size][0,:,0,1], label = \"Truth\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(val_dict[str(current_size*2)][0,0])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = (16+32)/2\n",
    "print(step_size)\n",
    "model = train_one_timestep(int(28), train_dict[str(current_size)][:,:,i,j].unsqueeze(2), \n",
    "                           val_dict[str(current_size)][:,:,i,j].unsqueeze(2), \n",
    "                           val_dict[str(current_size)][:,:,i,j].unsqueeze(2), current_size)\n",
    "#                        dt = 1, n_forward = 5, noise=0, make_new = False, dont_train = True, \n",
    "#                        lr = 1e-3, max_epochs = 10000, batch_size = 50,threshold = 1e-4, \n",
    "#                        model_dir = './models/toy2',i=None, j = None):\n",
    "    \n",
    "#     train_dict[str(current_size)][:,:,i,j], \n",
    "#                                                               val_dict[str(current_size)][:,:,i,j], \n",
    "#                                                               val_dict[str(current_size)][:,:,i,j], current_size,model_dir=model_dir, \n",
    "#                                                               i=i, j=j, start_k = max(0,k-1), largest_k = k+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 1\n",
    "resolved, loss, unresolved_list = find_error_4(val_dict[str(current_size)][:,:,i,j], \n",
    "                                                               model, \n",
    "                                                               val_dict[str(current_size*2)][:,:, i*current_size:(i+1)*current_size, j*current_size:(j+1)*current_size])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model =  models[idx_lowest]\n",
    "print(idx_lowest)\n",
    "n_timesteps = 500\n",
    "n_points = 10\n",
    "dim = 1\n",
    "# plt.plot(model(val_dict[str(current_size)][:,:,i,j].unsqueeze(2).unsqueeze(3))[0,:,0,0].detach().numpy(), label = \"predicted\")\n",
    "print(val_dict[str(current_size)][:,0,i,j].unsqueeze(1).shape)\n",
    "val_data_this = val_dict[str(current_size)][:,0,i,j].unsqueeze(1)\n",
    "predicted = model.uni_scale_forecast(val_data_this, n_steps=n_timesteps-1)\n",
    "print(predicted.shape)\n",
    "predicted_reshape = predicted.reshape((  n_points, n_timesteps-1, dim,dim))\n",
    "plt.plot(predicted_reshape[0,:,0], label = \"predicted\")\n",
    "print(val_dict[str(current_size*2)][:,:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size].shape)\n",
    "# plt.plot(val_dict[str(current_size*2)][:,1:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size][0,:,0,0], label = \"Truth\")\n",
    "\n",
    "plt.plot(val_dict[str(current_size*2)][:,1:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size][0,:,0,1], label = \"Truth\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
