{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multiscale model fitting for Toy2a\n",
    "\n",
    "Toy2a is a simplified version of toy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start with initalizing many things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "# import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from tqdm.notebook import tqdm\n",
    "# import time\n",
    "import math\n",
    "\n",
    "# module_path = os.path.abspath(os.path.join('../'))\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)\n",
    "    \n",
    "    \n",
    "# import torch_cae_multilevel_V4 as net\n",
    "import double_Resnet as tnet\n",
    "# import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "data_dir = '../data/toy2a'\n",
    "model_dir = '../models/toy2a'\n",
    "result_dir = '../result/toy2a'\n",
    "\n",
    "#load data\n",
    "train_data = torch.tensor(np.load(os.path.join(data_dir, 'train_data.npy')))\n",
    "val_data = torch.tensor(np.load(os.path.join(data_dir, 'val_data.npy')))\n",
    "test_data = torch.tensor(np.load(os.path.join(data_dir, 'test_data.npy')))\n",
    "\n",
    "data_of_sizes = {}\n",
    "current_size = 2\n",
    "unresolved_dict = {}\n",
    "model_keep = list()\n",
    "model_used_dict = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_size =  32\n",
      "x_end_idx =  33\n",
      "y_start_idx =  64\n",
      "y_end_idx =  225\n",
      "range(0, 33, 32)\n",
      "self.train_x shape =  torch.Size([100, 128])\n",
      "train_ys shape =  torch.Size([100, 6, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16kat\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\double\\utils_time2.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.train_x = torch.tensor(train_data[:, x_idx:x_end_idx:step_size, :]).float().to(self.device)\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\double\\utils_time2.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.train_ys = torch.tensor(train_data[:, y_start_idx:y_end_idx:step_size, :]).float().to(self.device)\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\double\\utils_time2.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.val_x = torch.tensor(val_data[:, x_idx:x_end_idx:step_size, :]).float().to(self.device)\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\double\\utils_time2.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.val_ys = torch.tensor(val_data[:, y_start_idx:y_end_idx:step_size, :]).float().to(self.device)\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\double\\utils_time2.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.test_x = torch.tensor(test_data[:, 0, :]).float().to(self.device)\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\double\\utils_time2.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.test_ys = torch.tensor(test_data[:, 1:, :]).float().to(self.device)\n"
     ]
    }
   ],
   "source": [
    "#testing dataset new structure\n",
    "dt = 1\n",
    "step_size = 32\n",
    "n_forward = 5\n",
    "dataset = tnet.DataSet(torch.flatten(train_data,2,3), torch.flatten(val_data,2,3), torch.flatten(test_data,2,3), dt, step_size, n_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 6, 64])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.train_ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions, will move these to a utils file eventually \n",
    "#====================================================================================\n",
    "# def data_of_size(data,size):\n",
    "#     \"\"\"\n",
    "#     Takes averages to shrink size of data\n",
    "#     Takes data of size (n_points, dim, dim) and shrinks to size (n_points, size, size)\n",
    "#     takes averages to shrink\n",
    "#     \"\"\"\n",
    "#     return decrease_to_size(torch.tensor(data).unsqueeze(1), size)[:,0,:,:]\n",
    "#====================================================================================\n",
    "\n",
    "\n",
    "def isPowerOfTwo(n):\n",
    "    \"\"\"\n",
    "    checks if n is a power of two\n",
    "    \n",
    "    input: n, int\n",
    "    \n",
    "    output: boolean\n",
    "    \"\"\"\n",
    "    return (np.ceil(np.log2(n)) == np.floor(np.log2(n)));\n",
    "#====================================================================================\n",
    "def shrink(data, low_dim):\n",
    "    '''\n",
    "    Shrinks data to certain size; either averages or takes endpoints\n",
    "    \n",
    "    inputs:\n",
    "        data: array of size (n_points, n_timesteps, dim, dim) that will shrink\n",
    "        low_dim: int, size to shrink to, low_dim must be less than or equal to dim\n",
    "        \n",
    "    output:\n",
    "        data: array of size (n_points, n_timesteps, low_dim, low_dim)\n",
    "    '''\n",
    "    \n",
    "    #check inputs\n",
    "    assert len(data.shape) == 4\n",
    "    n_points, n_timesteps, dim, _ = data.shape\n",
    "    assert dim >= low_dim\n",
    "    assert isPowerOfTwo(low_dim)\n",
    "    \n",
    "    if dim == low_dim: #same size, no change\n",
    "        return data\n",
    "    \n",
    "    while(dim > low_dim):\n",
    "        #shrink by 1 level until same size\n",
    "        data = apply_local_op(data.float(), 'cpu', ave=average)\n",
    "        current_size = data.shape[-1]\n",
    "        \n",
    "    return data\n",
    "#====================================================================================\n",
    "def ave_one_level(data):\n",
    "    '''\n",
    "    takes averages to shrink data 1 level\n",
    "    \n",
    "    inputs:\n",
    "        data: tensor of size (n_points, n_timesteps, dim, dim) that will shrink\n",
    "        \n",
    "    output:\n",
    "        processed data: tensor of size (n_points, n_timesteps, dim/2, dim/2)\n",
    "    '''\n",
    "    device = 'cpu'\n",
    "    if not torch.is_tensor(data): #needs to be a tensor\n",
    "        data = torch.tensor(data)\n",
    "        \n",
    "    print(\"data shape = \", data.shape)\n",
    "    assert len(data.shape) == 4\n",
    "#     if data.shape != 4:\n",
    "#         print(\"data.shape = \", data.shape)\n",
    "#         print(\"data.shape should be of length 4\")\n",
    "    n_points, n_timesteps, dim, _ = data.shape\n",
    "    \n",
    "    #dim needs to be even \n",
    "    assert dim % 2 == 0\n",
    "    \n",
    "    data_right_size = torch.flatten(data, 0,1).unsqueeze(1).float()\n",
    "    \n",
    "#     n = min(in_channels, out_channels)\n",
    "    op = torch.nn.Conv2d(1, 1, 2, stride=2, padding=0).to(device)\n",
    "   \n",
    "    op.weight.data = torch.zeros(op.weight.data.size()).to(device)\n",
    "    op.bias.data = torch.zeros(op.bias.data.size()).to(device)\n",
    "    op.weight.data[0,0, :, :] = torch.ones(op.weight.data[0,0, :, :].size()).to(device) / 4\n",
    "\n",
    "    # make them non-trainable\n",
    "    for param in op.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    print(\"Transforming\")\n",
    "        \n",
    "    shrunk = op(data_right_size)\n",
    "    \n",
    "    print(\"reshape to print\")\n",
    "    \n",
    "    return shrunk.squeeze(1).reshape((n_points, n_timesteps, dim//2, dim//2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 500, 8, 8])\n",
      "data shape =  torch.Size([100, 500, 8, 8])\n",
      "Transforming\n",
      "reshape to print\n",
      "torch.Size([100, 500, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "processed = ave_one_level(train_data)\n",
    "print(processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#make a dictionary with train data of every size 128->1\n",
    "#====================================================================================\n",
    "\n",
    "def make_dict_all_sizes(data):\n",
    "    \"\"\"\n",
    "    Makes a dictionary of data at every refinedment size from current->1\n",
    "    \n",
    "    inputs:\n",
    "        data: tensor(or array) of size (n_points, n_timesteps, dim, dim)\n",
    "        \n",
    "    outputs: \n",
    "        dic: dictionary of tensors. Keys are dim size, tensors are size (n_points, n_timesteps, dim, dim)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n_points, n_timesteps, dim, _ = data.shape\n",
    "    \n",
    "    if not torch.is_tensor(data): #needs to be a tensor\n",
    "        data = torch.tensor(data)\n",
    "        \n",
    "    assert isPowerOfTwo(dim)\n",
    "        \n",
    "    dic = {str(dim): data}\n",
    "    \n",
    "    for i in range(int(np.log2(dim))):\n",
    "        #decrease\n",
    "        print(\"i = \", i)\n",
    "        data = ave_one_level(data)\n",
    "        dic[str(data.shape[-1])] = data\n",
    "    \n",
    "    print(dic.keys())\n",
    "    \n",
    "    return dic\n",
    "#====================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  0\n",
      "data shape =  torch.Size([100, 500, 8, 8])\n",
      "Transforming\n",
      "reshape to print\n",
      "i =  1\n",
      "data shape =  torch.Size([100, 500, 4, 4])\n",
      "Transforming\n",
      "reshape to print\n",
      "i =  2\n",
      "data shape =  torch.Size([100, 500, 2, 2])\n",
      "Transforming\n",
      "reshape to print\n",
      "dict_keys(['8', '4', '2', '1'])\n",
      "i =  0\n",
      "data shape =  torch.Size([10, 500, 8, 8])\n",
      "Transforming\n",
      "reshape to print\n",
      "i =  1\n",
      "data shape =  torch.Size([10, 500, 4, 4])\n",
      "Transforming\n",
      "reshape to print\n",
      "i =  2\n",
      "data shape =  torch.Size([10, 500, 2, 2])\n",
      "Transforming\n",
      "reshape to print\n",
      "dict_keys(['8', '4', '2', '1'])\n"
     ]
    }
   ],
   "source": [
    "train_dict = make_dict_all_sizes(train_data)\n",
    "val_dict = make_dict_all_sizes(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================\n",
    "def train_one_timestep(step_size, train_data, val_data, test_data, current_size, \n",
    "                       dt = 1, n_forward = 5, noise=0, make_new = False, dont_train = True, \n",
    "                       lr = 1e-3, max_epochs = 10000, batch_size = 50,threshold = 1e-4, \n",
    "                       model_dir = './models/toy2',i=None, j = None,print_every=1000):\n",
    "\n",
    "    \"\"\"\n",
    "    fits or loads model at 1 timestep\n",
    "    \n",
    "    inputs:\n",
    "        step_size: int \n",
    "        train_data: tensor size (n_points, n_timesteps, dim**2) \n",
    "        val_data:tensor size (n_val_points, n_timesteps, dim**2) \n",
    "        test_data:tensor size (n_test_points, n_timesteps, dim**2) \n",
    "        current_size: int, only used in file naming\n",
    "        dt = 1: float\n",
    "        n_forward = 5: int, number of steps to consider during training\n",
    "        noise=0: float, level of noise, (right now just used in file naming)\n",
    "        make_new = False: boolean, whether or not to make a new model if old already exists\n",
    "        dont_train = True: boolean, whether or not to train more if model loaded\n",
    "        lr = 1e-3: float, learning rate\n",
    "        max_epochs = 10000: int \n",
    "        batch_size = 50: int\n",
    "        threshold=1e-4: float, stop training when validation gets below threshold\n",
    "         \n",
    "    \n",
    "    outputs:\n",
    "        model_time: ResNet object of trained model. Also saved\n",
    "    \"\"\"\n",
    "    if (i is not None) and (j is not None):\n",
    "        \n",
    "        model_name = 'model_L{}_D{}_noise{}_i{}_j{}.pt'.format(current_size,step_size, noise, i, j)\n",
    "    else:\n",
    "        model_name = 'model_L{}_D{}_noise{}.pt'.format(current_size,step_size, noise)\n",
    "    model_path_this = os.path.join(model_dir, model_name)\n",
    "    \n",
    "    n_points, n_timesteps, total_dim = train_data.shape\n",
    "    arch = [total_dim*2, 128, 128, 128, total_dim] \n",
    "    \n",
    "    try: #if we already have a model saved\n",
    "        if make_new:\n",
    "            print(\"Making a new model. Old one deleted. model {}\".format(model_name))\n",
    "            assert False\n",
    "        model_time = torch.load(model_path_this)\n",
    "        print(\"model loaded: \", model_name)\n",
    "        print(\"don't train = \", dont_train)\n",
    "        if dont_train: #just load model, no training\n",
    "            return model_time\n",
    "    except:\n",
    "        print('create model {} ...'.format(model_name))\n",
    "        model_time = tnet.ResNet(arch=arch, dt=dt, step_size=step_size)\n",
    "\n",
    "    print(\"train_data before dataset\", train_data.shape)\n",
    "#     dataset = tnet.DataSet(train_data, val_data, test_data, dt, step_size, n_forward)\n",
    "    dataset =  utils.make_dataset_max_repeat(train_data, val_data, test_data, dt, \n",
    "                                         step_size=step_size, n_forward=n_forward,n_input_points=2)\n",
    "    print(\"train_x after\", dataset.train_x.shape)\n",
    "    \n",
    "#     #plot the inputed data\n",
    "#     plt.figure()\n",
    "#     plt.plot(dataset.val_ys[0, :, 0])#, '.')\n",
    "# #     plt.plot(np.arange(len(dataset.val_ys[point_num, :, i]))*dataset.step_size, dataset.val_ys[point_num, :, i], '.')\n",
    "# #     plt.plot(torch.flatten(val_data, 2,3)[0,s_size:,0])\n",
    "#     plt.title('step size = '+str(step_size))\n",
    "#     #   plt.xlim([0,100])\n",
    "#     plt.show()\n",
    "    \n",
    "    # training\n",
    "    model_time.train_net(dataset, max_epoch=max_epochs, batch_size=batch_size, lr=lr,\n",
    "                    model_path=model_path_this,threshold= threshold,print_every=print_every)\n",
    "    \n",
    "    return model_time\n",
    "#====================================================================================\n",
    "\n",
    "def find_best_timestep(train_data, val_data, test_data, current_size, start_k = 0, largest_k = 7, \n",
    "                       dt = 1, n_forward = 5, noise=0, make_new = False, dont_train = True,\n",
    "                       lr = 1e-3, max_epochs = 10000, batch_size = 50,threshold = 1e-4, \n",
    "                       criterion = torch.nn.MSELoss(reduction='none'), model_dir = \"./models/toy2\",\n",
    "                       i=None, j = None,print_every= 1000):\n",
    "    \"\"\"\n",
    "    Trains models with different timestep sizes and finds lowest error\n",
    "    \n",
    "    inputs:\n",
    "     n_forward = 5, noise=0, make_new = False, dont_train = False):\n",
    "    \n",
    "        train_data: tensor size (n_points, n_timesteps, dim, dim), or  size (n_points, n_timesteps)\n",
    "        val_data:tensor size (n_val_points, n_timesteps, dim, dim) , or  size (n_val_points, n_timesteps)\n",
    "        test_data:tensor size (n_test_points, n_timesteps, dim, dim) , or  size (n_test_points, n_timesteps)\n",
    "        current_size: int, only used in file naming\n",
    "        start_k = 0: int, smallest timestep will be 2**start_k\n",
    "        largest_k = 7:int, largest timestep will be 2**largest_k\n",
    "        dt = 1: float\n",
    "        n_forward = 5: int, number of steps to consider during training\n",
    "        noise=0: float, level of noise, (right now just used in file naming)\n",
    "        make_new = False: boolean, whether or not to make a new model if old already exists\n",
    "        dont_train = False: boolean, whether or not to train more if model loaded\n",
    "        lr = 1e-3: float, learning rate\n",
    "        max_epochs = 10000: int \n",
    "        batch_size = 50: int\n",
    "        threshold=1e-4: float\n",
    "        criterion = torch.nn.MSELoss(reduction='none'))\n",
    "         \n",
    "         \n",
    "    outputs:\n",
    "        models: list of ResNet models\n",
    "        step_sizes: list of ints for the steps_sizes of models \n",
    "        mse_list: list of floats, mse of models \n",
    "        idx_lowest: int, index value with lowest mse\n",
    "         \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #transform data shapes if needed\n",
    "    if(len(train_data.shape)== 2):\n",
    "        train_data = train_data.unsqueeze(2).unsqueeze(3)\n",
    "        val_data = val_data.unsqueeze(2).unsqueeze(3)\n",
    "        test_data = test_data.unsqueeze(2).unsqueeze(3)\n",
    "    assert(len(train_data.shape)== 4)\n",
    "    assert(len(val_data.shape)== 4)\n",
    "    assert(len(test_data.shape)== 4)\n",
    "    \n",
    "    models = list()\n",
    "    step_sizes = list()\n",
    "    n_forward_list = list()\n",
    "    mse_lowest = 1e10 #big number\n",
    "    mse_list = list()\n",
    "    mse_less = 0\n",
    "    idx_lowest = -1\n",
    "    \n",
    "    #make data flat to right dim (n_points, n_timesteps, dim**2)\n",
    "    train_data = torch.flatten(train_data, 2,3)\n",
    "    val_data = torch.flatten(val_data, 2,3)\n",
    "    test_data = torch.flatten(test_data, 2,3)\n",
    "    \n",
    "    n_points, n_timesteps, total_dim = train_data.shape\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, k in enumerate(range(start_k, largest_k)):\n",
    "        step_size = 2**k\n",
    "        step_sizes.append(step_size)\n",
    "        \n",
    "        #going to make n_forward the max if can be \n",
    "#         n_forward = int((np.floor(n_timesteps/step_size)-1)/2)\n",
    "#         n_forward_list.append(n_forward)\n",
    "#         print(\"n_forward = \", n_forward)\n",
    "        \n",
    "        print(\"train_data shape = \", train_data.shape)\n",
    "        model_time = train_one_timestep(step_size, train_data, val_data, test_data, current_size, \n",
    "                                        make_new = make_new, dont_train = dont_train,i=i, j=j, \n",
    "                                        n_forward=n_forward, max_epochs=max_epochs,model_dir=model_dir, \n",
    "                                        print_every = print_every)\n",
    "        \n",
    "        \n",
    "        models.append(model_time)\n",
    "    \n",
    "        #find error\n",
    "        \n",
    "        y_preds = model_time.uni_scale_forecast(val_data[:, :2, 0].float(), n_steps=n_timesteps-1)\n",
    "        mse_all = criterion(val_data[:, 1:, :].float(), y_preds).mean(-1)\n",
    "\n",
    "        mean = mse_all.mean(0).detach().numpy()\n",
    "#         print(mean.shape)\n",
    "        mse_less = mean.mean()\n",
    "        mse_list.append(mse_less)\n",
    "\n",
    "        print(\"mse_lowest = \", mse_lowest)\n",
    "        print(\"mse_less= \", mse_less)\n",
    "        \n",
    "        if (mse_less< mse_lowest) or (math.isnan(mse_lowest)) or (math.isnan(mse_less)):\n",
    "            mse_lowest = mse_less\n",
    "            idx_lowest = idx\n",
    "\n",
    "    return models, step_sizes, mse_list, idx_lowest, n_forward_list\n",
    "#====================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_size = 1\n",
    "# train_data1 = torch.flatten(train_dict[str(current_size)], 2,3)\n",
    "# val_data1 = torch.flatten(val_dict[str(current_size)], 2,3)\n",
    "# # test_data1 = torch.flatten(val_dict[str(current_size)], 2,3)\n",
    "# model_time = tnet.ResNet(arch=[2,3,3,4,1], dt=dt, step_size=step_size)\n",
    "# print(model_time.n_dim )\n",
    "\n",
    "# # print(\"train_data before dataset\", train_data.shape)\n",
    "# dataset = tnet.DataSet(train_data1, val_data1, val_data1, dt, step_size, n_forward)\n",
    "# print(\"dataset ndim = \", dataset.n_dim)\n",
    "# print(dataset.train_x.shape)\n",
    "\n",
    "# # calculate_loss(model_time,dataset.train_x, dataset.train_ys)\n",
    "\n",
    "# model_time.train_net(dataset, 1, 100, w=1.0, lr=1e-3, model_path=None, threshold = 1e-8, print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape =  torch.Size([100, 500, 1])\n",
      "step_size =  4\n",
      "model loaded:  model_L1_D4_noise0.pt\n",
      "don't train =  False\n",
      "train_data before dataset torch.Size([100, 500, 1])\n",
      "train_data_repeats shape =  torch.Size([1400, 35, 1])\n",
      "step_size =  4\n",
      "x_end_idx =  5\n",
      "y_start_idx =  8\n",
      "y_end_idx =  29\n",
      "range(0, 5, 4)\n",
      "self.train_x shape =  torch.Size([1400, 2])\n",
      "train_ys shape =  torch.Size([1400, 6, 1])\n",
      "step_size datset =  4\n",
      "model step_size =  4\n",
      "train_x after torch.Size([1400, 2])\n",
      "self.n_dim=  2\n",
      "dataset.n_dim =  2\n",
      "--> new model saved @ epoch 1\n",
      "x_prev shape =  torch.Size([10, 2])\n",
      "y_pred shape =  torch.Size([10, 1, 126])\n",
      "steps =  [0, 3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55, 59, 63, 67, 71, 75, 79, 83, 87, 91, 95, 99, 103, 107, 111, 115, 119, 123, 127, 131, 135, 139, 143, 147, 151, 155, 159, 163, 167, 171, 175, 179, 183, 187, 191, 195, 199, 203, 207, 211, 215, 219, 223, 227, 231, 235, 239, 243, 247, 251, 255, 259, 263, 267, 271, 275, 279, 283, 287, 291, 295, 299, 303, 307, 311, 315, 319, 323, 327, 331, 335, 339, 343, 347, 351, 355, 359, 363, 367, 371, 375, 379, 383, 387, 391, 395, 399, 403, 407, 411, 415, 419, 423, 427, 431, 435, 439, 443, 447, 451, 455, 459, 463, 467, 471, 475, 479, 483, 487, 491, 495, 499]\n",
      "len steps =  126\n",
      "mse_lowest =  10000000000.0\n",
      "mse_less=  0.10950452\n",
      "train_data shape =  torch.Size([100, 500, 1])\n",
      "step_size =  8\n",
      "model loaded:  model_L1_D8_noise0.pt\n",
      "don't train =  False\n",
      "train_data before dataset torch.Size([100, 500, 1])\n",
      "train_data_repeats shape =  torch.Size([700, 63, 1])\n",
      "step_size =  8\n",
      "x_end_idx =  9\n",
      "y_start_idx =  16\n",
      "y_end_idx =  57\n",
      "range(0, 9, 8)\n",
      "self.train_x shape =  torch.Size([700, 2])\n",
      "train_ys shape =  torch.Size([700, 6, 1])\n",
      "step_size datset =  8\n",
      "model step_size =  8\n",
      "train_x after torch.Size([700, 2])\n",
      "self.n_dim=  2\n",
      "dataset.n_dim =  2\n",
      "--> new model saved @ epoch 1\n",
      "x_prev shape =  torch.Size([10, 2])\n",
      "y_pred shape =  torch.Size([10, 1, 64])\n",
      "steps =  [0, 7, 15, 23, 31, 39, 47, 55, 63, 71, 79, 87, 95, 103, 111, 119, 127, 135, 143, 151, 159, 167, 175, 183, 191, 199, 207, 215, 223, 231, 239, 247, 255, 263, 271, 279, 287, 295, 303, 311, 319, 327, 335, 343, 351, 359, 367, 375, 383, 391, 399, 407, 415, 423, 431, 439, 447, 455, 463, 471, 479, 487, 495, 503]\n",
      "len steps =  64\n",
      "mse_lowest =  0.10950452\n",
      "mse_less=  0.088769704\n",
      "train_data shape =  torch.Size([100, 500, 1])\n",
      "step_size =  16\n",
      "model loaded:  model_L1_D16_noise0.pt\n",
      "don't train =  False\n",
      "train_data before dataset torch.Size([100, 500, 1])\n",
      "train_data_repeats shape =  torch.Size([400, 119, 1])\n",
      "step_size =  16\n",
      "x_end_idx =  17\n",
      "y_start_idx =  32\n",
      "y_end_idx =  113\n",
      "range(0, 17, 16)\n",
      "self.train_x shape =  torch.Size([400, 2])\n",
      "train_ys shape =  torch.Size([400, 6, 1])\n",
      "step_size datset =  16\n",
      "model step_size =  16\n",
      "train_x after torch.Size([400, 2])\n",
      "self.n_dim=  2\n",
      "dataset.n_dim =  2\n",
      "--> new model saved @ epoch 1\n",
      "x_prev shape =  torch.Size([10, 2])\n",
      "y_pred shape =  torch.Size([10, 1, 33])\n",
      "steps =  [0, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223, 239, 255, 271, 287, 303, 319, 335, 351, 367, 383, 399, 415, 431, 447, 463, 479, 495, 511]\n",
      "len steps =  33\n",
      "mse_lowest =  0.088769704\n",
      "mse_less=  0.11609815\n",
      "train_data shape =  torch.Size([100, 500, 1])\n",
      "step_size =  32\n",
      "model loaded:  model_L1_D32_noise0.pt\n",
      "don't train =  False\n",
      "train_data before dataset torch.Size([100, 500, 1])\n",
      "train_data_repeats shape =  torch.Size([200, 231, 1])\n",
      "step_size =  32\n",
      "x_end_idx =  33\n",
      "y_start_idx =  64\n",
      "y_end_idx =  225\n",
      "range(0, 33, 32)\n",
      "self.train_x shape =  torch.Size([200, 2])\n",
      "train_ys shape =  torch.Size([200, 6, 1])\n",
      "step_size datset =  32\n",
      "model step_size =  32\n",
      "train_x after torch.Size([200, 2])\n",
      "self.n_dim=  2\n",
      "dataset.n_dim =  2\n",
      "--> new model saved @ epoch 1\n",
      "x_prev shape =  torch.Size([10, 2])\n",
      "y_pred shape =  torch.Size([10, 1, 17])\n",
      "steps =  [0, 31, 63, 95, 127, 159, 191, 223, 255, 287, 319, 351, 383, 415, 447, 479, 511]\n",
      "len steps =  17\n",
      "mse_lowest =  0.088769704\n",
      "mse_less=  0.14013271\n"
     ]
    }
   ],
   "source": [
    "import double_Resnet as tnet\n",
    "current_size = 1\n",
    "models, step_sizes, mse_list, idx_lowest,n_forward_list = find_best_timestep(train_dict[str(current_size)], \n",
    "                                                              val_dict[str(current_size)], \n",
    "                                                              val_dict[str(current_size)], current_size,model_dir=model_dir,# make_new=True, \n",
    "                                                             start_k=2, largest_k = 6, max_epochs = 100, print_every=100, dont_train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plot a bunch\n",
    "# # \n",
    "# step_size = 4\n",
    "# dt =1\n",
    "# n_forward = int(500/step_size - 1)\n",
    "# model_name = 'model_L{}_D{}_noise{}.pt'.format(current_size,step_size, 0)\n",
    "# model_path_this = os.path.join(model_dir, model_name)\n",
    "\n",
    "# n_points, n_timesteps, total_dim = torch.flatten(train_dict[str(current_size)], 2,3).shape\n",
    "# arch = [total_dim, 128, 128, 128, total_dim] \n",
    "\n",
    "\n",
    "# model_time = tnet.ResNet(arch=arch, dt=dt, step_size=step_size)\n",
    "\n",
    "# dataset = tnet.DataSet(torch.flatten(train_dict[str(current_size)], 2,3), torch.flatten(val_dict[str(current_size)], 2,3), \n",
    "#                        torch.flatten(val_dict[str(current_size)], 2,3), dt, step_size, n_forward)\n",
    "\n",
    "# #plot the inputed data\n",
    "# plt.figure()\n",
    "# plt.plot(dataset.val_ys[0, :, 0])#, '.')\n",
    "# #     plt.plot(np.arange(len(dataset.val_ys[point_num, :, i]))*dataset.step_size, dataset.val_ys[point_num, :, i], '.')\n",
    "# #     plt.plot(torch.flatten(val_data, 2,3)[0,s_size:,0])\n",
    "# plt.title('step size = '+str(step_size)+\" start\")\n",
    "# #   plt.xlim([0,100])\n",
    "# plt.show()\n",
    "\n",
    "# # training\n",
    "# for i in range(10):\n",
    "#     model_time.train_net(dataset, batch_size = 32, max_epoch=10,model_path=model_path_this)\n",
    "#     models = list()\n",
    "#     models.append(model_time)\n",
    "#     plt.figure()\n",
    "# #     plt.plot(dataset.val_ys[0, :, 0])\n",
    "#     predicted = tnet.multi_scale_forecast(dataset.val_ys[:,0,:],n_forward, models)\n",
    "#     plt.plot(dataset.val_ys[0,:,0], '.')\n",
    "#     plt.plot(predicted[0,:,0])\n",
    "#     plt.title('i = '+str(i))\n",
    "#     #   plt.xlim([0,100])\n",
    "#     plt.show()\n",
    "\n",
    "# # return model_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[4, 8, 16, 32] [0.10950452, 0.088769704, 0.11609815, 0.14013271]\n",
      "x_prev shape =  torch.Size([100, 2])\n",
      "y_pred shape =  torch.Size([100, 1, 17])\n",
      "steps =  [0, 31, 63, 95, 127, 159, 191, 223, 255, 287, 319, 351, 383, 415, 447, 479, 511]\n",
      "len steps =  17\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_prev shape =  torch.Size([10, 2])\n",
      "y_pred shape =  torch.Size([10, 1, 17])\n",
      "steps =  [0, 31, 63, 95, 127, 159, 191, 223, 255, 287, 319, 351, 383, 415, 447, 479, 511]\n",
      "len steps =  17\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "#====================================================================================\n",
    "def plot_lowest_error(data, model, i = 0):\n",
    "    \"\"\"\n",
    "    Plot data at model, idx\n",
    "    \n",
    "    inputs:\n",
    "        data: tensor of shape (n_points, n_timesteps, dim, dim)\n",
    "        model: Resnet model to predict on \n",
    "        i: int, which validation point to graph\n",
    "    outputs:\n",
    "        No returned values, but graph shown\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    data  = torch.flatten(data, 2,3)\n",
    "    _, total_steps, _ = data.shape\n",
    "    y_preds = model_time.uni_scale_forecast(data[:, :2, 0].float(), n_steps=total_steps-1)\n",
    "\n",
    "#     y_preds = model.uni_scale_forecast(torch.tensor(data[:,0:2,:]).float(), n_steps=total_steps-1)\n",
    "    plt.plot(y_preds[i,:,0], label = \"Predicted\")\n",
    "    plt.plot(data[i,1:,0], label = \"Truth\")\n",
    "    plt.ylim([-.1, 1.1])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#====================================================================================\n",
    "\n",
    "print(step_sizes[idx_lowest])    \n",
    "print(step_sizes, mse_list)\n",
    "plot_lowest_error(train_dict[str(current_size)], models[0], i =2)\n",
    "plot_lowest_error(val_dict[str(current_size)], models[1], i =2)\n",
    "print(models[1].step_size)\n",
    "print(models[0].step_size)\n",
    "\n",
    "# print(train_data.shape)\n",
    "# dataset = tnet.DataSet(torch.flatten(train_data, 2,3), torch.flatten(val_data, 2,3), \n",
    "#                        torch.flatten(val_data, 2,3), 1, step_sizes[idx_lowest], 5)\n",
    "# dataset.plot_val_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_val_data(dataset, point_num = 0, i = 0, other_plot = None):\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(len(dataset.val_ys[point_num, :, i]))*dataset.step_size, dataset.val_ys[point_num, :, i])#, '.')\n",
    "        if other_plot is not None:\n",
    "            plt.plot(other_plot)\n",
    "#             plt.xlim([0,100])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset.train_x.shape)\n",
    "# print(128**2)\n",
    "train_data = train_dict['1']\n",
    "val_data = val_dict['1']\n",
    "plt.plot(val_data[0,:,0,0])\n",
    "print(train_data.shape)\n",
    "s_size = 8\n",
    "dataset = tnet.DataSet(torch.flatten(train_data, 2,3), torch.flatten(val_data, 2,3), \n",
    "                       torch.flatten(val_data, 2,3), 1, s_size, int(np.floor(499/s_size)))\n",
    "# plot_val_data(dataset, other_plot=torch.flatten(val_data, 2,3)[0,s_size:,0])\n",
    "point_num = 0\n",
    "i = 0\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(dataset.val_ys[point_num, :, i]))*dataset.step_size, dataset.val_ys[point_num, :, i])#, '.')\n",
    "plt.plot(np.arange(len(dataset.val_ys[point_num, :, i]))*dataset.step_size, dataset.val_ys[point_num, :, i], '.')\n",
    "plt.plot(torch.flatten(val_data, 2,3)[0,s_size:,0])\n",
    "#   plt.xlim([0,100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 32\n",
    "n_forward = int(500/step_size - 1)\n",
    "current_size = 1\n",
    "train_data = train_dict[str(current_size)]\n",
    "val_data = val_dict[str(current_size)]\n",
    "model_time = train_one_timestep(step_size, torch.flatten(train_data, 2,3), \n",
    "                                         torch.flatten(val_data, 2,3),  torch.flatten(val_data, 2,3), \n",
    "                                         1, n_forward=n_forward,  max_epochs = 1000,)\n",
    "dataset = tnet.DataSet(torch.flatten(train_data, 2,3), torch.flatten(val_data, 2,3), \n",
    "                       torch.flatten(val_data, 2,3), 1, step_size, n_forward)\n",
    "# print(model_time(dataset.val_ys[:,0,:],n_forward).shape)\n",
    "models = list()\n",
    "models.append(model_time)\n",
    "predicted = tnet.multi_scale_forecast(dataset.val_ys[:,0,:],n_forward, models)\n",
    "plt.plot(dataset.val_ys[0,:,0], '.')\n",
    "plt.plot(predicted[0,:,0])\n",
    "# plt.xlim([0,500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================\n",
    "def find_error_4(data, model, truth_data, tol = 1e-5):\n",
    "    \"\"\"\n",
    "    Find error over the 4 squares \n",
    "    \n",
    "    inputs:\n",
    "        data: tensor of size (n_points, n_timesteps, dim, dim) to be predicted or size (n_points, n_timesteps)\n",
    "        model: Resnet object to predict data on\n",
    "        truth_data: tensor of size (n_points, n_timesteps, dim_larger, dim_larger) compared on \n",
    "        tol = 1e-5: tolerance level to mark points as resolved or not\n",
    "        criterion = torch.nn.MSELoss(reduction='none')\n",
    "        \n",
    "    outputs:\n",
    "        resolved: boolean whether complete area is resolved or not\n",
    "        loss: array of floats for size (dim, dim) with mse of each square\n",
    "        unresolved: array of booleans, whether that part is resolved or not. (1 unresolved, 0 resolved)\n",
    "    \"\"\"\n",
    "    if(len(data.shape))==2:\n",
    "        data = data.unsqueeze(2).unsqueeze(3)\n",
    "    assert len(data.shape) == 4\n",
    "    n_points, n_timesteps, dim, _ = data.shape\n",
    "    data  = torch.flatten(data, 2,3)\n",
    "    y_preds = model.uni_scale_forecast(torch.tensor(data[:,0,:]).float(), n_steps=n_timesteps-1).reshape(( n_points, n_timesteps-1, dim,dim))\n",
    "    \n",
    "    _,_, truth_dim, _ = truth_data.shape\n",
    "    assert truth_dim >= dim\n",
    "    \n",
    "    loss = mse(y_preds, truth_data[:,1:])\n",
    "    \n",
    "    resolved =  loss.max() <= tol\n",
    "    unresolved_array = torch.tensor(loss <= tol)\n",
    "    \n",
    "    return resolved, loss, 1-unresolved_array.float()\n",
    "\n",
    "\n",
    "\n",
    "#====================================================================================    \n",
    "    \n",
    "def mse(data1, data2):\n",
    "    \"\"\"\n",
    "    Finds Mean Squared Error between data1 and data2\n",
    "    \n",
    "    inputs:\n",
    "        data1: tensor of shape (n_points, n_timestep, dim1, dim1)\n",
    "        data2: tensor of shape (n_points, n_timestep, dim2, dim2)\n",
    "        \n",
    "    output:\n",
    "        mse: array of size (min_dim, min_dim) with mse \n",
    "    \n",
    "    \"\"\"\n",
    "    #find bigger dim\n",
    "    size1 = data1.shape[-1]\n",
    "    size2 = data2.shape[-1]\n",
    "    size_max = max(size1, size2)\n",
    "    \n",
    "    #grow to save sizes and find mse\n",
    "    mse = np.mean((grow(data1, size_max) - grow(data2, size_max))**2, axis = (0, 1))\n",
    "    return mse\n",
    "#====================================================================================\n",
    "    \n",
    "def grow(data, dim_full=128):\n",
    "    '''\n",
    "    Grow tensor from any size to a bigger size\n",
    "    inputs: \n",
    "        data: tensor to grow, size (n_points, n_timesteps, dim_small, dim_small)\n",
    "        dim_full = 128: int of size to grow data to\n",
    "\n",
    "    outputs:\n",
    "        data_full: tensor size (n_points, n_timesteps, size_full, size_full)\n",
    "    '''\n",
    "    n_points, n_timesteps, dim_small, _ = data.shape \n",
    "    assert dim_full % dim_small == 0 #need small to be multiple of full\n",
    "\n",
    "    divide = dim_full // dim_small\n",
    "\n",
    "    data_full = np.zeros((n_points, n_timesteps, dim_full,dim_full))\n",
    "    for i in range(dim_small):\n",
    "        for j in range(dim_small):\n",
    "            repeated = np.repeat(np.repeat(data[:,:,i,j].reshape(n_points,n_timesteps,1,1), divide, axis = 2), divide, axis = 3)\n",
    "            data_full[:,:,i*divide:(i+1)*divide, j*divide:(j+1)*divide] = repeated\n",
    "    return data_full\n",
    "#====================================================================================\n",
    "\n",
    "\n",
    "resolved, loss, unresolved_list = find_error_4(val_dict['1'], models[idx_lowest], val_dict['2'])\n",
    "print(loss.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unresolved_dict[str(current_size)] = torch.tensor(unresolved_list)\n",
    "\n",
    "print(unresolved_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_train_data = unresolved_list * train_dict[str(current_size*2)]\n",
    "print(next_train_data.shape)\n",
    "plt.imshow(next_train_data[0,0])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keep.append(models[idx_lowest])\n",
    "model_used_dict[str(current_size)] = [[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================\n",
    "def find_error_1(data, model, tol = 1e-5):\n",
    "    \"\"\"\n",
    "    Find error over the 1 square\n",
    "    \n",
    "    inputs:\n",
    "        data: tensor of size (n_points, n_timesteps, dim, dim) to be predicted\n",
    "        model: Resnet object to predict data on\n",
    "        tol = 1e-5: tolerance level to mark points as resolved or not\n",
    "        criterion = torch.nn.MSELoss(reduction='none')\n",
    "        \n",
    "    outputs:\n",
    "        loss: float of mse\n",
    "        resolved: boolean whether resolved or not\n",
    "    \"\"\"\n",
    "    n_points, n_timesteps  = data.shape\n",
    "    dim = 1\n",
    "    data_input  = data.unsqueeze(2)\n",
    "    y_preds = model.uni_scale_forecast(torch.tensor(data_input[:,0,:]).float(), n_steps=n_timesteps-1).reshape(( n_points, n_timesteps-1, dim,dim))\n",
    "    data1 = data[:,1:]\n",
    "    data2 = y_preds[:,:,0,0]\n",
    "#     print()\n",
    "    loss = torch.mean((data1-data2)**2)#mse(y_preds, data[:,1:])\n",
    "    \n",
    "#     print(loss)\n",
    "    \n",
    "    return loss, loss <= tol\n",
    "\n",
    "#====================================================================================\n",
    "\n",
    "\n",
    "current_size = 2\n",
    "next_train_data = unresolved_list * train_dict[str(current_size)]\n",
    "\n",
    "model_idx_list = np.ones((current_size, current_size))*(-1) #start with all -1\n",
    "\n",
    "for i in range(current_size):\n",
    "    for j in range(current_size):\n",
    "        data_this = next_train_data[:,:,i,j]\n",
    "        if (torch.min(data_this) == 0) and (torch.max(data_this) == 0):\n",
    "            #don't need to do anything is model is resolved\n",
    "            continue\n",
    "        else:\n",
    "        #see if the error is low enough on already made model\n",
    "            for m, model in enumerate(model_keep):\n",
    "                loss, resolved = find_error_1(data_this, model)\n",
    "                step_size = model.step_size\n",
    "                print(\"loss = \", loss)\n",
    "                print(\"step_size = \", step_size)\n",
    "                if resolved:\n",
    "                    model_idx_list[i,j] == m\n",
    "                    break\n",
    "                else:\n",
    "                    pass\n",
    "            if not resolved:\n",
    "                i = 0\n",
    "                j = 1\n",
    "                k = int(np.log2(step_size))\n",
    "                print(\"k = \", k)\n",
    "                print(\"train_dict[str(current_size)][:,:,i,j] shape = \", train_dict[str(current_size)][:,:,i,j].shape)\n",
    "                #if no model good, train new model\n",
    "                models, step_sizes, mse_list, idx_lowest = find_best_timestep(train_dict[str(current_size)][:,:,i,j], \n",
    "                                                              val_dict[str(current_size)][:,:,i,j], \n",
    "                                                              val_dict[str(current_size)][:,:,i,j], current_size,model_dir=model_dir,\n",
    "                                                              i=i, j=j, start_k = max(0,k-1), largest_k = k+2)\n",
    "                \n",
    "                vbnm\n",
    "                resolved, loss, unresolved_list = find_error_4(val_dict[str(current_size)][:,:,i,j], \n",
    "                                                               models[idx_lowest], \n",
    "                                                               val_dict[str(current_size)][:,:, i*current_size:(i+1)*current_size, j*current_size:(j+1)*current_size])\n",
    "                model_keep.append(models[idx_lowest])\n",
    "                model_idx_list[i,j] == len(model_keep) #last model will be the one for this square\n",
    "            \n",
    "#             predicted = model.uni_scale_forecast(torch.tensor(data[:,0,:]).float(), n_steps=n_timesteps-1).reshape((  n_points, n_timesteps-1, dim,dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(step_sizes, mse_list, idx_lowest)\n",
    "resolved, loss, unresolved_list = find_error_4(val_dict[str(current_size)][:,:,i,j], \n",
    "                                                               models[idx_lowest], \n",
    "                                                               val_dict[str(current_size*2)][:,:, i*current_size:(i+1)*current_size, j*current_size:(j+1)*current_size])\n",
    "print(loss)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  models[idx_lowest]\n",
    "print(idx_lowest)\n",
    "n_timesteps = 500\n",
    "n_points = 10\n",
    "dim = 1\n",
    "# plt.plot(model(val_dict[str(current_size)][:,:,i,j].unsqueeze(2).unsqueeze(3))[0,:,0,0].detach().numpy(), label = \"predicted\")\n",
    "print(val_dict[str(current_size)][:,0,i,j].unsqueeze(1).shape)\n",
    "val_data_this = val_dict[str(current_size)][:,0,i,j].unsqueeze(1)\n",
    "predicted = model.uni_scale_forecast(val_data_this, n_steps=n_timesteps-1)\n",
    "print(predicted.shape)\n",
    "predicted_reshape = predicted.reshape((  n_points, n_timesteps-1, dim,dim))\n",
    "plt.plot(predicted_reshape[0,:,0], label = \"predicted\")\n",
    "print(val_dict[str(current_size*2)][:,:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size].shape)\n",
    "# plt.plot(val_dict[str(current_size*2)][:,1:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size][0,:,0,0], label = \"Truth\")\n",
    "\n",
    "plt.plot(val_dict[str(current_size*2)][:,1:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size][0,:,0,1], label = \"Truth\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(val_dict[str(current_size*2)][0,0])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = (16+32)/2\n",
    "print(step_size)\n",
    "model = train_one_timestep(int(28), train_dict[str(current_size)][:,:,i,j].unsqueeze(2), \n",
    "                           val_dict[str(current_size)][:,:,i,j].unsqueeze(2), \n",
    "                           val_dict[str(current_size)][:,:,i,j].unsqueeze(2), current_size)\n",
    "#                        dt = 1, n_forward = 5, noise=0, make_new = False, dont_train = True, \n",
    "#                        lr = 1e-3, max_epochs = 10000, batch_size = 50,threshold = 1e-4, \n",
    "#                        model_dir = './models/toy2',i=None, j = None):\n",
    "    \n",
    "#     train_dict[str(current_size)][:,:,i,j], \n",
    "#                                                               val_dict[str(current_size)][:,:,i,j], \n",
    "#                                                               val_dict[str(current_size)][:,:,i,j], current_size,model_dir=model_dir, \n",
    "#                                                               i=i, j=j, start_k = max(0,k-1), largest_k = k+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 1\n",
    "resolved, loss, unresolved_list = find_error_4(val_dict[str(current_size)][:,:,i,j], \n",
    "                                                               model, \n",
    "                                                               val_dict[str(current_size*2)][:,:, i*current_size:(i+1)*current_size, j*current_size:(j+1)*current_size])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model =  models[idx_lowest]\n",
    "print(idx_lowest)\n",
    "n_timesteps = 500\n",
    "n_points = 10\n",
    "dim = 1\n",
    "# plt.plot(model(val_dict[str(current_size)][:,:,i,j].unsqueeze(2).unsqueeze(3))[0,:,0,0].detach().numpy(), label = \"predicted\")\n",
    "print(val_dict[str(current_size)][:,0,i,j].unsqueeze(1).shape)\n",
    "val_data_this = val_dict[str(current_size)][:,0,i,j].unsqueeze(1)\n",
    "predicted = model.uni_scale_forecast(val_data_this, n_steps=n_timesteps-1)\n",
    "print(predicted.shape)\n",
    "predicted_reshape = predicted.reshape((  n_points, n_timesteps-1, dim,dim))\n",
    "plt.plot(predicted_reshape[0,:,0], label = \"predicted\")\n",
    "print(val_dict[str(current_size*2)][:,:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size].shape)\n",
    "# plt.plot(val_dict[str(current_size*2)][:,1:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size][0,:,0,0], label = \"Truth\")\n",
    "\n",
    "plt.plot(val_dict[str(current_size*2)][:,1:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size][0,:,0,1], label = \"Truth\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "n_repeats =  7\n",
      "points_needed =  63\n",
      "torch.Size([100, 500, 1])\n",
      "torch.Size([100, 63, 1])\n",
      "torch.Size([700, 63, 1])\n",
      "i =  0\n",
      "train_data_700[i*100:(i+1)*100, :,:] shape =  torch.Size([100, 63, 1])\n",
      "traind_data_100[:,i*63:(i+1)*63,:] shape =  torch.Size([100, 63, 1])\n",
      "i =  1\n",
      "train_data_700[i*100:(i+1)*100, :,:] shape =  torch.Size([100, 63, 1])\n",
      "traind_data_100[:,i*63:(i+1)*63,:] shape =  torch.Size([100, 63, 1])\n",
      "i =  2\n",
      "train_data_700[i*100:(i+1)*100, :,:] shape =  torch.Size([100, 63, 1])\n",
      "traind_data_100[:,i*63:(i+1)*63,:] shape =  torch.Size([100, 63, 1])\n",
      "i =  3\n",
      "train_data_700[i*100:(i+1)*100, :,:] shape =  torch.Size([100, 63, 1])\n",
      "traind_data_100[:,i*63:(i+1)*63,:] shape =  torch.Size([100, 63, 1])\n",
      "i =  4\n",
      "train_data_700[i*100:(i+1)*100, :,:] shape =  torch.Size([100, 63, 1])\n",
      "traind_data_100[:,i*63:(i+1)*63,:] shape =  torch.Size([100, 63, 1])\n",
      "i =  5\n",
      "train_data_700[i*100:(i+1)*100, :,:] shape =  torch.Size([100, 63, 1])\n",
      "traind_data_100[:,i*63:(i+1)*63,:] shape =  torch.Size([100, 63, 1])\n",
      "i =  6\n",
      "train_data_700[i*100:(i+1)*100, :,:] shape =  torch.Size([100, 63, 1])\n",
      "traind_data_100[:,i*63:(i+1)*63,:] shape =  torch.Size([100, 63, 1])\n",
      "torch.Size([700, 63, 1])\n",
      "step_size =  8\n",
      "self.train_x shape =  torch.Size([700, 2])\n",
      "train_ys shape =  torch.Size([700, 6, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-53-520ed3970dfc>:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.train_x = torch.tensor(train_data[:, x_idx:x_end_idx:step_size, :]).float().to(self.device)\n",
      "<ipython-input-53-520ed3970dfc>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.train_ys = torch.tensor(train_data[:, y_start_idx:y_end_idx:step_size, :]).float().to(self.device)\n",
      "<ipython-input-53-520ed3970dfc>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.val_x = torch.tensor(val_data[:, x_idx:x_end_idx:step_size, :]).float().to(self.device)\n",
      "<ipython-input-53-520ed3970dfc>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.val_ys = torch.tensor(val_data[:, y_start_idx:y_end_idx:step_size, :]).float().to(self.device)\n",
      "<ipython-input-53-520ed3970dfc>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.test_x = torch.tensor(test_data[:, 0, :]).float().to(self.device)\n",
      "<ipython-input-53-520ed3970dfc>:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.test_ys = torch.tensor(test_data[:, 1:, :]).float().to(self.device)\n"
     ]
    }
   ],
   "source": [
    "#practing to make the dataset class have more points if possible\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self, train_data, val_data, test_data, dt, step_size, n_forward):\n",
    "        \"\"\"\n",
    "        :param train_data: array of shape n_train x train_steps x input_dim\n",
    "                            where train_steps = max_step x (n_steps + 1)\n",
    "        :param val_data: array of shape n_val x val_steps x input_dim\n",
    "        :param test_data: array of shape n_test x test_steps x input_dim\n",
    "        :param dt: the unit time step\n",
    "        :param step_size: an integer indicating the step sizes\n",
    "        :param n_forward: number of steps forward\n",
    "        \"\"\"\n",
    "        n_train, train_steps, n_dim = train_data.shape\n",
    "        n_val, val_steps, _ = val_data.shape\n",
    "        n_test, test_steps, _ = test_data.shape\n",
    "        assert step_size*n_forward+1 <= train_steps and step_size*n_forward+1 <= val_steps\n",
    "\n",
    "        # params\n",
    "        self.dt = dt\n",
    "        self.n_dim = n_dim *2\n",
    "        self.step_size = step_size\n",
    "        print(\"step_size = \", step_size)\n",
    "        self.n_forward = n_forward\n",
    "        self.n_train = n_train\n",
    "        self.n_val = n_val\n",
    "        self.n_test = n_test\n",
    "        self.n_input_points = 2\n",
    "\n",
    "        # device\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "        # data\n",
    "        x_idx = 0\n",
    "        x_end_idx = x_idx + step_size*(self.n_input_points-1)+1\n",
    "        y_start_idx = step_size*(self.n_input_points)\n",
    "        y_end_idx = y_start_idx + step_size*n_forward + 1\n",
    "\n",
    "\n",
    "        self.train_x = torch.tensor(train_data[:, x_idx:x_end_idx:step_size, :]).float().to(self.device)\n",
    "        self.train_x = torch.flatten(self.train_x, start_dim = 1)\n",
    "        print(\"self.train_x shape = \", self.train_x.shape)\n",
    "        self.train_ys = torch.tensor(train_data[:, y_start_idx:y_end_idx:step_size, :]).float().to(self.device)\n",
    "        print(\"train_ys shape = \", self.train_ys.shape)\n",
    "        self.val_x = torch.tensor(val_data[:, x_idx:x_end_idx:step_size, :]).float().to(self.device)\n",
    "        self.val_x = torch.flatten(self.val_x, start_dim = 1)\n",
    "        self.val_ys = torch.tensor(val_data[:, y_start_idx:y_end_idx:step_size, :]).float().to(self.device)\n",
    "        # self.val_ys = torch.flatten(self.val_ys, start_dim = 1)\n",
    "        self.test_x = torch.tensor(test_data[:, 0, :]).float().to(self.device)\n",
    "        self.test_ys = torch.tensor(test_data[:, 1:, :]).float().to(self.device)\n",
    "\n",
    "        \n",
    "n_forward = 5\n",
    "max_step = n_forward + 2\n",
    "n_steps = 8\n",
    "print(max_step *(n_steps + 1))\n",
    "\n",
    "points_needed = max_step *(n_steps + 1)\n",
    "n_repeats = int(500/points_needed)\n",
    "traind_data_100 = torch.flatten(train_dict['1'], 2)\n",
    "n_points, n_timesteps, n_dim = traind_data_100.shape\n",
    "print(\"n_repeats = \",n_repeats )\n",
    "print(\"points_needed = \", points_needed)\n",
    "print(traind_data_100.shape)\n",
    "print(torch.flatten(train_dict['1'], 2)[:,:points_needed].shape)\n",
    "\n",
    "#need to reform training data to be 700,63,1\n",
    "train_data_700 = torch.zeros((n_points*n_repeats, points_needed, n_dim))\n",
    "print(train_data_700.shape)\n",
    "for i in range(n_repeats):\n",
    "#     pass\n",
    "    print(\"i = \", i)\n",
    "    print(\"train_data_700[i*100:(i+1)*100, :,:] shape = \", train_data_700[i*n_points:(i+1)*n_points, :,:].shape)\n",
    "    print(\"traind_data_100[:,i*63:(i+1)*63,:] shape = \", traind_data_100[:,i*points_needed:(i+1)*points_needed,:].shape)\n",
    "    \n",
    "    train_data_700[i*n_points:(i+1)*n_points, :,:] = traind_data_100[:,i*points_needed:(i+1)*points_needed,:]\n",
    "    \n",
    "print(train_data_700.shape)\n",
    "dataset = DataSet(train_data_700, torch.flatten(train_dict['1'], 2), torch.flatten(train_dict['1'], 2), 1, n_steps, n_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_repeats shape =  torch.Size([1700, 28, 1])\n",
      "step_size =  3\n",
      "x_end_idx =  4\n",
      "y_start_idx =  6\n",
      "y_end_idx =  22\n",
      "range(0, 4, 3)\n",
      "self.train_x shape =  torch.Size([1700, 2])\n",
      "train_ys shape =  torch.Size([1700, 6, 1])\n",
      "torch.Size([1700, 2])\n",
      "torch.Size([170, 2])\n"
     ]
    }
   ],
   "source": [
    "import utils_time2 as utils\n",
    "\n",
    "dataset =  utils.make_dataset_max_repeat(torch.flatten(train_dict['1'], 2), torch.flatten(val_dict['1'], 2), torch.flatten(val_dict['1'], 2), dt, \n",
    "                                         step_size=3, n_forward=5,n_input_points=2)\n",
    "\n",
    "print(dataset.train_x.shape)\n",
    "print(dataset.val_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
