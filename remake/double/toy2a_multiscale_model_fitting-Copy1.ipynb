{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multiscale model fitting for Toy2a\n",
    "\n",
    "Toy2a is a simplified version of toy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start with initalizing many things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "# import sys\n",
    "import torch\n",
    "# import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from tqdm.notebook import tqdm\n",
    "# import time\n",
    "import math\n",
    "\n",
    "# module_path = os.path.abspath(os.path.join('../src/'))\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)\n",
    "    \n",
    "    \n",
    "# import torch_cae_multilevel_V4 as net\n",
    "import ResNet as tnet\n",
    "# import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "data_dir = '../data/toy2a'\n",
    "model_dir = '../models/toy2a'\n",
    "result_dir = '../result/toy2a'\n",
    "\n",
    "#load data\n",
    "train_data = torch.tensor(np.load(os.path.join(data_dir, 'train_data.npy')))\n",
    "val_data = torch.tensor(np.load(os.path.join(data_dir, 'val_data.npy')))\n",
    "test_data = torch.tensor(np.load(os.path.join(data_dir, 'test_data.npy')))\n",
    "\n",
    "data_of_sizes = {}\n",
    "current_size = 2\n",
    "unresolved_dict = {}\n",
    "model_keep = list()\n",
    "model_used_dict = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_size =  32\n",
      "x_end_idx =  33\n",
      "y_start_idx =  64\n",
      "y_end_idx =  225\n",
      "range(0, 33, 32)\n",
      "self.train_x shape =  torch.Size([100, 128])\n",
      "train_ys shape =  torch.Size([100, 6, 64])\n"
     ]
    }
   ],
   "source": [
    "#testing dataset new structure\n",
    "dt = 1\n",
    "step_size = 32\n",
    "n_forward = 5\n",
    "dataset = tnet.DataSet(torch.flatten(train_data,2,3), torch.flatten(val_data,2,3), torch.flatten(test_data,2,3), dt, step_size, n_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 6, 64])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.train_ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions, will move these to a utils file eventually \n",
    "#====================================================================================\n",
    "# def data_of_size(data,size):\n",
    "#     \"\"\"\n",
    "#     Takes averages to shrink size of data\n",
    "#     Takes data of size (n_points, dim, dim) and shrinks to size (n_points, size, size)\n",
    "#     takes averages to shrink\n",
    "#     \"\"\"\n",
    "#     return decrease_to_size(torch.tensor(data).unsqueeze(1), size)[:,0,:,:]\n",
    "#====================================================================================\n",
    "\n",
    "\n",
    "def isPowerOfTwo(n):\n",
    "    \"\"\"\n",
    "    checks if n is a power of two\n",
    "    \n",
    "    input: n, int\n",
    "    \n",
    "    output: boolean\n",
    "    \"\"\"\n",
    "    return (np.ceil(np.log2(n)) == np.floor(np.log2(n)));\n",
    "#====================================================================================\n",
    "def shrink(data, low_dim):\n",
    "    '''\n",
    "    Shrinks data to certain size; either averages or takes endpoints\n",
    "    \n",
    "    inputs:\n",
    "        data: array of size (n_points, n_timesteps, dim, dim) that will shrink\n",
    "        low_dim: int, size to shrink to, low_dim must be less than or equal to dim\n",
    "        \n",
    "    output:\n",
    "        data: array of size (n_points, n_timesteps, low_dim, low_dim)\n",
    "    '''\n",
    "    \n",
    "    #check inputs\n",
    "    assert len(data.shape) == 4\n",
    "    n_points, n_timesteps, dim, _ = data.shape\n",
    "    assert dim >= low_dim\n",
    "    assert isPowerOfTwo(low_dim)\n",
    "    \n",
    "    if dim == low_dim: #same size, no change\n",
    "        return data\n",
    "    \n",
    "    while(dim > low_dim):\n",
    "        #shrink by 1 level until same size\n",
    "        data = apply_local_op(data.float(), 'cpu', ave=average)\n",
    "        current_size = data.shape[-1]\n",
    "        \n",
    "    return data\n",
    "#====================================================================================\n",
    "def ave_one_level(data):\n",
    "    '''\n",
    "    takes averages to shrink data 1 level\n",
    "    \n",
    "    inputs:\n",
    "        data: tensor of size (n_points, n_timesteps, dim, dim) that will shrink\n",
    "        \n",
    "    output:\n",
    "        processed data: tensor of size (n_points, n_timesteps, dim/2, dim/2)\n",
    "    '''\n",
    "    device = 'cpu'\n",
    "    if not torch.is_tensor(data): #needs to be a tensor\n",
    "        data = torch.tensor(data)\n",
    "        \n",
    "    assert len(data.shape) == 4\n",
    "#     if data.shape != 4:\n",
    "#         print(\"data.shape = \", data.shape)\n",
    "#         print(\"data.shape should be of length 4\")\n",
    "    n_points, n_timesteps, dim, _ = data.shape\n",
    "    \n",
    "    #dim needs to be even \n",
    "    assert dim % 2 == 0\n",
    "    \n",
    "    data_right_size = torch.flatten(data, 0,1).unsqueeze(1).float()\n",
    "    \n",
    "#     n = min(in_channels, out_channels)\n",
    "    op = torch.nn.Conv2d(1, 1, 2, stride=2, padding=0).to(device)\n",
    "   \n",
    "    op.weight.data = torch.zeros(op.weight.data.size()).to(device)\n",
    "    op.bias.data = torch.zeros(op.bias.data.size()).to(device)\n",
    "    op.weight.data[0,0, :, :] = torch.ones(op.weight.data[0,0, :, :].size()).to(device) / 4\n",
    "\n",
    "    # make them non-trainable\n",
    "    for param in op.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    print(\"Transforming\")\n",
    "        \n",
    "    shrunk = op(data_right_size)\n",
    "    \n",
    "    print(\"reshape to print\")\n",
    "    \n",
    "    return shrunk.squeeze(1).reshape((n_points, n_timesteps, dim//2, dim//2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_data.shape)\n",
    "# processed = ave_one_level(train_data)\n",
    "# print(processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#make a dictionary with train data of every size 128->1\n",
    "#====================================================================================\n",
    "\n",
    "def make_dict_all_sizes(data):\n",
    "    \"\"\"\n",
    "    Makes a dictionary of data at every refinedment size from current->1\n",
    "    \n",
    "    inputs:\n",
    "        data: tensor(or array) of size (n_points, n_timesteps, dim, dim)\n",
    "        \n",
    "    outputs: \n",
    "        dic: dictionary of tensors. Keys are dim size, tensors are size (n_points, n_timesteps, dim, dim)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n_points, n_timesteps, dim, _ = data.shape\n",
    "    \n",
    "    if not torch.is_tensor(data): #needs to be a tensor\n",
    "        data = torch.tensor(data)\n",
    "        \n",
    "    assert isPowerOfTwo(dim)\n",
    "        \n",
    "    dic = {str(dim): data}\n",
    "    \n",
    "    for i in range(int(np.log2(dim))):\n",
    "        #decrease\n",
    "        print(\"i = \", i)\n",
    "        data = ave_one_level(data)\n",
    "        dic[str(data.shape[-1])] = data\n",
    "    \n",
    "    print(dic.keys())\n",
    "    \n",
    "    return dic\n",
    "#====================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  0\n",
      "Transforming\n",
      "reshape to print\n",
      "i =  1\n",
      "Transforming\n",
      "reshape to print\n",
      "i =  2\n",
      "Transforming\n",
      "reshape to print\n",
      "dict_keys(['8', '4', '2', '1'])\n",
      "i =  0\n",
      "Transforming\n",
      "reshape to print\n",
      "i =  1\n",
      "Transforming\n",
      "reshape to print\n",
      "i =  2\n",
      "Transforming\n",
      "reshape to print\n",
      "dict_keys(['8', '4', '2', '1'])\n"
     ]
    }
   ],
   "source": [
    "train_dict = make_dict_all_sizes(train_data)\n",
    "val_dict = make_dict_all_sizes(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([499, 1])\n",
      "torch.Size([499, 1])\n"
     ]
    }
   ],
   "source": [
    "train_x = train_dict['1'][0,:-1,0]\n",
    "print(train_x.shape)\n",
    "train_y = train_dict['1'][0,1:,0]\n",
    "print(train_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using new ResNet thing\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_data(data, step_size = 1):\n",
    "    \"\"\"\n",
    "    Forms data to input to network. \n",
    "    \n",
    "    inputs:\n",
    "        data: torch. shape, (n_points, n_timesteps, 1, 1)\n",
    "        step_size: int\n",
    "        \n",
    "    outputs:\n",
    "        inputs, torch shape (max_points, 3)\n",
    "        outputs, torch shape (max_points, 1)\n",
    "    \"\"\"\n",
    "    print(\"data shape = \", data.shape)\n",
    "    train_data = data[:,::step_size]\n",
    "    inputs = torch.cat((train_data[:,:-3,0], train_data[:,1:-2,0], train_data[:,2:-1,0]), axis = 2)\n",
    "    inputs = torch.flatten(inputs, end_dim=1)\n",
    "    outputs = train_data[:,3:,0]\n",
    "    outputs = torch.flatten(outputs, end_dim=1)\n",
    "    \n",
    "    return inputs, outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using new ResNet thing\n"
     ]
    }
   ],
   "source": [
    "import ResNet as tnet\n",
    "#====================================================================================\n",
    "def train_one_timestep(step_size, train_data, val_data=None, test_data=None, current_size=1, \n",
    "                       dt = 1, n_forward = 5, noise=0, make_new = False, dont_train = True, \n",
    "                       lr = 1e-3, max_epochs = 10000, batch_size = 50,threshold = 1e-4, \n",
    "                       model_dir = './models/toy2',i=None, j = None,print_every=1000):\n",
    "\n",
    "    \"\"\"\n",
    "    fits or loads model at 1 timestep\n",
    "    \n",
    "    inputs:\n",
    "        step_size: int \n",
    "        train_data: tensor size (n_points, n_timesteps, dim**2) \n",
    "        val_data:tensor size (n_val_points, n_timesteps, dim**2) \n",
    "        test_data:tensor size (n_test_points, n_timesteps, dim**2) \n",
    "        current_size: int, only used in file naming\n",
    "        dt = 1: float\n",
    "        n_forward = 5: int, number of steps to consider during training\n",
    "        noise=0: float, level of noise, (right now just used in file naming)\n",
    "        make_new = False: boolean, whether or not to make a new model if old already exists\n",
    "        dont_train = True: boolean, whether or not to train more if model loaded\n",
    "        lr = 1e-3: float, learning rate\n",
    "        max_epochs = 10000: int \n",
    "        batch_size = 50: int\n",
    "        threshold=1e-4: float, stop training when validation gets below threshold\n",
    "         \n",
    "    \n",
    "    outputs:\n",
    "        model_time: ResNet object of trained model. Also saved\n",
    "    \"\"\"\n",
    "    print(\"inside train_one_timestep\")\n",
    "    if (i is not None) and (j is not None):\n",
    "        model_name = 'model_L{}_D{}_noise{}_i{}_j{}.pt'.format(current_size,step_size, noise, i, j)\n",
    "    else:\n",
    "        model_name = 'model_L{}_D{}_noise{}.pt'.format(current_size,step_size, noise)\n",
    "    model_path_this = os.path.join(model_dir, model_name)\n",
    "    \n",
    "#     n_points, n_timesteps, total_dim = train_data.shape\n",
    "#     arch = [total_dim, 128, 128, 128, total_dim] \n",
    "    \n",
    "    try: #if we already have a model saved\n",
    "        if make_new:\n",
    "            print(\"Making a new model. Old one deleted. model {}\".format(model_name))\n",
    "            assert False\n",
    "        model_time = torch.load(model_path_this)\n",
    "        print(\"model loaded: \", model_name)\n",
    "        print(\"don't train = \", dont_train)\n",
    "        if dont_train: #just load model, no training\n",
    "            return model_time\n",
    "    except:\n",
    "        print('create model {} ...'.format(model_name))\n",
    "        inputs, outputs = form_data(train_data, step_size)\n",
    "        val_inputs, val_outputs = form_data(val_data, step_size)\n",
    "    \n",
    "        model_time = tnet.ResNet(inputs, outputs,val_inputs, val_outputs,step_size, model_name=model_name)\n",
    "    \n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model_time.parameters())\n",
    "\n",
    "    model_time.train_model(optimizer, criterion)\n",
    "    \n",
    "    return model_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside train_one_timestep\n",
      "create model model_L1_D32_noise0.pt ...\n",
      "data shape =  torch.Size([100, 500, 1, 1])\n",
      "data shape =  torch.Size([10, 500, 1, 1])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-07e67b60409d>\u001b[0m in \u001b[0;36mtrain_one_timestep\u001b[1;34m(step_size, train_data, val_data, test_data, current_size, dt, n_forward, noise, make_new, dont_train, lr, max_epochs, batch_size, threshold, model_dir, i, j, print_every)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mmodel_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path_this\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model loaded: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './models/toy2\\\\model_L1_D32_noise0.pt'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-3cc141a5e49e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_one_timestep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-79-07e67b60409d>\u001b[0m in \u001b[0;36mtrain_one_timestep\u001b[1;34m(step_size, train_data, val_data, test_data, current_size, dt, n_forward, noise, make_new, dont_train, lr, max_epochs, batch_size, threshold, model_dir, i, j, print_every)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mval_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mform_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         model_time = tnet.ResNet(inputs, outputs,val_input, val_outputs,\n\u001b[0m\u001b[0;32m     56\u001b[0m         step_size, model_name=model_name)\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_input' is not defined"
     ]
    }
   ],
   "source": [
    "step_size = 32\n",
    "model_time = train_one_timestep(step_size, train_dict['1'], val_dict['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-81-6cdaa836ad83>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs_first = torch.tensor(inputs[:-1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x22cc179ea00>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(torch.tensor(x[0:1]).unsqueeze(1))\n",
    "# inputs = np.zeros((498, 2))\n",
    "# inputs = torch.cat((train_dict['1'][0,:-2,0], train_dict['1'][0,1:-1,0]), axis = 1)\n",
    "data_smaller = train_dict['1'][:,::step_size]\n",
    "i = 3\n",
    "inputs = torch.cat((data_smaller[i,:-3,0], data_smaller[i,1:-2,0], data_smaller[i,2:-1,0]), axis = 1)\n",
    "outputs = data_smaller[i,3:,0]\n",
    "# inputs = inputs[::step_size]\n",
    "# outputs = outputs[::4]\n",
    "    \n",
    "# inputs = train_dict['1'][0,:-1,0]\n",
    "# outputs = train_dict['1'][0,2:,0]\n",
    "inputs_first = torch.tensor(inputs[:-1])\n",
    "plt.plot(inputs[:,0], label = \"inputs\")\n",
    "# plt.plot(outputs[:,0], label = \"outputs\")#,'--bo')\n",
    "t = 0\n",
    "y_pred = model_time(inputs_first[0:3].float())\n",
    "y_pred = torch.cat((inputs_first[0:3,0:2].float(),y_pred), axis = 1)\n",
    "to_plot = [y_pred.detach().numpy()[0,0]]\n",
    "# plt.plot(t,y_pred.detach().numpy()[0,0],'.')\n",
    "for i in range(int(498/step_size)):\n",
    "    y_next = model_time(y_pred)\n",
    "    y_next = torch.cat((y_pred[:, 1:3],y_next), axis = 1)\n",
    "    to_plot.append(y_next.detach().numpy()[0,0])\n",
    "#     plt.plot(i + 2, y_next.detach().numpy()[0,0],'.')\n",
    "    y_pred = y_next\n",
    "plt.plot(to_plot, label = \"predicted\")\n",
    "plt.legend()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#====================================================================================\n",
    "\n",
    "def find_best_timestep(train_data, val_data, test_data, current_size, start_k = 0, largest_k = 7, \n",
    "                       dt = 1, n_forward = 5, noise=0, make_new = False, dont_train = True,\n",
    "                       lr = 1e-3, max_epochs = 10000, batch_size = 50,threshold = 1e-4, \n",
    "                       criterion = torch.nn.MSELoss(reduction='none'), model_dir = \"./models/toy2\",\n",
    "                       i=None, j = None,print_every= 1000):\n",
    "    \"\"\"\n",
    "    Trains models with different timestep sizes and finds lowest error\n",
    "    \n",
    "    inputs:\n",
    "     n_forward = 5, noise=0, make_new = False, dont_train = False):\n",
    "    \n",
    "        train_data: tensor size (n_points, n_timesteps, dim, dim), or  size (n_points, n_timesteps)\n",
    "        val_data:tensor size (n_val_points, n_timesteps, dim, dim) , or  size (n_val_points, n_timesteps)\n",
    "        test_data:tensor size (n_test_points, n_timesteps, dim, dim) , or  size (n_test_points, n_timesteps)\n",
    "        current_size: int, only used in file naming\n",
    "        start_k = 0: int, smallest timestep will be 2**start_k\n",
    "        largest_k = 7:int, largest timestep will be 2**largest_k\n",
    "        dt = 1: float\n",
    "        n_forward = 5: int, number of steps to consider during training\n",
    "        noise=0: float, level of noise, (right now just used in file naming)\n",
    "        make_new = False: boolean, whether or not to make a new model if old already exists\n",
    "        dont_train = False: boolean, whether or not to train more if model loaded\n",
    "        lr = 1e-3: float, learning rate\n",
    "        max_epochs = 10000: int \n",
    "        batch_size = 50: int\n",
    "        threshold=1e-4: float\n",
    "        criterion = torch.nn.MSELoss(reduction='none'))\n",
    "         \n",
    "         \n",
    "    outputs:\n",
    "        models: list of ResNet models\n",
    "        step_sizes: list of ints for the steps_sizes of models \n",
    "        mse_list: list of floats, mse of models \n",
    "        idx_lowest: int, index value with lowest mse\n",
    "         \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #transform data shapes if needed\n",
    "    if(len(train_data.shape)== 2):\n",
    "        train_data = train_data.unsqueeze(2).unsqueeze(3)\n",
    "        val_data = val_data.unsqueeze(2).unsqueeze(3)\n",
    "        test_data = test_data.unsqueeze(2).unsqueeze(3)\n",
    "    assert(len(train_data.shape)== 4)\n",
    "    assert(len(val_data.shape)== 4)\n",
    "    assert(len(test_data.shape)== 4)\n",
    "    \n",
    "    models = list()\n",
    "    step_sizes = list()\n",
    "    n_forward_list = list()\n",
    "    mse_lowest = 1e10 #big number\n",
    "    mse_list = list()\n",
    "    mse_less = 0\n",
    "    idx_lowest = -1\n",
    "    \n",
    "    #make data flat to right dim (n_points, n_timesteps, dim**2)\n",
    "#     train_data = torch.flatten(train_data, 2,3)\n",
    "#     val_data = torch.flatten(val_data, 2,3)\n",
    "#     test_data = torch.flatten(test_data, 2,3)\n",
    "    \n",
    "    n_points, n_timesteps, _,_ = train_data.shape\n",
    "    \n",
    "    \n",
    "    for idx, k in enumerate(range(start_k, largest_k)):\n",
    "        step_size = 2**k\n",
    "        step_sizes.append(step_size)\n",
    "        \n",
    "#         model_time = train_one_timestep(step_size, train_data, val_data, test_data, current_size, \n",
    "#                                         make_new = make_new, dont_train = dont_train,i=i, j=j, \n",
    "#                                         n_forward=n_forward, max_epochs=max_epochs,model_dir=model_dir, print_every = print_every)\n",
    "        \n",
    "        model_time = train_one_timestep(step_size, train_data, val_data)\n",
    "        \n",
    "        models.append(model_time)\n",
    "    \n",
    "        #find error\n",
    "        \n",
    "        y_preds = model_time.uni_scale_forecast(val_data[:, 0, :].float(), n_steps=n_timesteps-1)\n",
    "        mse_all = criterion(val_data[:, 1:, :].float(), y_preds).mean(-1)\n",
    "\n",
    "        mean = mse_all.mean(0).detach().numpy()\n",
    "#         print(mean.shape)\n",
    "        mse_less = mean.mean()\n",
    "        mse_list.append(mse_less)\n",
    "\n",
    "        print(\"mse_lowest = \", mse_lowest)\n",
    "        print(\"mse_less= \", mse_less)\n",
    "        \n",
    "        if (mse_less< mse_lowest) or (math.isnan(mse_lowest)) or (math.isnan(mse_less)):\n",
    "            mse_lowest = mse_less\n",
    "            idx_lowest = idx\n",
    "\n",
    "    return models, step_sizes, mse_list, idx_lowest, n_forward_list\n",
    "#====================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 500, 1, 1])\n",
      "inside train_one_timestep\n",
      "create model model_L1_D4_noise0.pt ...\n",
      "data shape =  torch.Size([100, 500, 1, 1])\n",
      "data shape =  torch.Size([10, 500, 1, 1])\n",
      "epoch  0 : train_error:  0.55573124 : val_loss  0.54958874\n",
      "epoch  100 : train_error:  0.043707527 : val_loss  0.043520708\n",
      "epoch  200 : train_error:  0.0072000367 : val_loss  0.0066443644\n",
      "epoch  300 : train_error:  0.00043423031 : val_loss  0.0004297197\n",
      "epoch  400 : train_error:  0.000101258236 : val_loss  0.000101032914\n",
      "epoch  500 : train_error:  4.2519092e-05 : val_loss  4.2163425e-05\n",
      "epoch  600 : train_error:  2.0990321e-05 : val_loss  2.0908385e-05\n",
      "epoch  700 : train_error:  1.1208473e-05 : val_loss  1.1294518e-05\n",
      "epoch  800 : train_error:  7.1870286e-06 : val_loss  7.09289e-06\n",
      "epoch  900 : train_error:  4.9058135e-06 : val_loss  4.9861937e-06\n",
      "x_prev shape =  torch.Size([10, 1, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (10x1 and 3x20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-7a312fe220f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcurrent_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m models, step_sizes, mse_list, idx_lowest,n_forward_list = find_best_timestep(train_dict[str(current_size)], \n\u001b[0m\u001b[0;32m      4\u001b[0m                                                               \u001b[0mval_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                                               \u001b[0mval_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmake_new\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m#print_every=100,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-99-68dedbcbd2a0>\u001b[0m in \u001b[0;36mfind_best_timestep\u001b[1;34m(train_data, val_data, test_data, current_size, start_k, largest_k, dt, n_forward, noise, make_new, dont_train, lr, max_epochs, batch_size, threshold, criterion, model_dir, i, j, print_every)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;31m#find error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0my_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_time\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muni_scale_forecast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_timesteps\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[0mmse_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_preds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\double\\ResNet.py\u001b[0m in \u001b[0;36muni_scale_forecast\u001b[1;34m(self, x_init, n_steps, interpolate)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[0mcur_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mcur_step\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mn_steps\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m             \u001b[0mx_next\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_prev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_next\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[0msteps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\double\\ResNet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;31m#relu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#.float()))      # activation function for hidden layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_hidden_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Linear_{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1692\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1693\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x1 and 3x20)"
     ]
    }
   ],
   "source": [
    "current_size = 1\n",
    "print(train_dict[str(current_size)].shape)\n",
    "models, step_sizes, mse_list, idx_lowest,n_forward_list = find_best_timestep(train_dict[str(current_size)], \n",
    "                                                              val_dict[str(current_size)], \n",
    "                                                              val_dict[str(current_size)], current_size,model_dir=model_dir, make_new=True, #print_every=100, \n",
    "                                                             start_k=2, largest_k = 3)#, dont_train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1500/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plot a bunch\n",
    "# # \n",
    "# step_size = 4\n",
    "# dt =1\n",
    "# n_forward = int(500/step_size - 1)\n",
    "# model_name = 'model_L{}_D{}_noise{}.pt'.format(current_size,step_size, 0)\n",
    "# model_path_this = os.path.join(model_dir, model_name)\n",
    "\n",
    "# n_points, n_timesteps, total_dim = torch.flatten(train_dict[str(current_size)], 2,3).shape\n",
    "# arch = [total_dim, 128, 128, 128, total_dim] \n",
    "\n",
    "\n",
    "# model_time = tnet.ResNet(arch=arch, dt=dt, step_size=step_size)\n",
    "\n",
    "# dataset = tnet.DataSet(torch.flatten(train_dict[str(current_size)], 2,3), torch.flatten(val_dict[str(current_size)], 2,3), \n",
    "#                        torch.flatten(val_dict[str(current_size)], 2,3), dt, step_size, n_forward)\n",
    "\n",
    "# #plot the inputed data\n",
    "# plt.figure()\n",
    "# plt.plot(dataset.val_ys[0, :, 0])#, '.')\n",
    "# #     plt.plot(np.arange(len(dataset.val_ys[point_num, :, i]))*dataset.step_size, dataset.val_ys[point_num, :, i], '.')\n",
    "# #     plt.plot(torch.flatten(val_data, 2,3)[0,s_size:,0])\n",
    "# plt.title('step size = '+str(step_size)+\" start\")\n",
    "# #   plt.xlim([0,100])\n",
    "# plt.show()\n",
    "\n",
    "# # training\n",
    "# for i in range(10):\n",
    "#     model_time.train_net(dataset, batch_size = 32, max_epoch=10,model_path=model_path_this)\n",
    "#     models = list()\n",
    "#     models.append(model_time)\n",
    "#     plt.figure()\n",
    "# #     plt.plot(dataset.val_ys[0, :, 0])\n",
    "#     predicted = tnet.multi_scale_forecast(dataset.val_ys[:,0,:],n_forward, models)\n",
    "#     plt.plot(dataset.val_ys[0,:,0], '.')\n",
    "#     plt.plot(predicted[0,:,0])\n",
    "#     plt.title('i = '+str(i))\n",
    "#     #   plt.xlim([0,100])\n",
    "#     plt.show()\n",
    "\n",
    "# # return model_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================\n",
    "def plot_lowest_error(data, model, i = 0):\n",
    "    \"\"\"\n",
    "    Plot data at model, idx\n",
    "    \n",
    "    inputs:\n",
    "        data: tensor of shape (n_points, n_timesteps, dim, dim)\n",
    "        model: Resnet model to predict on \n",
    "        i: int, which validation point to graph\n",
    "    outputs:\n",
    "        No returned values, but graph shown\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    data  = torch.flatten(data, 2,3)\n",
    "    _, total_steps, _ = data.shape\n",
    "    y_preds = model.uni_scale_forecast(torch.tensor(data[:,0,:]).float(), n_steps=total_steps-1)\n",
    "    plt.plot(y_preds[i,:,0], label = \"Predicted\")\n",
    "    plt.plot(data[i,1:,0], label = \"Truth\")\n",
    "    plt.ylim([-.1, 1.1])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#====================================================================================\n",
    "\n",
    "print(step_sizes[idx_lowest])    \n",
    "print(step_sizes, mse_list)\n",
    "plot_lowest_error(val_dict[str(current_size)], models[idx_lowest], i =2)\n",
    "\n",
    "# print(train_data.shape)\n",
    "# dataset = tnet.DataSet(torch.flatten(train_data, 2,3), torch.flatten(val_data, 2,3), \n",
    "#                        torch.flatten(val_data, 2,3), 1, step_sizes[idx_lowest], 5)\n",
    "# dataset.plot_val_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_val_data(dataset, point_num = 0, i = 0, other_plot = None):\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(len(dataset.val_ys[point_num, :, i]))*dataset.step_size, dataset.val_ys[point_num, :, i])#, '.')\n",
    "        if other_plot is not None:\n",
    "            plt.plot(other_plot)\n",
    "#             plt.xlim([0,100])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset.train_x.shape)\n",
    "# print(128**2)\n",
    "train_data = train_dict['1']\n",
    "val_data = val_dict['1']\n",
    "plt.plot(val_data[0,:,0,0])\n",
    "print(train_data.shape)\n",
    "s_size = 8\n",
    "dataset = tnet.DataSet(torch.flatten(train_data, 2,3), torch.flatten(val_data, 2,3), \n",
    "                       torch.flatten(val_data, 2,3), 1, s_size, int(np.floor(499/s_size)))\n",
    "# plot_val_data(dataset, other_plot=torch.flatten(val_data, 2,3)[0,s_size:,0])\n",
    "point_num = 0\n",
    "i = 0\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(dataset.val_ys[point_num, :, i]))*dataset.step_size, dataset.val_ys[point_num, :, i])#, '.')\n",
    "plt.plot(np.arange(len(dataset.val_ys[point_num, :, i]))*dataset.step_size, dataset.val_ys[point_num, :, i], '.')\n",
    "plt.plot(torch.flatten(val_data, 2,3)[0,s_size:,0])\n",
    "#   plt.xlim([0,100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 32\n",
    "n_forward = int(500/step_size - 1)\n",
    "current_size = 1\n",
    "train_data = train_dict[str(current_size)]\n",
    "val_data = val_dict[str(current_size)]\n",
    "model_time = train_one_timestep(step_size, torch.flatten(train_data, 2,3), \n",
    "                                         torch.flatten(val_data, 2,3),  torch.flatten(val_data, 2,3), \n",
    "                                         1, n_forward=n_forward,  max_epochs = 1000,)\n",
    "dataset = tnet.DataSet(torch.flatten(train_data, 2,3), torch.flatten(val_data, 2,3), \n",
    "                       torch.flatten(val_data, 2,3), 1, step_size, n_forward)\n",
    "# print(model_time(dataset.val_ys[:,0,:],n_forward).shape)\n",
    "models = list()\n",
    "models.append(model_time)\n",
    "predicted = tnet.multi_scale_forecast(dataset.val_ys[:,0,:],n_forward, models)\n",
    "plt.plot(dataset.val_ys[0,:,0], '.')\n",
    "plt.plot(predicted[0,:,0])\n",
    "# plt.xlim([0,500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================\n",
    "def find_error_4(data, model, truth_data, tol = 1e-5):\n",
    "    \"\"\"\n",
    "    Find error over the 4 squares \n",
    "    \n",
    "    inputs:\n",
    "        data: tensor of size (n_points, n_timesteps, dim, dim) to be predicted or size (n_points, n_timesteps)\n",
    "        model: Resnet object to predict data on\n",
    "        truth_data: tensor of size (n_points, n_timesteps, dim_larger, dim_larger) compared on \n",
    "        tol = 1e-5: tolerance level to mark points as resolved or not\n",
    "        criterion = torch.nn.MSELoss(reduction='none')\n",
    "        \n",
    "    outputs:\n",
    "        resolved: boolean whether complete area is resolved or not\n",
    "        loss: array of floats for size (dim, dim) with mse of each square\n",
    "        unresolved: array of booleans, whether that part is resolved or not. (1 unresolved, 0 resolved)\n",
    "    \"\"\"\n",
    "    if(len(data.shape))==2:\n",
    "        data = data.unsqueeze(2).unsqueeze(3)\n",
    "    assert len(data.shape) == 4\n",
    "    n_points, n_timesteps, dim, _ = data.shape\n",
    "    data  = torch.flatten(data, 2,3)\n",
    "    y_preds = model.uni_scale_forecast(torch.tensor(data[:,0,:]).float(), n_steps=n_timesteps-1).reshape(( n_points, n_timesteps-1, dim,dim))\n",
    "    \n",
    "    _,_, truth_dim, _ = truth_data.shape\n",
    "    assert truth_dim >= dim\n",
    "    \n",
    "    loss = mse(y_preds, truth_data[:,1:])\n",
    "    \n",
    "    resolved =  loss.max() <= tol\n",
    "    unresolved_array = torch.tensor(loss <= tol)\n",
    "    \n",
    "    return resolved, loss, 1-unresolved_array.float()\n",
    "\n",
    "\n",
    "\n",
    "#====================================================================================    \n",
    "    \n",
    "def mse(data1, data2):\n",
    "    \"\"\"\n",
    "    Finds Mean Squared Error between data1 and data2\n",
    "    \n",
    "    inputs:\n",
    "        data1: tensor of shape (n_points, n_timestep, dim1, dim1)\n",
    "        data2: tensor of shape (n_points, n_timestep, dim2, dim2)\n",
    "        \n",
    "    output:\n",
    "        mse: array of size (min_dim, min_dim) with mse \n",
    "    \n",
    "    \"\"\"\n",
    "    #find bigger dim\n",
    "    size1 = data1.shape[-1]\n",
    "    size2 = data2.shape[-1]\n",
    "    size_max = max(size1, size2)\n",
    "    \n",
    "    #grow to save sizes and find mse\n",
    "    mse = np.mean((grow(data1, size_max) - grow(data2, size_max))**2, axis = (0, 1))\n",
    "    return mse\n",
    "#====================================================================================\n",
    "    \n",
    "def grow(data, dim_full=128):\n",
    "    '''\n",
    "    Grow tensor from any size to a bigger size\n",
    "    inputs: \n",
    "        data: tensor to grow, size (n_points, n_timesteps, dim_small, dim_small)\n",
    "        dim_full = 128: int of size to grow data to\n",
    "\n",
    "    outputs:\n",
    "        data_full: tensor size (n_points, n_timesteps, size_full, size_full)\n",
    "    '''\n",
    "    n_points, n_timesteps, dim_small, _ = data.shape \n",
    "    assert dim_full % dim_small == 0 #need small to be multiple of full\n",
    "\n",
    "    divide = dim_full // dim_small\n",
    "\n",
    "    data_full = np.zeros((n_points, n_timesteps, dim_full,dim_full))\n",
    "    for i in range(dim_small):\n",
    "        for j in range(dim_small):\n",
    "            repeated = np.repeat(np.repeat(data[:,:,i,j].reshape(n_points,n_timesteps,1,1), divide, axis = 2), divide, axis = 3)\n",
    "            data_full[:,:,i*divide:(i+1)*divide, j*divide:(j+1)*divide] = repeated\n",
    "    return data_full\n",
    "#====================================================================================\n",
    "\n",
    "\n",
    "resolved, loss, unresolved_list = find_error_4(val_dict['1'], models[idx_lowest], val_dict['2'])\n",
    "print(loss.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unresolved_dict[str(current_size)] = torch.tensor(unresolved_list)\n",
    "\n",
    "print(unresolved_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_train_data = unresolved_list * train_dict[str(current_size*2)]\n",
    "print(next_train_data.shape)\n",
    "plt.imshow(next_train_data[0,0])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keep.append(models[idx_lowest])\n",
    "model_used_dict[str(current_size)] = [[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================\n",
    "def find_error_1(data, model, tol = 1e-5):\n",
    "    \"\"\"\n",
    "    Find error over the 1 square\n",
    "    \n",
    "    inputs:\n",
    "        data: tensor of size (n_points, n_timesteps, dim, dim) to be predicted\n",
    "        model: Resnet object to predict data on\n",
    "        tol = 1e-5: tolerance level to mark points as resolved or not\n",
    "        criterion = torch.nn.MSELoss(reduction='none')\n",
    "        \n",
    "    outputs:\n",
    "        loss: float of mse\n",
    "        resolved: boolean whether resolved or not\n",
    "    \"\"\"\n",
    "    n_points, n_timesteps  = data.shape\n",
    "    dim = 1\n",
    "    data_input  = data.unsqueeze(2)\n",
    "    y_preds = model.uni_scale_forecast(torch.tensor(data_input[:,0,:]).float(), n_steps=n_timesteps-1).reshape(( n_points, n_timesteps-1, dim,dim))\n",
    "    data1 = data[:,1:]\n",
    "    data2 = y_preds[:,:,0,0]\n",
    "#     print()\n",
    "    loss = torch.mean((data1-data2)**2)#mse(y_preds, data[:,1:])\n",
    "    \n",
    "#     print(loss)\n",
    "    \n",
    "    return loss, loss <= tol\n",
    "\n",
    "#====================================================================================\n",
    "\n",
    "\n",
    "current_size = 2\n",
    "next_train_data = unresolved_list * train_dict[str(current_size)]\n",
    "\n",
    "model_idx_list = np.ones((current_size, current_size))*(-1) #start with all -1\n",
    "\n",
    "for i in range(current_size):\n",
    "    for j in range(current_size):\n",
    "        data_this = next_train_data[:,:,i,j]\n",
    "        if (torch.min(data_this) == 0) and (torch.max(data_this) == 0):\n",
    "            #don't need to do anything is model is resolved\n",
    "            continue\n",
    "        else:\n",
    "        #see if the error is low enough on already made model\n",
    "            for m, model in enumerate(model_keep):\n",
    "                loss, resolved = find_error_1(data_this, model)\n",
    "                step_size = model.step_size\n",
    "                print(\"loss = \", loss)\n",
    "                print(\"step_size = \", step_size)\n",
    "                if resolved:\n",
    "                    model_idx_list[i,j] == m\n",
    "                    break\n",
    "                else:\n",
    "                    pass\n",
    "            if not resolved:\n",
    "                i = 0\n",
    "                j = 1\n",
    "                k = int(np.log2(step_size))\n",
    "                print(\"k = \", k)\n",
    "                print(\"train_dict[str(current_size)][:,:,i,j] shape = \", train_dict[str(current_size)][:,:,i,j].shape)\n",
    "                #if no model good, train new model\n",
    "                models, step_sizes, mse_list, idx_lowest = find_best_timestep(train_dict[str(current_size)][:,:,i,j], \n",
    "                                                              val_dict[str(current_size)][:,:,i,j], \n",
    "                                                              val_dict[str(current_size)][:,:,i,j], current_size,model_dir=model_dir,\n",
    "                                                              i=i, j=j, start_k = max(0,k-1), largest_k = k+2)\n",
    "                \n",
    "                vbnm\n",
    "                resolved, loss, unresolved_list = find_error_4(val_dict[str(current_size)][:,:,i,j], \n",
    "                                                               models[idx_lowest], \n",
    "                                                               val_dict[str(current_size)][:,:, i*current_size:(i+1)*current_size, j*current_size:(j+1)*current_size])\n",
    "                model_keep.append(models[idx_lowest])\n",
    "                model_idx_list[i,j] == len(model_keep) #last model will be the one for this square\n",
    "            \n",
    "#             predicted = model.uni_scale_forecast(torch.tensor(data[:,0,:]).float(), n_steps=n_timesteps-1).reshape((  n_points, n_timesteps-1, dim,dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(step_sizes, mse_list, idx_lowest)\n",
    "resolved, loss, unresolved_list = find_error_4(val_dict[str(current_size)][:,:,i,j], \n",
    "                                                               models[idx_lowest], \n",
    "                                                               val_dict[str(current_size*2)][:,:, i*current_size:(i+1)*current_size, j*current_size:(j+1)*current_size])\n",
    "print(loss)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  models[idx_lowest]\n",
    "print(idx_lowest)\n",
    "n_timesteps = 500\n",
    "n_points = 10\n",
    "dim = 1\n",
    "# plt.plot(model(val_dict[str(current_size)][:,:,i,j].unsqueeze(2).unsqueeze(3))[0,:,0,0].detach().numpy(), label = \"predicted\")\n",
    "print(val_dict[str(current_size)][:,0,i,j].unsqueeze(1).shape)\n",
    "val_data_this = val_dict[str(current_size)][:,0,i,j].unsqueeze(1)\n",
    "predicted = model.uni_scale_forecast(val_data_this, n_steps=n_timesteps-1)\n",
    "print(predicted.shape)\n",
    "predicted_reshape = predicted.reshape((  n_points, n_timesteps-1, dim,dim))\n",
    "plt.plot(predicted_reshape[0,:,0], label = \"predicted\")\n",
    "print(val_dict[str(current_size*2)][:,:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size].shape)\n",
    "# plt.plot(val_dict[str(current_size*2)][:,1:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size][0,:,0,0], label = \"Truth\")\n",
    "\n",
    "plt.plot(val_dict[str(current_size*2)][:,1:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size][0,:,0,1], label = \"Truth\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(val_dict[str(current_size*2)][0,0])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = (16+32)/2\n",
    "print(step_size)\n",
    "model = train_one_timestep(int(28), train_dict[str(current_size)][:,:,i,j].unsqueeze(2), \n",
    "                           val_dict[str(current_size)][:,:,i,j].unsqueeze(2), \n",
    "                           val_dict[str(current_size)][:,:,i,j].unsqueeze(2), current_size)\n",
    "#                        dt = 1, n_forward = 5, noise=0, make_new = False, dont_train = True, \n",
    "#                        lr = 1e-3, max_epochs = 10000, batch_size = 50,threshold = 1e-4, \n",
    "#                        model_dir = './models/toy2',i=None, j = None):\n",
    "    \n",
    "#     train_dict[str(current_size)][:,:,i,j], \n",
    "#                                                               val_dict[str(current_size)][:,:,i,j], \n",
    "#                                                               val_dict[str(current_size)][:,:,i,j], current_size,model_dir=model_dir, \n",
    "#                                                               i=i, j=j, start_k = max(0,k-1), largest_k = k+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 1\n",
    "resolved, loss, unresolved_list = find_error_4(val_dict[str(current_size)][:,:,i,j], \n",
    "                                                               model, \n",
    "                                                               val_dict[str(current_size*2)][:,:, i*current_size:(i+1)*current_size, j*current_size:(j+1)*current_size])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model =  models[idx_lowest]\n",
    "print(idx_lowest)\n",
    "n_timesteps = 500\n",
    "n_points = 10\n",
    "dim = 1\n",
    "# plt.plot(model(val_dict[str(current_size)][:,:,i,j].unsqueeze(2).unsqueeze(3))[0,:,0,0].detach().numpy(), label = \"predicted\")\n",
    "print(val_dict[str(current_size)][:,0,i,j].unsqueeze(1).shape)\n",
    "val_data_this = val_dict[str(current_size)][:,0,i,j].unsqueeze(1)\n",
    "predicted = model.uni_scale_forecast(val_data_this, n_steps=n_timesteps-1)\n",
    "print(predicted.shape)\n",
    "predicted_reshape = predicted.reshape((  n_points, n_timesteps-1, dim,dim))\n",
    "plt.plot(predicted_reshape[0,:,0], label = \"predicted\")\n",
    "print(val_dict[str(current_size*2)][:,:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size].shape)\n",
    "# plt.plot(val_dict[str(current_size*2)][:,1:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size][0,:,0,0], label = \"Truth\")\n",
    "\n",
    "plt.plot(val_dict[str(current_size*2)][:,1:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size][0,:,0,1], label = \"Truth\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
