{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multiscale model fitting for Toy2a\n",
    "\n",
    "Toy2a is a simplified version of toy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start with initalizing many things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using new ResNet thing\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "# import sys\n",
    "import torch\n",
    "# import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from tqdm.notebook import tqdm\n",
    "# import time\n",
    "import math\n",
    "\n",
    "# module_path = os.path.abspath(os.path.join('../src/'))\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)\n",
    "    \n",
    "    \n",
    "# import torch_cae_multilevel_V4 as net\n",
    "import ResNet as tnet\n",
    "# import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "data_dir = '../data/toy2a'\n",
    "model_dir = '../models/toy2a'\n",
    "result_dir = '../result/toy2a'\n",
    "\n",
    "#load data\n",
    "train_data = torch.tensor(np.load(os.path.join(data_dir, 'train_data.npy')))\n",
    "val_data = torch.tensor(np.load(os.path.join(data_dir, 'val_data.npy')))\n",
    "test_data = torch.tensor(np.load(os.path.join(data_dir, 'test_data.npy')))\n",
    "\n",
    "data_of_sizes = {}\n",
    "current_size = 2\n",
    "unresolved_dict = {}\n",
    "model_keep = list()\n",
    "model_used_dict = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_size =  32\n",
      "x_end_idx =  33\n",
      "y_start_idx =  64\n",
      "y_end_idx =  225\n",
      "range(0, 33, 32)\n",
      "self.train_x shape =  torch.Size([100, 128])\n",
      "train_ys shape =  torch.Size([100, 6, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16kat\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\double\\utils_time2.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.train_x = torch.tensor(train_data[:, x_idx:x_end_idx:step_size, :]).float().to(self.device)\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\double\\utils_time2.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.train_ys = torch.tensor(train_data[:, y_start_idx:y_end_idx:step_size, :]).float().to(self.device)\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\double\\utils_time2.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.val_x = torch.tensor(val_data[:, x_idx:x_end_idx:step_size, :]).float().to(self.device)\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\double\\utils_time2.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.val_ys = torch.tensor(val_data[:, y_start_idx:y_end_idx:step_size, :]).float().to(self.device)\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\double\\utils_time2.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.test_x = torch.tensor(test_data[:, 0, :]).float().to(self.device)\n",
      "C:\\Users\\16kat\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\double\\utils_time2.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.test_ys = torch.tensor(test_data[:, 1:, :]).float().to(self.device)\n"
     ]
    }
   ],
   "source": [
    "#testing dataset new structure\n",
    "dt = 1\n",
    "step_size = 32\n",
    "n_forward = 5\n",
    "dataset = tnet.DataSet(torch.flatten(train_data,2,3), torch.flatten(val_data,2,3), torch.flatten(test_data,2,3), dt, step_size, n_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 6, 64])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.train_ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions, will move these to a utils file eventually \n",
    "#====================================================================================\n",
    "# def data_of_size(data,size):\n",
    "#     \"\"\"\n",
    "#     Takes averages to shrink size of data\n",
    "#     Takes data of size (n_points, dim, dim) and shrinks to size (n_points, size, size)\n",
    "#     takes averages to shrink\n",
    "#     \"\"\"\n",
    "#     return decrease_to_size(torch.tensor(data).unsqueeze(1), size)[:,0,:,:]\n",
    "#====================================================================================\n",
    "\n",
    "\n",
    "def isPowerOfTwo(n):\n",
    "    \"\"\"\n",
    "    checks if n is a power of two\n",
    "    \n",
    "    input: n, int\n",
    "    \n",
    "    output: boolean\n",
    "    \"\"\"\n",
    "    return (np.ceil(np.log2(n)) == np.floor(np.log2(n)));\n",
    "#====================================================================================\n",
    "def shrink(data, low_dim):\n",
    "    '''\n",
    "    Shrinks data to certain size; either averages or takes endpoints\n",
    "    \n",
    "    inputs:\n",
    "        data: array of size (n_points, n_timesteps, dim, dim) that will shrink\n",
    "        low_dim: int, size to shrink to, low_dim must be less than or equal to dim\n",
    "        \n",
    "    output:\n",
    "        data: array of size (n_points, n_timesteps, low_dim, low_dim)\n",
    "    '''\n",
    "    \n",
    "    #check inputs\n",
    "    assert len(data.shape) == 4\n",
    "    n_points, n_timesteps, dim, _ = data.shape\n",
    "    assert dim >= low_dim\n",
    "    assert isPowerOfTwo(low_dim)\n",
    "    \n",
    "    if dim == low_dim: #same size, no change\n",
    "        return data\n",
    "    \n",
    "    while(dim > low_dim):\n",
    "        #shrink by 1 level until same size\n",
    "        data = apply_local_op(data.float(), 'cpu', ave=average)\n",
    "        current_size = data.shape[-1]\n",
    "        \n",
    "    return data\n",
    "#====================================================================================\n",
    "def ave_one_level(data):\n",
    "    '''\n",
    "    takes averages to shrink data 1 level\n",
    "    \n",
    "    inputs:\n",
    "        data: tensor of size (n_points, n_timesteps, dim, dim) that will shrink\n",
    "        \n",
    "    output:\n",
    "        processed data: tensor of size (n_points, n_timesteps, dim/2, dim/2)\n",
    "    '''\n",
    "    device = 'cpu'\n",
    "    if not torch.is_tensor(data): #needs to be a tensor\n",
    "        data = torch.tensor(data)\n",
    "        \n",
    "    assert len(data.shape) == 4\n",
    "#     if data.shape != 4:\n",
    "#         print(\"data.shape = \", data.shape)\n",
    "#         print(\"data.shape should be of length 4\")\n",
    "    n_points, n_timesteps, dim, _ = data.shape\n",
    "    \n",
    "    #dim needs to be even \n",
    "    assert dim % 2 == 0\n",
    "    \n",
    "    data_right_size = torch.flatten(data, 0,1).unsqueeze(1).float()\n",
    "    \n",
    "#     n = min(in_channels, out_channels)\n",
    "    op = torch.nn.Conv2d(1, 1, 2, stride=2, padding=0).to(device)\n",
    "   \n",
    "    op.weight.data = torch.zeros(op.weight.data.size()).to(device)\n",
    "    op.bias.data = torch.zeros(op.bias.data.size()).to(device)\n",
    "    op.weight.data[0,0, :, :] = torch.ones(op.weight.data[0,0, :, :].size()).to(device) / 4\n",
    "\n",
    "    # make them non-trainable\n",
    "    for param in op.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    print(\"Transforming\")\n",
    "        \n",
    "    shrunk = op(data_right_size)\n",
    "    \n",
    "    print(\"reshape to print\")\n",
    "    \n",
    "    return shrunk.squeeze(1).reshape((n_points, n_timesteps, dim//2, dim//2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_data.shape)\n",
    "# processed = ave_one_level(train_data)\n",
    "# print(processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#make a dictionary with train data of every size 128->1\n",
    "#====================================================================================\n",
    "\n",
    "def make_dict_all_sizes(data):\n",
    "    \"\"\"\n",
    "    Makes a dictionary of data at every refinedment size from current->1\n",
    "    \n",
    "    inputs:\n",
    "        data: tensor(or array) of size (n_points, n_timesteps, dim, dim)\n",
    "        \n",
    "    outputs: \n",
    "        dic: dictionary of tensors. Keys are dim size, tensors are size (n_points, n_timesteps, dim, dim)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n_points, n_timesteps, dim, _ = data.shape\n",
    "    \n",
    "    if not torch.is_tensor(data): #needs to be a tensor\n",
    "        data = torch.tensor(data)\n",
    "        \n",
    "    assert isPowerOfTwo(dim)\n",
    "        \n",
    "    dic = {str(dim): data}\n",
    "    \n",
    "    for i in range(int(np.log2(dim))):\n",
    "        #decrease\n",
    "        print(\"i = \", i)\n",
    "        data = ave_one_level(data)\n",
    "        dic[str(data.shape[-1])] = data\n",
    "    \n",
    "    print(dic.keys())\n",
    "    \n",
    "    return dic\n",
    "#====================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  0\n",
      "Transforming\n",
      "reshape to print\n",
      "i =  1\n",
      "Transforming\n",
      "reshape to print\n",
      "i =  2\n",
      "Transforming\n",
      "reshape to print\n",
      "dict_keys(['8', '4', '2', '1'])\n",
      "i =  0\n",
      "Transforming\n",
      "reshape to print\n",
      "i =  1\n",
      "Transforming\n",
      "reshape to print\n",
      "i =  2\n",
      "Transforming\n",
      "reshape to print\n",
      "dict_keys(['8', '4', '2', '1'])\n"
     ]
    }
   ],
   "source": [
    "train_dict = make_dict_all_sizes(train_data)\n",
    "val_dict = make_dict_all_sizes(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([499, 1])\n",
      "torch.Size([499, 1])\n"
     ]
    }
   ],
   "source": [
    "train_x = train_dict['1'][0,:-1,0]\n",
    "print(train_x.shape)\n",
    "train_y = train_dict['1'][0,1:,0]\n",
    "print(train_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ResNet as tnet\n",
    "#====================================================================================\n",
    "def train_one_timestep(step_size, train_data, val_data=None, test_data=None, current_size=1, \n",
    "                       dt = 1, n_forward = 5, noise=0, make_new = False, dont_train = True, \n",
    "                       lr = 1e-3, max_epochs = 10000, batch_size = 50,threshold = 1e-4, \n",
    "                       model_dir = './models/toy2',i=None, j = None,print_every=1000):\n",
    "\n",
    "    \"\"\"\n",
    "    fits or loads model at 1 timestep\n",
    "    \n",
    "    inputs:\n",
    "        step_size: int \n",
    "        train_data: tensor size (n_points, n_timesteps, dim**2) \n",
    "        val_data:tensor size (n_val_points, n_timesteps, dim**2) \n",
    "        test_data:tensor size (n_test_points, n_timesteps, dim**2) \n",
    "        current_size: int, only used in file naming\n",
    "        dt = 1: float\n",
    "        n_forward = 5: int, number of steps to consider during training\n",
    "        noise=0: float, level of noise, (right now just used in file naming)\n",
    "        make_new = False: boolean, whether or not to make a new model if old already exists\n",
    "        dont_train = True: boolean, whether or not to train more if model loaded\n",
    "        lr = 1e-3: float, learning rate\n",
    "        max_epochs = 10000: int \n",
    "        batch_size = 50: int\n",
    "        threshold=1e-4: float, stop training when validation gets below threshold\n",
    "         \n",
    "    \n",
    "    outputs:\n",
    "        model_time: ResNet object of trained model. Also saved\n",
    "    \"\"\"\n",
    "    print(\"inside train_one_timestep\")\n",
    "    if (i is not None) and (j is not None):\n",
    "        \n",
    "        model_name = 'model_L{}_D{}_noise{}_i{}_j{}.pt'.format(current_size,step_size, noise, i, j)\n",
    "    else:\n",
    "        model_name = 'model_L{}_D{}_noise{}.pt'.format(current_size,step_size, noise)\n",
    "    model_path_this = os.path.join(model_dir, model_name)\n",
    "    \n",
    "#     n_points, n_timesteps, total_dim = train_data.shape\n",
    "#     arch = [total_dim, 128, 128, 128, total_dim] \n",
    "    \n",
    "#     try: #if we already have a model saved\n",
    "#         if make_new:\n",
    "#             print(\"Making a new model. Old one deleted. model {}\".format(model_name))\n",
    "#             assert False\n",
    "#         model_time = torch.load(model_path_this)\n",
    "#         print(\"model loaded: \", model_name)\n",
    "#         print(\"don't train = \", dont_train)\n",
    "#         if dont_train: #just load model, no training\n",
    "#             return model_time\n",
    "#     except:\n",
    "#         print('create model {} ...'.format(model_name))\n",
    "    model_time = tnet.ResNet()#arch=arch, dt=dt, step_size=step_size)\n",
    "\n",
    "#     print(\"train data before, \", train_data.shape)\n",
    "#     dataset = tnet.DataSet(train_data, val_data, test_data, dt, step_size, n_forward)\n",
    "#     print(\"train data after = \", dataset.train_x.shape)\n",
    "#     print(\"train y after = \", dataset.train_ys.shape)\n",
    "    print(train_data.shape)\n",
    "    train_x = train_data[0,:-1,0]\n",
    "    print(train_x.shape)\n",
    "    train_y = train_data[0,1:,0]\n",
    "    print(train_y.shape)\n",
    "    \n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model_time.parameters())\n",
    "\n",
    "    model_time.train_model(optimizer, criterion,  train_x, train_y)\n",
    "    \n",
    "    return model_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside train_one_timestep\n",
      "torch.Size([100, 500, 1, 1])\n",
      "torch.Size([499, 1])\n",
      "torch.Size([499, 1])\n",
      "inputs size =  torch.Size([499, 1])\n",
      "outputs size =  torch.Size([499, 1])\n",
      "Epoch =  0 : Training error =  0.14328499\n",
      "Epoch =  1 : Training error =  0.13947417\n",
      "Epoch =  2 : Training error =  0.1357455\n",
      "Epoch =  3 : Training error =  0.1321102\n",
      "Epoch =  4 : Training error =  0.1286011\n",
      "Epoch =  5 : Training error =  0.12521563\n",
      "Epoch =  6 : Training error =  0.122013256\n",
      "Epoch =  7 : Training error =  0.11898882\n",
      "Epoch =  8 : Training error =  0.11620607\n",
      "Epoch =  9 : Training error =  0.11355029\n",
      "Epoch =  10 : Training error =  0.11095535\n",
      "Epoch =  11 : Training error =  0.108386904\n",
      "Epoch =  12 : Training error =  0.10586692\n",
      "Epoch =  13 : Training error =  0.1034038\n",
      "Epoch =  14 : Training error =  0.1009731\n",
      "Epoch =  15 : Training error =  0.098412864\n",
      "Epoch =  16 : Training error =  0.09581023\n",
      "Epoch =  17 : Training error =  0.09327812\n",
      "Epoch =  18 : Training error =  0.090828605\n",
      "Epoch =  19 : Training error =  0.08847322\n",
      "Epoch =  20 : Training error =  0.0862095\n",
      "Epoch =  21 : Training error =  0.08405919\n",
      "Epoch =  22 : Training error =  0.082149565\n",
      "Epoch =  23 : Training error =  0.080421485\n",
      "Epoch =  24 : Training error =  0.07885103\n",
      "Epoch =  25 : Training error =  0.07752332\n",
      "Epoch =  26 : Training error =  0.0763568\n",
      "Epoch =  27 : Training error =  0.07534389\n",
      "Epoch =  28 : Training error =  0.07446229\n",
      "Epoch =  29 : Training error =  0.0737053\n",
      "Epoch =  30 : Training error =  0.073103465\n",
      "Epoch =  31 : Training error =  0.07285378\n",
      "Epoch =  32 : Training error =  0.07277809\n",
      "Epoch =  33 : Training error =  0.07280061\n",
      "Epoch =  34 : Training error =  0.07283844\n",
      "Epoch =  35 : Training error =  0.072871104\n",
      "Epoch =  36 : Training error =  0.07289532\n",
      "Epoch =  37 : Training error =  0.07290249\n",
      "Epoch =  38 : Training error =  0.07288425\n",
      "Epoch =  39 : Training error =  0.07283387\n",
      "Epoch =  40 : Training error =  0.0727461\n",
      "Epoch =  41 : Training error =  0.07261296\n",
      "Epoch =  42 : Training error =  0.07242798\n",
      "Epoch =  43 : Training error =  0.07218808\n",
      "Epoch =  44 : Training error =  0.07189303\n",
      "Epoch =  45 : Training error =  0.071544945\n",
      "Epoch =  46 : Training error =  0.07114544\n",
      "Epoch =  47 : Training error =  0.07069359\n",
      "Epoch =  48 : Training error =  0.07020709\n",
      "Epoch =  49 : Training error =  0.06969558\n",
      "Epoch =  50 : Training error =  0.06917821\n",
      "Epoch =  51 : Training error =  0.06867324\n",
      "Epoch =  52 : Training error =  0.06819738\n",
      "Epoch =  53 : Training error =  0.067766696\n",
      "Epoch =  54 : Training error =  0.06741954\n",
      "Epoch =  55 : Training error =  0.06708224\n",
      "Epoch =  56 : Training error =  0.06674753\n",
      "Epoch =  57 : Training error =  0.06646499\n",
      "Epoch =  58 : Training error =  0.06614293\n",
      "Epoch =  59 : Training error =  0.06576508\n",
      "Epoch =  60 : Training error =  0.06533426\n",
      "Epoch =  61 : Training error =  0.06485407\n",
      "Epoch =  62 : Training error =  0.06432704\n",
      "Epoch =  63 : Training error =  0.06375983\n",
      "Epoch =  64 : Training error =  0.06316858\n",
      "Epoch =  65 : Training error =  0.06255791\n",
      "Epoch =  66 : Training error =  0.06193182\n",
      "Epoch =  67 : Training error =  0.061303813\n",
      "Epoch =  68 : Training error =  0.0607126\n",
      "Epoch =  69 : Training error =  0.060123555\n",
      "Epoch =  70 : Training error =  0.059545055\n",
      "Epoch =  71 : Training error =  0.05892628\n",
      "Epoch =  72 : Training error =  0.05824127\n",
      "Epoch =  73 : Training error =  0.05748924\n",
      "Epoch =  74 : Training error =  0.056684118\n",
      "Epoch =  75 : Training error =  0.05586194\n",
      "Epoch =  76 : Training error =  0.05501105\n",
      "Epoch =  77 : Training error =  0.054130178\n",
      "Epoch =  78 : Training error =  0.05322294\n",
      "Epoch =  79 : Training error =  0.052287955\n",
      "Epoch =  80 : Training error =  0.051319506\n",
      "Epoch =  81 : Training error =  0.05029608\n",
      "Epoch =  82 : Training error =  0.049209688\n",
      "Epoch =  83 : Training error =  0.048023634\n",
      "Epoch =  84 : Training error =  0.046682358\n",
      "Epoch =  85 : Training error =  0.045173377\n",
      "Epoch =  86 : Training error =  0.043568112\n",
      "Epoch =  87 : Training error =  0.041923136\n",
      "Epoch =  88 : Training error =  0.04021343\n",
      "Epoch =  89 : Training error =  0.038429182\n",
      "Epoch =  90 : Training error =  0.03658668\n",
      "Epoch =  91 : Training error =  0.03471639\n",
      "Epoch =  92 : Training error =  0.032839138\n",
      "Epoch =  93 : Training error =  0.030958632\n",
      "Epoch =  94 : Training error =  0.029039154\n",
      "Epoch =  95 : Training error =  0.027075844\n",
      "Epoch =  96 : Training error =  0.025077764\n",
      "Epoch =  97 : Training error =  0.02305105\n",
      "Epoch =  98 : Training error =  0.02103514\n",
      "Epoch =  99 : Training error =  0.019133389\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_time = train_one_timestep(4, train_dict['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([499, 1])\n",
      "torch.Size([499, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(torch.tensor(x[0:1]).unsqueeze(1))\n",
    "inputs = train_dict['1'][0,:-1,0]\n",
    "outputs = train_dict['1'][0,1:,0]\n",
    "print(inputs.shape)\n",
    "print(outputs.shape)\n",
    "# inputs_first = torch.tensor(inputs[:98])\n",
    "# print(range(2,100),inputs_first.shape)\n",
    "# print(range(2, 100),outputs.shape)\n",
    "plt.plot(range(1, 500),outputs)\n",
    "t = 1\n",
    "# print(inputs_first[0:2].float().shape)\n",
    "y_pred = model_time(inputs[0:1].float())#inputs_first[0:2].float())\n",
    "print(y_pred.shape)\n",
    "# y_pred = torch.cat((inputs_first[0:2,0:1].float(),y_pred), axis = 1)\n",
    "print(y_pred.shape)\n",
    "plt.plot(t,y_pred.detach().numpy()[0,0],'.')\n",
    "for i in range(99):\n",
    "    y_next = model_time(y_pred)\n",
    "#     y_next = torch.cat((y_pred[:, 1:2], y_next), axis = 1)\n",
    "    plt.plot(i + 2, y_next.detach().numpy()[0,0],'.')\n",
    "    y_pred = y_next\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#====================================================================================\n",
    "\n",
    "def find_best_timestep(train_data, val_data, test_data, current_size, start_k = 0, largest_k = 7, \n",
    "                       dt = 1, n_forward = 5, noise=0, make_new = False, dont_train = True,\n",
    "                       lr = 1e-3, max_epochs = 10000, batch_size = 50,threshold = 1e-4, \n",
    "                       criterion = torch.nn.MSELoss(reduction='none'), model_dir = \"./models/toy2\",\n",
    "                       i=None, j = None,print_every= 1000):\n",
    "    \"\"\"\n",
    "    Trains models with different timestep sizes and finds lowest error\n",
    "    \n",
    "    inputs:\n",
    "     n_forward = 5, noise=0, make_new = False, dont_train = False):\n",
    "    \n",
    "        train_data: tensor size (n_points, n_timesteps, dim, dim), or  size (n_points, n_timesteps)\n",
    "        val_data:tensor size (n_val_points, n_timesteps, dim, dim) , or  size (n_val_points, n_timesteps)\n",
    "        test_data:tensor size (n_test_points, n_timesteps, dim, dim) , or  size (n_test_points, n_timesteps)\n",
    "        current_size: int, only used in file naming\n",
    "        start_k = 0: int, smallest timestep will be 2**start_k\n",
    "        largest_k = 7:int, largest timestep will be 2**largest_k\n",
    "        dt = 1: float\n",
    "        n_forward = 5: int, number of steps to consider during training\n",
    "        noise=0: float, level of noise, (right now just used in file naming)\n",
    "        make_new = False: boolean, whether or not to make a new model if old already exists\n",
    "        dont_train = False: boolean, whether or not to train more if model loaded\n",
    "        lr = 1e-3: float, learning rate\n",
    "        max_epochs = 10000: int \n",
    "        batch_size = 50: int\n",
    "        threshold=1e-4: float\n",
    "        criterion = torch.nn.MSELoss(reduction='none'))\n",
    "         \n",
    "         \n",
    "    outputs:\n",
    "        models: list of ResNet models\n",
    "        step_sizes: list of ints for the steps_sizes of models \n",
    "        mse_list: list of floats, mse of models \n",
    "        idx_lowest: int, index value with lowest mse\n",
    "         \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #transform data shapes if needed\n",
    "    if(len(train_data.shape)== 2):\n",
    "        train_data = train_data.unsqueeze(2).unsqueeze(3)\n",
    "        val_data = val_data.unsqueeze(2).unsqueeze(3)\n",
    "        test_data = test_data.unsqueeze(2).unsqueeze(3)\n",
    "    assert(len(train_data.shape)== 4)\n",
    "    assert(len(val_data.shape)== 4)\n",
    "    assert(len(test_data.shape)== 4)\n",
    "    \n",
    "    models = list()\n",
    "    step_sizes = list()\n",
    "    n_forward_list = list()\n",
    "    mse_lowest = 1e10 #big number\n",
    "    mse_list = list()\n",
    "    mse_less = 0\n",
    "    idx_lowest = -1\n",
    "    \n",
    "    #make data flat to right dim (n_points, n_timesteps, dim**2)\n",
    "    train_data = torch.flatten(train_data, 2,3)\n",
    "    val_data = torch.flatten(val_data, 2,3)\n",
    "    test_data = torch.flatten(test_data, 2,3)\n",
    "    \n",
    "    n_points, n_timesteps, total_dim = train_data.shape\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, k in enumerate(range(start_k, largest_k)):\n",
    "        step_size = 2**k\n",
    "        step_sizes.append(step_size)\n",
    "        \n",
    "        #going to make n_forward the max if can be \n",
    "#         n_forward = int((np.floor(n_timesteps/step_size)-1)/2)\n",
    "#         n_forward_list.append(n_forward)\n",
    "#         print(\"n_forward = \", n_forward)\n",
    "        \n",
    "        model_time = train_one_timestep(step_size, train_data, val_data, test_data, current_size, \n",
    "                                        make_new = make_new, dont_train = dont_train,i=i, j=j, \n",
    "                                        n_forward=n_forward, max_epochs=max_epochs,model_dir=model_dir, print_every = print_every)\n",
    "        \n",
    "        models.append(model_time)\n",
    "    \n",
    "        #find error\n",
    "        \n",
    "        y_preds = model_time.uni_scale_forecast(val_data[:, 0, :].float(), n_steps=n_timesteps-1)\n",
    "        mse_all = criterion(val_data[:, 1:, :].float(), y_preds).mean(-1)\n",
    "\n",
    "        mean = mse_all.mean(0).detach().numpy()\n",
    "#         print(mean.shape)\n",
    "        mse_less = mean.mean()\n",
    "        mse_list.append(mse_less)\n",
    "\n",
    "        print(\"mse_lowest = \", mse_lowest)\n",
    "        print(\"mse_less= \", mse_less)\n",
    "        \n",
    "        if (mse_less< mse_lowest) or (math.isnan(mse_lowest)) or (math.isnan(mse_less)):\n",
    "            mse_lowest = mse_less\n",
    "            idx_lowest = idx\n",
    "\n",
    "    return models, step_sizes, mse_list, idx_lowest, n_forward_list\n",
    "#====================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-545b9b5e03dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# print(torch.tensor(x[0:1]).unsqueeze(1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0minputs_first\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m98\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minputs_first\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 500, 1, 1])\n",
      "inside train_one_timestep\n",
      "model loaded:  model_L1_D4_noise0.pt\n",
      "don't train =  True\n",
      "x_prev shape =  torch.Size([10, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (10x1 and 2x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-9cf47312e6ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcurrent_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m models, step_sizes, mse_list, idx_lowest,n_forward_list = find_best_timestep(train_dict[str(current_size)], \n\u001b[0m\u001b[0;32m      4\u001b[0m                                                               \u001b[0mval_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                                               \u001b[0mval_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;31m# make_new=True, print_every=100,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-00a310c24bc1>\u001b[0m in \u001b[0;36mfind_best_timestep\u001b[1;34m(train_data, val_data, test_data, current_size, start_k, largest_k, dt, n_forward, noise, make_new, dont_train, lr, max_epochs, batch_size, threshold, criterion, model_dir, i, j, print_every)\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;31m#find error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         \u001b[0my_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_time\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muni_scale_forecast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_timesteps\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m         \u001b[0mmse_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_preds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\double\\double_Resnet.py\u001b[0m in \u001b[0;36muni_scale_forecast\u001b[1;34m(self, x_init, n_steps, interpolate)\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mcur_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mcur_step\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mn_steps\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m             \u001b[0mx_next\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_prev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_next\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[0msteps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\double\\double_Resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x_init)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnext\u001b[0m \u001b[0mstep\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0mx\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \"\"\"\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_init\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'increment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_init\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[1;31m# print(\"xinit shape = \", x_init.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;31m# print('ans shape = ', ans.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - acornonsite.com\\Time_Space_multiscale\\remake\\double\\double_Resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# print(\"x shape = \", x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_layers\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Linear_{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[1;31m# print(\"x shape = \", x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m# no nonlinear activations in the last layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1688\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1689\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1690\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1691\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x1 and 2x128)"
     ]
    }
   ],
   "source": [
    "current_size = 1\n",
    "print(train_dict[str(current_size)].shape)\n",
    "models, step_sizes, mse_list, idx_lowest,n_forward_list = find_best_timestep(train_dict[str(current_size)], \n",
    "                                                              val_dict[str(current_size)], \n",
    "                                                              val_dict[str(current_size)], current_size,model_dir=model_dir,# make_new=True, print_every=100, \n",
    "                                                             start_k=2, largest_k = 3)#, dont_train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1500/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plot a bunch\n",
    "# # \n",
    "# step_size = 4\n",
    "# dt =1\n",
    "# n_forward = int(500/step_size - 1)\n",
    "# model_name = 'model_L{}_D{}_noise{}.pt'.format(current_size,step_size, 0)\n",
    "# model_path_this = os.path.join(model_dir, model_name)\n",
    "\n",
    "# n_points, n_timesteps, total_dim = torch.flatten(train_dict[str(current_size)], 2,3).shape\n",
    "# arch = [total_dim, 128, 128, 128, total_dim] \n",
    "\n",
    "\n",
    "# model_time = tnet.ResNet(arch=arch, dt=dt, step_size=step_size)\n",
    "\n",
    "# dataset = tnet.DataSet(torch.flatten(train_dict[str(current_size)], 2,3), torch.flatten(val_dict[str(current_size)], 2,3), \n",
    "#                        torch.flatten(val_dict[str(current_size)], 2,3), dt, step_size, n_forward)\n",
    "\n",
    "# #plot the inputed data\n",
    "# plt.figure()\n",
    "# plt.plot(dataset.val_ys[0, :, 0])#, '.')\n",
    "# #     plt.plot(np.arange(len(dataset.val_ys[point_num, :, i]))*dataset.step_size, dataset.val_ys[point_num, :, i], '.')\n",
    "# #     plt.plot(torch.flatten(val_data, 2,3)[0,s_size:,0])\n",
    "# plt.title('step size = '+str(step_size)+\" start\")\n",
    "# #   plt.xlim([0,100])\n",
    "# plt.show()\n",
    "\n",
    "# # training\n",
    "# for i in range(10):\n",
    "#     model_time.train_net(dataset, batch_size = 32, max_epoch=10,model_path=model_path_this)\n",
    "#     models = list()\n",
    "#     models.append(model_time)\n",
    "#     plt.figure()\n",
    "# #     plt.plot(dataset.val_ys[0, :, 0])\n",
    "#     predicted = tnet.multi_scale_forecast(dataset.val_ys[:,0,:],n_forward, models)\n",
    "#     plt.plot(dataset.val_ys[0,:,0], '.')\n",
    "#     plt.plot(predicted[0,:,0])\n",
    "#     plt.title('i = '+str(i))\n",
    "#     #   plt.xlim([0,100])\n",
    "#     plt.show()\n",
    "\n",
    "# # return model_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================\n",
    "def plot_lowest_error(data, model, i = 0):\n",
    "    \"\"\"\n",
    "    Plot data at model, idx\n",
    "    \n",
    "    inputs:\n",
    "        data: tensor of shape (n_points, n_timesteps, dim, dim)\n",
    "        model: Resnet model to predict on \n",
    "        i: int, which validation point to graph\n",
    "    outputs:\n",
    "        No returned values, but graph shown\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    data  = torch.flatten(data, 2,3)\n",
    "    _, total_steps, _ = data.shape\n",
    "    y_preds = model.uni_scale_forecast(torch.tensor(data[:,0,:]).float(), n_steps=total_steps-1)\n",
    "    plt.plot(y_preds[i,:,0], label = \"Predicted\")\n",
    "    plt.plot(data[i,1:,0], label = \"Truth\")\n",
    "    plt.ylim([-.1, 1.1])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#====================================================================================\n",
    "\n",
    "print(step_sizes[idx_lowest])    \n",
    "print(step_sizes, mse_list)\n",
    "plot_lowest_error(val_dict[str(current_size)], models[idx_lowest], i =2)\n",
    "\n",
    "# print(train_data.shape)\n",
    "# dataset = tnet.DataSet(torch.flatten(train_data, 2,3), torch.flatten(val_data, 2,3), \n",
    "#                        torch.flatten(val_data, 2,3), 1, step_sizes[idx_lowest], 5)\n",
    "# dataset.plot_val_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_val_data(dataset, point_num = 0, i = 0, other_plot = None):\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(len(dataset.val_ys[point_num, :, i]))*dataset.step_size, dataset.val_ys[point_num, :, i])#, '.')\n",
    "        if other_plot is not None:\n",
    "            plt.plot(other_plot)\n",
    "#             plt.xlim([0,100])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset.train_x.shape)\n",
    "# print(128**2)\n",
    "train_data = train_dict['1']\n",
    "val_data = val_dict['1']\n",
    "plt.plot(val_data[0,:,0,0])\n",
    "print(train_data.shape)\n",
    "s_size = 8\n",
    "dataset = tnet.DataSet(torch.flatten(train_data, 2,3), torch.flatten(val_data, 2,3), \n",
    "                       torch.flatten(val_data, 2,3), 1, s_size, int(np.floor(499/s_size)))\n",
    "# plot_val_data(dataset, other_plot=torch.flatten(val_data, 2,3)[0,s_size:,0])\n",
    "point_num = 0\n",
    "i = 0\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(dataset.val_ys[point_num, :, i]))*dataset.step_size, dataset.val_ys[point_num, :, i])#, '.')\n",
    "plt.plot(np.arange(len(dataset.val_ys[point_num, :, i]))*dataset.step_size, dataset.val_ys[point_num, :, i], '.')\n",
    "plt.plot(torch.flatten(val_data, 2,3)[0,s_size:,0])\n",
    "#   plt.xlim([0,100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 32\n",
    "n_forward = int(500/step_size - 1)\n",
    "current_size = 1\n",
    "train_data = train_dict[str(current_size)]\n",
    "val_data = val_dict[str(current_size)]\n",
    "model_time = train_one_timestep(step_size, torch.flatten(train_data, 2,3), \n",
    "                                         torch.flatten(val_data, 2,3),  torch.flatten(val_data, 2,3), \n",
    "                                         1, n_forward=n_forward,  max_epochs = 1000,)\n",
    "dataset = tnet.DataSet(torch.flatten(train_data, 2,3), torch.flatten(val_data, 2,3), \n",
    "                       torch.flatten(val_data, 2,3), 1, step_size, n_forward)\n",
    "# print(model_time(dataset.val_ys[:,0,:],n_forward).shape)\n",
    "models = list()\n",
    "models.append(model_time)\n",
    "predicted = tnet.multi_scale_forecast(dataset.val_ys[:,0,:],n_forward, models)\n",
    "plt.plot(dataset.val_ys[0,:,0], '.')\n",
    "plt.plot(predicted[0,:,0])\n",
    "# plt.xlim([0,500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================\n",
    "def find_error_4(data, model, truth_data, tol = 1e-5):\n",
    "    \"\"\"\n",
    "    Find error over the 4 squares \n",
    "    \n",
    "    inputs:\n",
    "        data: tensor of size (n_points, n_timesteps, dim, dim) to be predicted or size (n_points, n_timesteps)\n",
    "        model: Resnet object to predict data on\n",
    "        truth_data: tensor of size (n_points, n_timesteps, dim_larger, dim_larger) compared on \n",
    "        tol = 1e-5: tolerance level to mark points as resolved or not\n",
    "        criterion = torch.nn.MSELoss(reduction='none')\n",
    "        \n",
    "    outputs:\n",
    "        resolved: boolean whether complete area is resolved or not\n",
    "        loss: array of floats for size (dim, dim) with mse of each square\n",
    "        unresolved: array of booleans, whether that part is resolved or not. (1 unresolved, 0 resolved)\n",
    "    \"\"\"\n",
    "    if(len(data.shape))==2:\n",
    "        data = data.unsqueeze(2).unsqueeze(3)\n",
    "    assert len(data.shape) == 4\n",
    "    n_points, n_timesteps, dim, _ = data.shape\n",
    "    data  = torch.flatten(data, 2,3)\n",
    "    y_preds = model.uni_scale_forecast(torch.tensor(data[:,0,:]).float(), n_steps=n_timesteps-1).reshape(( n_points, n_timesteps-1, dim,dim))\n",
    "    \n",
    "    _,_, truth_dim, _ = truth_data.shape\n",
    "    assert truth_dim >= dim\n",
    "    \n",
    "    loss = mse(y_preds, truth_data[:,1:])\n",
    "    \n",
    "    resolved =  loss.max() <= tol\n",
    "    unresolved_array = torch.tensor(loss <= tol)\n",
    "    \n",
    "    return resolved, loss, 1-unresolved_array.float()\n",
    "\n",
    "\n",
    "\n",
    "#====================================================================================    \n",
    "    \n",
    "def mse(data1, data2):\n",
    "    \"\"\"\n",
    "    Finds Mean Squared Error between data1 and data2\n",
    "    \n",
    "    inputs:\n",
    "        data1: tensor of shape (n_points, n_timestep, dim1, dim1)\n",
    "        data2: tensor of shape (n_points, n_timestep, dim2, dim2)\n",
    "        \n",
    "    output:\n",
    "        mse: array of size (min_dim, min_dim) with mse \n",
    "    \n",
    "    \"\"\"\n",
    "    #find bigger dim\n",
    "    size1 = data1.shape[-1]\n",
    "    size2 = data2.shape[-1]\n",
    "    size_max = max(size1, size2)\n",
    "    \n",
    "    #grow to save sizes and find mse\n",
    "    mse = np.mean((grow(data1, size_max) - grow(data2, size_max))**2, axis = (0, 1))\n",
    "    return mse\n",
    "#====================================================================================\n",
    "    \n",
    "def grow(data, dim_full=128):\n",
    "    '''\n",
    "    Grow tensor from any size to a bigger size\n",
    "    inputs: \n",
    "        data: tensor to grow, size (n_points, n_timesteps, dim_small, dim_small)\n",
    "        dim_full = 128: int of size to grow data to\n",
    "\n",
    "    outputs:\n",
    "        data_full: tensor size (n_points, n_timesteps, size_full, size_full)\n",
    "    '''\n",
    "    n_points, n_timesteps, dim_small, _ = data.shape \n",
    "    assert dim_full % dim_small == 0 #need small to be multiple of full\n",
    "\n",
    "    divide = dim_full // dim_small\n",
    "\n",
    "    data_full = np.zeros((n_points, n_timesteps, dim_full,dim_full))\n",
    "    for i in range(dim_small):\n",
    "        for j in range(dim_small):\n",
    "            repeated = np.repeat(np.repeat(data[:,:,i,j].reshape(n_points,n_timesteps,1,1), divide, axis = 2), divide, axis = 3)\n",
    "            data_full[:,:,i*divide:(i+1)*divide, j*divide:(j+1)*divide] = repeated\n",
    "    return data_full\n",
    "#====================================================================================\n",
    "\n",
    "\n",
    "resolved, loss, unresolved_list = find_error_4(val_dict['1'], models[idx_lowest], val_dict['2'])\n",
    "print(loss.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unresolved_dict[str(current_size)] = torch.tensor(unresolved_list)\n",
    "\n",
    "print(unresolved_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_train_data = unresolved_list * train_dict[str(current_size*2)]\n",
    "print(next_train_data.shape)\n",
    "plt.imshow(next_train_data[0,0])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keep.append(models[idx_lowest])\n",
    "model_used_dict[str(current_size)] = [[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================\n",
    "def find_error_1(data, model, tol = 1e-5):\n",
    "    \"\"\"\n",
    "    Find error over the 1 square\n",
    "    \n",
    "    inputs:\n",
    "        data: tensor of size (n_points, n_timesteps, dim, dim) to be predicted\n",
    "        model: Resnet object to predict data on\n",
    "        tol = 1e-5: tolerance level to mark points as resolved or not\n",
    "        criterion = torch.nn.MSELoss(reduction='none')\n",
    "        \n",
    "    outputs:\n",
    "        loss: float of mse\n",
    "        resolved: boolean whether resolved or not\n",
    "    \"\"\"\n",
    "    n_points, n_timesteps  = data.shape\n",
    "    dim = 1\n",
    "    data_input  = data.unsqueeze(2)\n",
    "    y_preds = model.uni_scale_forecast(torch.tensor(data_input[:,0,:]).float(), n_steps=n_timesteps-1).reshape(( n_points, n_timesteps-1, dim,dim))\n",
    "    data1 = data[:,1:]\n",
    "    data2 = y_preds[:,:,0,0]\n",
    "#     print()\n",
    "    loss = torch.mean((data1-data2)**2)#mse(y_preds, data[:,1:])\n",
    "    \n",
    "#     print(loss)\n",
    "    \n",
    "    return loss, loss <= tol\n",
    "\n",
    "#====================================================================================\n",
    "\n",
    "\n",
    "current_size = 2\n",
    "next_train_data = unresolved_list * train_dict[str(current_size)]\n",
    "\n",
    "model_idx_list = np.ones((current_size, current_size))*(-1) #start with all -1\n",
    "\n",
    "for i in range(current_size):\n",
    "    for j in range(current_size):\n",
    "        data_this = next_train_data[:,:,i,j]\n",
    "        if (torch.min(data_this) == 0) and (torch.max(data_this) == 0):\n",
    "            #don't need to do anything is model is resolved\n",
    "            continue\n",
    "        else:\n",
    "        #see if the error is low enough on already made model\n",
    "            for m, model in enumerate(model_keep):\n",
    "                loss, resolved = find_error_1(data_this, model)\n",
    "                step_size = model.step_size\n",
    "                print(\"loss = \", loss)\n",
    "                print(\"step_size = \", step_size)\n",
    "                if resolved:\n",
    "                    model_idx_list[i,j] == m\n",
    "                    break\n",
    "                else:\n",
    "                    pass\n",
    "            if not resolved:\n",
    "                i = 0\n",
    "                j = 1\n",
    "                k = int(np.log2(step_size))\n",
    "                print(\"k = \", k)\n",
    "                print(\"train_dict[str(current_size)][:,:,i,j] shape = \", train_dict[str(current_size)][:,:,i,j].shape)\n",
    "                #if no model good, train new model\n",
    "                models, step_sizes, mse_list, idx_lowest = find_best_timestep(train_dict[str(current_size)][:,:,i,j], \n",
    "                                                              val_dict[str(current_size)][:,:,i,j], \n",
    "                                                              val_dict[str(current_size)][:,:,i,j], current_size,model_dir=model_dir,\n",
    "                                                              i=i, j=j, start_k = max(0,k-1), largest_k = k+2)\n",
    "                \n",
    "                vbnm\n",
    "                resolved, loss, unresolved_list = find_error_4(val_dict[str(current_size)][:,:,i,j], \n",
    "                                                               models[idx_lowest], \n",
    "                                                               val_dict[str(current_size)][:,:, i*current_size:(i+1)*current_size, j*current_size:(j+1)*current_size])\n",
    "                model_keep.append(models[idx_lowest])\n",
    "                model_idx_list[i,j] == len(model_keep) #last model will be the one for this square\n",
    "            \n",
    "#             predicted = model.uni_scale_forecast(torch.tensor(data[:,0,:]).float(), n_steps=n_timesteps-1).reshape((  n_points, n_timesteps-1, dim,dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(step_sizes, mse_list, idx_lowest)\n",
    "resolved, loss, unresolved_list = find_error_4(val_dict[str(current_size)][:,:,i,j], \n",
    "                                                               models[idx_lowest], \n",
    "                                                               val_dict[str(current_size*2)][:,:, i*current_size:(i+1)*current_size, j*current_size:(j+1)*current_size])\n",
    "print(loss)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  models[idx_lowest]\n",
    "print(idx_lowest)\n",
    "n_timesteps = 500\n",
    "n_points = 10\n",
    "dim = 1\n",
    "# plt.plot(model(val_dict[str(current_size)][:,:,i,j].unsqueeze(2).unsqueeze(3))[0,:,0,0].detach().numpy(), label = \"predicted\")\n",
    "print(val_dict[str(current_size)][:,0,i,j].unsqueeze(1).shape)\n",
    "val_data_this = val_dict[str(current_size)][:,0,i,j].unsqueeze(1)\n",
    "predicted = model.uni_scale_forecast(val_data_this, n_steps=n_timesteps-1)\n",
    "print(predicted.shape)\n",
    "predicted_reshape = predicted.reshape((  n_points, n_timesteps-1, dim,dim))\n",
    "plt.plot(predicted_reshape[0,:,0], label = \"predicted\")\n",
    "print(val_dict[str(current_size*2)][:,:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size].shape)\n",
    "# plt.plot(val_dict[str(current_size*2)][:,1:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size][0,:,0,0], label = \"Truth\")\n",
    "\n",
    "plt.plot(val_dict[str(current_size*2)][:,1:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size][0,:,0,1], label = \"Truth\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(val_dict[str(current_size*2)][0,0])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = (16+32)/2\n",
    "print(step_size)\n",
    "model = train_one_timestep(int(28), train_dict[str(current_size)][:,:,i,j].unsqueeze(2), \n",
    "                           val_dict[str(current_size)][:,:,i,j].unsqueeze(2), \n",
    "                           val_dict[str(current_size)][:,:,i,j].unsqueeze(2), current_size)\n",
    "#                        dt = 1, n_forward = 5, noise=0, make_new = False, dont_train = True, \n",
    "#                        lr = 1e-3, max_epochs = 10000, batch_size = 50,threshold = 1e-4, \n",
    "#                        model_dir = './models/toy2',i=None, j = None):\n",
    "    \n",
    "#     train_dict[str(current_size)][:,:,i,j], \n",
    "#                                                               val_dict[str(current_size)][:,:,i,j], \n",
    "#                                                               val_dict[str(current_size)][:,:,i,j], current_size,model_dir=model_dir, \n",
    "#                                                               i=i, j=j, start_k = max(0,k-1), largest_k = k+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 1\n",
    "resolved, loss, unresolved_list = find_error_4(val_dict[str(current_size)][:,:,i,j], \n",
    "                                                               model, \n",
    "                                                               val_dict[str(current_size*2)][:,:, i*current_size:(i+1)*current_size, j*current_size:(j+1)*current_size])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model =  models[idx_lowest]\n",
    "print(idx_lowest)\n",
    "n_timesteps = 500\n",
    "n_points = 10\n",
    "dim = 1\n",
    "# plt.plot(model(val_dict[str(current_size)][:,:,i,j].unsqueeze(2).unsqueeze(3))[0,:,0,0].detach().numpy(), label = \"predicted\")\n",
    "print(val_dict[str(current_size)][:,0,i,j].unsqueeze(1).shape)\n",
    "val_data_this = val_dict[str(current_size)][:,0,i,j].unsqueeze(1)\n",
    "predicted = model.uni_scale_forecast(val_data_this, n_steps=n_timesteps-1)\n",
    "print(predicted.shape)\n",
    "predicted_reshape = predicted.reshape((  n_points, n_timesteps-1, dim,dim))\n",
    "plt.plot(predicted_reshape[0,:,0], label = \"predicted\")\n",
    "print(val_dict[str(current_size*2)][:,:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size].shape)\n",
    "# plt.plot(val_dict[str(current_size*2)][:,1:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size][0,:,0,0], label = \"Truth\")\n",
    "\n",
    "plt.plot(val_dict[str(current_size*2)][:,1:, i*current_size:(i+1)*current_size, i*current_size:(i+1)*current_size][0,:,0,1], label = \"Truth\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
